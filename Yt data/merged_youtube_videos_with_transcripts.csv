id,title,description,publishedAt,tags,categoryId,defaultLanguage,defaultAudioLanguage,thumbnail_default,thumbnail_high,duration,privacyStatus,viewCount,likeCount,commentCount,channel_id,channel_title,channel_description,channel_country,channel_thumbnail,channel_subscriber,channel_videoCount,video_id,transcript
V8COJv2dG2g,"That ""quick question"" at 4:59 PM always hits different. ðŸ˜…","We know the question wonâ€™t be quick. 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Crystal Endless",2025-11-27T00:00:40Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Campaign: Video Type:G4D SV: Comedic skits;,ct: AIG;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/V8COJv2dG2g/default.jpg,https://i.ytimg.com/vi/V8COJv2dG2g/hqdefault.jpg,5,public,4000,45,2,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,,
a7nJeXk6VxU,Vibe coding and context engineering with ADK,"Go from an empty folder to a fully deployed AI agent using only natural language. This video introduces ""vibe coding,"" a powerful technique to build applications from scratch using the Gemini CLI.

Follow along with Sita and Amit as they demonstrate how to scaffold a new AI agent with the Agent Development Kit (ADK) in plain English. You'll then learn ""Context Engineering,"" the secret sauce for moving your agent from a prototype to production. See how to provide documentation, coding standards, and feature requirements to guide the AI in generating high-quality, maintainable code.

Finally, watch as they deploy the entire agent to Google Cloud Run with a single command.

Stick around for the ""Eng Talk"" segment, where Amit and Sita answer top community questions:
- Is RAG obsolete with million-token context windows?
- How do you manage context overload?
- What metrics should you use to track context retrieval?
- What are the best tips for token optimization?

Chapters:
0:00 - Introduction
0:34 - Core Concepts
3:39 - Setting up Gemini CLI in VS Code
5:05 - Loading ADK Documentation for the Agent
7:03 - Vibe coding an Expense Tracker from Scratch
11:36 - Refactoring Code into Tools and Models
16:10 - Debugging Python code with Gemini CLI
25:15 - Why You Need Context Engineering
26:36 - Creating Context Files: Gemini.md, PRD, & Summary
32:02 - Building Advanced Features with Context Engineering
35:52 - Preparing for Cloud Run Deployment (Docker & Server)
37:44 - One-Command Deploy to Google Cloud Run
39:30 - Recap
40:48 - Community questions
45:55 - Resources

Resources:
Learn more about the Agent Development Kit â†’ https://google.github.io/adk-docs/
Install the Gemini CLI â†’ https://github.com/google-gemini/gemini-cli 
Code from this tutorial â†’ https://github.com/amitkmaraj/expense-tracker-agent
A2A Protocol official site â†’ https://a2a-protocol.org/latest/

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Products mentioned: Google ADK, Gemini CLI, Google AI",2025-11-26T17:00:00Z,"Google,developers,pr_pr: Google Cloud Tech;,Purpose: Learn;,Video Type:DevByte;,ct: ;,gds:N/A;",28,en,en,https://i.ytimg.com/vi/a7nJeXk6VxU/default.jpg,https://i.ytimg.com/vi/a7nJeXk6VxU/hqdefault.jpg,2807,public,3710,172,19,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,a7nJeXk6VxU,"Today we're going to learn three
powerful techniques to build an AI
agent, but all without writing a single
line of code. And yes, we'll be using
plain English language and CLA commands
from start to the end. And then we'll
take the agent that we built and then
deployed to Cloud Run using just one CLA
command. And to do this, we have Amit
here with us today who's an expert at
creating context engineering workflows.
So Ahmed, why don't we start by
explaining what key concepts you'll be
showing us today?
>> Hi Sa. Yeah, thanks so much for having
me. So we're going to cover three
concepts today. Vibe coding, which is
essentially giving an AI player
programmer your vibe or what you want to
develop and having it go out and do that
for you. The second is context
engineering. Sometimes these AI agents
need more context in order to
successfully develop features and write
code. And so we're going to see how the
inclusion and creation of new documents
can help our AI agent have full context
on the features that we're trying to
develop. Finally, token optimization
will see our AI agent really drill down
on the most necessary documents it
should be focused on to complete its
feature development.
>> Okay, but hold on. Before we go ahead
and build this workflow, I think this
video needs to be free from buzzwords,
right? So white coding has been around
for a little bit now but context
engineering is relatively new. Uh why
don't you go ahead and tell us in depth
what the technical differences between
both these.
>> Yeah so vibe coding is really good at
sort of getting you up to a place where
you have a project and it's maybe like
80% of the way there. However, to get to
that final 20%, really context
engineering is what kind of unlocks this
for you because you don't have to
reprompt your AI agent every time you
want to develop a new feature to tell it
things like best practices, where it
should store certain pieces of code in
different files or where it should find
your documentation or how your API looks
like. Essentially, you give it access to
all of those types of documents and
context engineering just says that
anytime your AI agent runs and develops
a new feature or writes new code, it can
reference all of those documents to
continue staying on track and developing
the best production scale software that
you need.
>> Okay, that makes sense. So, white coding
is essentially an AI pad programmer that
you tell your natural language
instructions to and then it goes and
creates code for you. Uh but context
engineering is how you take that code
and then make this a little bit more
better by removing the guesswork and
giving it specific instructions or
context. Well, I also heard the term
token optimization that you use. So what
is that?
>> Yeah, great question. So you essentially
through context engineering are putting
together this like briefing packet for
your AI agent anytime it has to run. So
it has all access to all of this
information through all of these
different documents. But over time, the
briefing packet can become too large.
And so token optimization tries to
reduce that by really giving your AI
agent some direction on where it should
be focusing on because it's really easy
for your context window for your AI
agent to become really overloaded. So
you essentially just want to make sure
that it only includes the necessary
documents or information or context to
be able to complete its task.
>> Fantastic. So white coding is for speed
and context engineering is how you
improve the quality of the code created
and token optimization is for efficiency
and to bring down the cost lower and I
can't wait to see how all of this comes
together. So Amed we're starting from a
completely blank slate here right
completely blank. Let's dive right in.
So, in this demo, I want to show you how
you can build an expense tracking agent
with AI assistance. And we'll kind of go
from zero to deployment on Cloud Run um
from start to finish. And we'll use vibe
coding, context engineering, and token
optimization to get us there. Let's get
started. You can see here that I have a
blank slate on VS Code. And the first
thing that I want to do is install
Gemini CLI, the extension. So, you can
go to the extension tab here, type in
Gemini CLI, and you'll see Gemini CLI
pop up. I already have it installed, but
you'll have the little install button
here. You can run it. Now, we can just
go back into our folder structure here.
Um, so that we have our blank slate. I'm
going to remove it so we have some
space. And then you can pop open the
command panel and just type in Gemini
CLI. It gives you the option to run
Gemini CLI in a little terminal. Um, it
might pop up on the bottom side for you.
I have it configured to pop up on the
right hand side here. And you'll see
Gemini CLI pop up. You may be prompted
to log in if this is your first time
using Gemini CLI. You can use whichever
Gmail or Google account you have access
to. Um, and just make sure Gemini CLI is
enabled and you should be good to go.
You'll also see that we're working
within a blank directory. So, we have
everything scaffolded ready for us to
get started on developing our agent. So,
in order to actually start vibe coding,
we want to make sure Gemini CLI or
whatever agent we're working with has
access to all of the documentation and
really understands how to develop an
agent. And the way that we can do that
is by pulling all the documentation for
ADK. We're going to be using ADK for our
agent in this demo. So, I'm just going
to pop open a new terminal here and copy
and paste this command, which grabs all
of the ADK documentation. They prepared
it inside of a txt file for us. Um, and
it's specifically for LLMs to use it for
context engineering. So, we're just
going to curl it and then put it into
our directory. And within a couple
seconds, we can open up our directory
structure. And we'll see this file
popped open here. And if you open it,
you'll actually just see all of the text
for the documentation for ADK has been
updated since a few days ago. So,
>> so Ahmed, for those who don't know, what
exactly is the LLM's full.txt? PXD.
>> Yeah, great question, Sitta. So, this
gives us a nice little summary or
combined packet of all of the ADK
documentation for us. Um, and they put
it into this nice txt file, so it's as
small as possible. And then it gives
your LLMs, in our case, uh, we're using
Gemini CLI. It gives your Gemini CLI or
agent the ability to understand all of
the documentation based on what's inside
of the txt file. So they've essentially
taken all of the documentation on the
ADK docs and then combined into this one
file that can then be used as easily as
possible by your agent. So we're going
to tell Gemini CLI that this LLM's
full.txt file exists, point it towards
it, and then allow it to have full
context on developing its ADK agent
using all of the information in this
file. So, I'm just going to close this
one here, and we can get into the fun
part, which is actually running Gemini
CLI and telling it to actually build a
new ADK agent for us. So, all the
commands that I'm going to be running,
the prompts, and everything can be found
in the description of this video below.
Feel free to take a look if you'd like
to replicate this. However, with Vibe
Coding, of course, you're bound to
experience your own journey when running
through this. So the first thing we're
going to do is tell Gemini CLI that we
want to create a tracking agent for
expenses. And we're going to give it
some instructions. We're going to tell
it that we want to use UV for package
management. We want to use um tool
functions and make sure that everything
and all the functionality for our agent
is consolidated into a tools file. Then
the ADK agent itself is a separate file.
The entire prompt that I pasted into
here can be found in the description box
below.
And one of the most important things of
this prompt is the LLM's hyphen full.txt
file. You're seeing that we're
referencing it here at the bottom of our
prompt. We're essentially telling Gemini
CLI that, hey, we want to build this
entire thing, but I want you to look
inside of this file to make sure that
you know how to build it and what the
best practices are when working with
ADK.
So you'll see here all we're saying in
the beginning is to create a plan.
Sorry. So you can see create a simple
implementation plan using Python ADK.
This is very helpful when prompting your
agent to create a plan beforehand. So
you can ensure that whatever it plans on
doing looks good and before it actually
starts coding, you can approve or
decline accordingly.
You'll see it came up with a plan. The
full implementation plan has a project
setup, dependencies, agent tool
implementation, testing, so on so forth.
This looks good to me. So what I'll do
is tell it to proceed. So I'll do is
great proceed with the implementations
and again point it towards the LLM's
hyphen full.txt file. After a few
seconds, it actually prompts me to allow
execution of UV. We're using UV as a
package manager here to manage our
entire project.
With Gemini CLI, you have the option to
tell it whether or not you want to allow
it to run certain things, create new
files, read from files, execute certain
commands. You can tell it to do it once,
uh, allow always, or just suggest
changes. This case here, we want it to
run, um, through the entire process
smoothly. So, I'm just going to tell it
to always run. But obviously, you have
full control as the person who's running
the the agent.
But, in this case, we actually got a
good plan returned from the LLM in the
first try, right? But this is usually
not the case. we have some stumbles here
and there and then we have to debug our
agent and then redirect its scores. So
in these cases, how do we make sure to
capture all these tweaks um into our LLM
as well? So it it is sure to not repeat
the same mistakes again.
>> Yeah, great question. So there's a few
ways of approaching this kind of plan
curation/ alteration before moving
forward. You can essentially tell it to
refine the plan based on some
requirements that you see were not
included in the plan or you can tell it
to output the plan to a file and that
way you can edit and modify the plan
accordingly before moving forward. If
you do the latter option where you tell
it to spit the plan out into a file
first and foremost, you can go and edit
it, but then when you go and tell it to
continue moving forward, uh you would
just make sure it points to that file
with the plan included.
So we'll see here if we pop open the
main.py file, Gemini CLI actually has
all the right ideas in spitting out the
correct code to the file, but it seems
like it wasn't able to execute that last
piece. So I'm going to tell it that,
hey, Gemini CLI main. py seemed to have
the correct idea. Uh, but it didn't have
that final update. So we're just going
to point it to that file and make sure
that it does absolutely capture
everything that it wanted to do
initially.
Well, we're in the w coding land, so
anything can happen, right?
>> Totally.
Great. And you'll see Gemini CLI, one of
the nice things when working with it, if
you are sticking around while it's doing
its thing, it has a a bunch of funny um
whims that it comes up with anytimes it
anytime it runs. So, I'll go ahead and
tell it to always allow during this
session. And you'll see here it actually
created the um it created the agent
successfully. So, I'm going to close the
terminal for now. Um we'll pop it back
open in a moment and we'll take a look
at this file. You'll see it did exactly
what we wanted it to. It created a class
for expense so that it always has um
some level of structure when it comes to
creating a new expense for our expense
tracker. It has this um function here
for add expense and then it has um the
agent itself which is very simple and
then a tool attached to it which is the
ad expense. Next thing I'm going to ask
it to do is move that tool into its own
tools. py file and the reason we want to
do this is so that whenever we have new
tools as we continue creating new
features the agent has access to those
new tools and we can have it all
compartmentalized and in its own file so
we can continue to build upon it.
Okay. Okay, so I'm telling it here to
move the data class and the tools to
their own respective files so it's
easier to work with as we continue
building new features which you'll see
in a few short moments.
We'll see here it was able to move the
model to its own model file and then
tools to its own tools file.
The other thing I want to take a look at
is the pi project.toml file. This should
have some dependency Google ad for
example to get started. Uh so we're
going to go ahead and just add that in
another terminal. This one's pretty
simple. We can do Google uv add Google
ad. This will add Google ad directly
into the dependencies for our pi project
toml file. And this is really nice
actually. You'll see that there are some
things that we're doing here that are
kind of manual where you know obviously
this for example and we're asking it to
move the tools and the model. However,
when we create our best practices
document um it'll inform Gemini CLI
going forward for every feature. anytime
it adds a new dependency or it creates a
new feature, it'll have full knowledge
on where to put the appropriate pieces
of code and whether or not to update the
dependencies inside the PI project to
mobile file.
So you can see it's finally finished and
now we can move on to the next part. So
as we know from ADK, some of the best
practices when creating our agent is to
have all of our agent files inside of
its own directory. So we're going to
create a new directory called expense
tracker.
and we're going to move main models and
tools into there. So just move it
directly.
That looks great. The next thing that we
want to do is create av file. So I'll
just create that in the root folder
env. And we'll copy and paste
some of these variables so that we can
deploy successfully. We're going to use
vertex AI. We're going to have our
project ID a mirage development for me.
And then our location. I'm going to use
Europe West one.
Save this. I'll just move the terminal
for now.
And just to highlight why we're doing
this, we're only setting the environment
variables because we need to
authenticate to the model. So this would
depend upon the model that we use u how
you authenticate as well.
>> Yeah, that's correct. So um we want to
make sure that we are using the correct
model and the correct service when we
deploy it up to the cloud. Um
additionally when we run it locally uh
if as long as we're logged in locally
using Google cloud o we are able to use
vertex AI to coordinate with the model.
So you'll see inside of this models file
sorry inside of the main.py file you'll
see that the model that we're having
access to is Gemini 2.5 flash and this
is what we'll be using. So, in order to
access the deployed version of Gemini
2.5 Flash, we're going to be using
Vert.Ex AI's deployed instance of it.
>> And to also call out the fact that even
if you're not using Google Cloud or
Vertex AI, you can still follow through
this video and then run the um agent
because you can always swap these
environment variables for your own
configuration. So, you can get an API
key from Google AI Studio and then run
this whole thing.
>> Yeah, exactly. Good point, Sitta. Yeah.
And if you were going to use AI Studio,
you would just make sure that your API
key is inside of this ENV file and it
should work seamlessly. The last thing
that we want to do is rename our agent
file from main.py to agent.py.
We'll pop open our terminal again and
continue moving forward. The next thing
we want to make sure is that we're
logged in. So, I'm going do G-Cloud O
login. Press enter. And on another
screen, it popped up for me. You should
see it pop open in a browser for you.
Once logged in, you'll see it on the
right hand side successfully show that
message.
Okay, now we're at the fun part. We can
actually run ADK web locally to see if
this all works. Let's clear our terminal
here. And the first thing we're going to
do here is uv run adk web. Press enter.
And you'll see here after a few short
moments, it pops open with this URL. If
you click on it, it'll open this up in a
new web browser. You'll see by default
it has expense tracker and we can shoot
it a simple message. So we'll see here
we're running into an error. It says no
module named tools. So let's take a look
at our code and see what happened. We'll
pop this back open and close our
terminal for the moment.
And we'll see here that inside of our
agent. py file, it actually tries to
reference tools as it's its own package.
However, because it's lo referencing a
local file, we just have to make sure
that we're referring to it like it's a
local file here and
seems like that's good. Otherwise, in
tools py, we also have to make sure
because it's importing models, we're
importing that with a period at the
beginning.
Sure, our relative imports are all good
to go. So, we'll abort abort our ADK
web, run it again,
then we can refresh the page.
Try it one more time.
Press enter. And in a few short moments,
we should get a nice response. So, what
would you like to do? Let's try and
practice and see if it's able to
actually add an expense for us. So,
we'll do add an expense 50 bucks for
groceries.
short description of this expense for my
weekly groceries.
Okay, so now we're running into another
issue. Dict object has no attribute
amount. So we'll copy this error and
feed it into Gemini CLI because this is
a really good example of it creating
something that doesn't really work. and
we'll see how it's able to fix it. So,
so I tell it this is the error I'm
getting when it's trying to add an
expense. I paste in the error and let's
see what it comes up with. Great. And so
you'll see here that it actually saw
that this tool needs to be included at
the top of this function in order for it
to be treated as its own tool. So let's
try to run this one again and see what
happens.
add an expense $50 for groceries
description.
Okay, I'll copy this in case this
doesn't work again. We can try one more
time. Okay, so finally, we just want to
make sure that our tools are set up
correctly. It seems as though this tool
that it was added and you know this is a
really good practice when you're
creating functions in Python for
software engineering when it comes to
agentic development. We want to make
sure that our arguments inside of the
tools are as primitive as they can be
and are treated separately instead of
being able to take an expense which is
the object that we have in our side of
our model. We want to make sure that the
arguments are the actual values itself
and then we can cast it to expense.
So we fix that here and have amount
float category string description string
and then we can take that and use it
accordingly. Let's go ahead and save
this and then we'll run our ADK agent
again and see what happens.
Pop this open. We'll grab my expense
again.
And great, it looks like it was able to
find the tool, successfully call it,
execute it, and give us a response
saying successfully added expense,
groceries of the week, 15 groceries.
Let's try adding another expense.
120 bucks for utilities in the bills
category. You'll see it's also able to
successfully do that. And finally, we'll
ask it, what's my total spending?
So, you'll see it comes back with a
response saying, I can only add expenses
at the moment. I cannot track your total
spending. So, let's go ahead and stop
our server and develop the feature
that'll allow us to calculate total
spending.
So, for a quick recap of what we've seen
so far, we still haven't written a
single line of code. Uh, it's insane,
right? We've been using Gemini CLI to
build the agent to run it and to debug
it successfully and we've got working
finally. So for the next section, will
we be still using the same Gemini CLI or
maybe with other techniques?
>> Yeah, totally. So we will still be using
Gemini CLI and it's great to point out
that it was able to do all of this with
just that one file and it was able to
build an agent with tools, but we're
going to make it even more powerful with
context engineering and token
optimization in the next section.
Cool. So, we can jump back into our
Gemini CLI and give it another prompt.
I'm going to say that I want to add the
following functionality. Calculate my
total spending, get spending by
category, list recent expenses, and I
want it to say uh remember to add the
tools to the agent once they're created.
So, we wanted to create the tools and
then add it to the agent after. That
functionality should give us enough to
play around with the agent a little bit
more.
So, while that's running, we can take a
look at what it's doing. The first thing
we want to take a look at is the tools
file. And we'll see here that it has an
in-memory database. It's really smart in
kind of understanding that we're just
building this as a proof of concept and
it's not connected to a database. So, it
should have some local storage that it
can use to store all the data for us,
expenses in our case, to be able to pull
it back up and perform some computation
on it. For example, calculate total
spending should be able to pull up those
expenses from where it's stored and then
show it to us. You'll see calculate
total spending. It sums everything in
the expense database. Get spending by
category finds all of the spending in
each category that we've registered
already and spits back out to us. And
then finally lists all recent expenses.
We'll take a look at agent.py as well.
And you'll see here it followed our
instructions correctly. It was able to
import the tools and then throw it into
the tools array. So, looks like our
agent should be ready to go. Let's try
it out.
Another neat thing it did for us is it
told us some prompts that we can give
our agent in order to test some of the
functionality. So, what's my total
spending? How much have I spent on food?
And show me the last two expenses. This
is really nice and it's almost like a
pair programmer where you're asking your
programming friend to implement
something for you and then they interact
with you by saying logically this is
what I've implemented. This is how you
can test it. Go ahead and save this and
then run it inside of the agent to see
what it comes up with. Okay. Now that we
have our agent running, we can go back
into ADK web and add some expenses.
Great. So, we did this in our previous
step, but of course, because we
refreshed and changed the agent, we got
to do this again. So, we're just going
to add two expenses, and then we can ask
it what our total spending is. And let's
see if this flow runs again this time.
Great. You can see that it was able to
actually figure out which tool it should
run from the new tools that it developed
most recently. And then it gave us an
accurate number for what we've actually
spent. 120 + 50 is 170 last time I
checked.
Next thing we'll do is get spending by
category. Press enter.
And it prompts us because it knows that
by default we haven't given it a
category. So we can give it a category.
We'll say utilities. Press enter.
And we'll see that it runs that function
for us. And it says your total spending
for utilities is $0. And this is quite
interesting because we said that
utilities here is the name, but we put
it in the bills category. So it was able
to differentiate that this specific
expense isn't actually from the
utilities, but it's in the category of
bills. So next, we'll ask it if it can
give us our spending for the bills
category.
So you see here it runs it successfully
and it gives it back to us accurately.
Your total spending for bills is 120
bucks. Finally, we'll ask it to list
recent expenses
and it's able to successfully call that
final function that it created. And here
are your recent expenses. Groceries for
the week $50. Utilities 120. You can see
here it took the description groceries
for the week that I gave it. Similarly,
utilities which we didn't specify a
description but it used some inference
to find out that yes it should use that
for description.
>> So far we've seen how to use Gemini CLA
to create this agent and build tools and
um you know get it working. But then
this raises the important question of
why do we need context in this case?
Like what more do we want to achieve
using context engineering?
>> Yeah. So this is where context
engineering can transform good code into
great code. So we're going to set up
like three key files that will guide the
AI. We're going to first set up a
gemini.md file which will include
project standards and best practices, a
PRD for which stands for product
requirements document and that'll be for
developing new features and then a
project structure summary. So right now
we have our entire directory with all of
our files and anytime Gemini CLI runs it
has to build context on all those files.
It has to look through all those files
to see exactly where it needs to add
data start writing code remove code
update delete add so on so forth. So it'
be really nice if we can create an
overview project summary file that
essentially includes a overview of what
all of those files are and what they're
responsible for.
So, we're just going to shut down our
ADK agent and we'll close all the files
and we'll keep things a little neat and
tidy.
The first thing we're going to do is
edit our Gemini MD file.
Once you install Gemini CLI, a Gemini MD
file will be created for you inside of
your user directory inside of a hidden
folder called Gemini. So, we'll just
open that up. And by default, it'll be
blank. But for us, we just want to add
some best practices for our expense
tracker. So, I'm going to just paste all
of this in. If you scroll through here,
you'll actually see it's just expense
tracker agent, project context, project
overview, making sure to use specific
Python styles, error handling, data
storage, new features, whatever. And for
every project, you can add it directly
into this Gemini.md file so that you
have all of your best practices ready to
go anytime anyone develops with Gemini
CLI for that specific project.
So, we'll just save that and close it.
The next thing we're going to do is
create two folders. One of which will be
a docs folder and then the other of
which will be an examples folder. We
won't use the examples folder, but you
can imagine that you can create new
examples inside of that folder and then
give Gemini CLI that example anytime you
want to build something specific. So,
create those two folders. Docs
and examples. And then the first one
we're going to create is our PRD for our
new feature recurring expenses.
So create this file here and we'll paste
in our entire PRD. A PRD stands for a
product requirements document. And
historically, it's been the one document
that ties engineering and product
together. Product will come up with the
feature, come up with all of the
different user flows, and then create a
PRD, hand it off to the engineering team
that that will then use it to compute
and develop the entire feature from
start to finish. We can use the same
structure for a PRD because it does a
really good job of essentially combining
both product user flows and engineering
implementation practices.
So, we save our PRD here and we can
close it.
We'll jump right back into Gemini CLI.
I've closed it to begin with because we
edited the Gemini MD file. So, we got to
pop it back open so that it has full
context.
We'll see here that it's actually using
the one gemini.md file, but we can see
exactly that context and make sure that
it has all the context that we want from
that file before going forward. Just
type memory forward/memory and then
show. I'll actually show you all the
context it takes from the gemini.md file
and any other file that you feed into it
before computing.
The next thing that we want to do is
we're going to ask Gemini CLI to create
a full snapshot of the entire directory
for us and then save it into a project
summary file.
So once we run this, it essentially says
analyze the current directory structure
and create a concise summary that
includes a tree of all the files, brief
description, key dependencies, and
overall architectural pattern. This is
going to be extremely helpful going
forward because Gemini CLI won't have to
look through all of the files and
understand exactly what every file does
in order to make changes, updates,
deletions, creations in the future. All
it needs to do is start with the summary
file, see what file is responsible for
what, and then dive into that file
directly.
This will help speed up our entire
Gemini CLI development flow by
introducing what we call token
optimization. So instead of taking every
file and shoving in into the context
window, it can just take the one project
summary file, find out which file it
needs, then go and grab that file and
then throw that into the context window
to then compute.
>> So if we take a step back and look at
all the files that we've created so far,
uh we've created three important files,
right? First is the gemini.md which
specifies the coding standards and this
is the file that will be used each and
every time uh when the llm is invoked
from the gemini cla and then we also
created a products requirement document
or a prd file which describes what needs
to be done for this specific agent. So
it might have very specific things like
functionalities and features for this
agent, where to deploy it uh and all of
the other quer but only for the specific
agent that we're building. And then the
third file is the project summary file
which we're also feeding into the Gemini
CLI which basically tells us it is
snapshot of the whole uh project
structure that we have so that the LLM
doesn't have to go and then refer to the
project structure each time it's making
a call and doing something. It can just
refer to this one single file get an
overview of how the project is
organized.
>> Yeah, that's a good summary sitta.
Exactly. So starting with the last one
that you mentioned, we can see here the
Gemini CLI created a nice little file
for us that includes the entire file
structure here and a description for
every file that exists and all the
dependencies and such. And this is a
really nice concise overview because
every time Gemini CLI runs, it could
essentially just grab this project
summary, start there and then move on.
And then additionally, I created this
PRD recurring expenses file which
includes that requirements document for
that feature that we're going to
implement next.
Great. So, we're going to get to the
next point where we actually ask Gemini
CLI to use context engineering with all
the files we just created and create our
next feature for us, recurring expenses.
So, I'm going to copy the next prompt,
paste it in, and again, I'll point it to
the LLM's full.txt file. But
essentially, I'm just saying I want to
implement the recurring expenses feature
described in. Then I point it to docs
prduring expenses. Please review the PRD
and code structure inside a project
summary file following the coding
standards in gemini.mmd file. Extend the
agent py file and then update the data
model and include proper error handling.
You can see how everything is almost
coming together. Now we've included all
of these files that we're going to use
for feature development going forward.
And we don't want to have to repeat
ourselves every time we develop a new
feature inside of Gemini CLI. Obviously,
you can just go through all of these
files, find all the information it
needs, throw it into the context window,
whatever is relevant, and then produce
some output for us.
Great. Gemini CLI seems to have
successfully made this feature for us.
So, let's dive into the code and see
what it did.
Firstly, we'll take a look at the agent.
py file. You'll see that it created
three new tools for us. The add
recurring expense, list recurring
expense, and project spending. And then
it also threw those tools into the tools
array so the agent has access to them.
In the tools file, you can see it
created a new array for us specifically
for recurring expenses. And then it
created three new functions for us. Add
recurring expense, list recurring
expense, and then project spending so
that we can see over time what our
expenses look like. And then finally,
inside of the models file, it has a new
class for us called recurring expense.
So let's start up our agent and give
this one a whirl.
Great. It looks like ADK is running
successfully. So, let's create our first
recurring expense.
Add a recurring monthly expense, $1,200
for rent starting today, October 7th,
2025, and the description is rent
payment for home. You'll see our agent
was able to successfully find the tool
that it needed to run for adding this
recurring expense and gave us some nice
feedback saying that it was successfully
able to run it and add that expense.
Let's do another
add another recurring expense $15 for
Netflix monthly subscription starting
today 2025
October 7th description entertainment
and you'll see it successfully added it.
Then finally, I'm going to say project
my spending for the next 3 months, and
we'll see if it's able to calculate it
correctly for us. It finds the correct
tool. Your projected spending for the
next 3 months is $3,645.
So firstly, let's stop our agent from
running. We'll close everything down.
But the key difference here is actually
to call out what context engineering
specifically performed here because in
both cases we end up having an agent
that's working right. But if we look
into the code that the agent created
this time around you can see more
structure around it and more error
handling. Um is it correct?
>> Yeah exactly. I think without context
engineering, the Gemini CLI or any agent
may not be able to create very complex
code that is integrated within
pre-existing files and follows best
practices and examples from what we've
developed previously
>> and so far we've seen how without
writing a single line of code mostly
debugging but not writing code we've
created a fully functional working agent
but I think for the next step we'll need
a little bit of scaffolding um to make
this agent deployable.
>> Yeah, good point, Sita. So, getting into
the final portion of this demo, we're
going to deploy this agent because when
we deploy to Cloud Run, Cloud Run uses a
Docker file and that Docker file needs
to be configured specifically in order
to get this AI agent to run. So, these
two simple files, which can be found in
the description of this video, will
allow us to deploy successfully without
any issues.
So, let's dive in. The two files that
we're going to need, server.py py and
docker file. Both of them are going to
be in the root of our directory.
So create a new file server.py
and copy and paste this fast API
scaffolding that'll help run our ADK
agent on the web and give it access by
API as well.
Similarly, we'll create our Docker file,
paste this in, and we should be good to
go.
So, I'm going to pop open the terminal
here, and I'm going to configure G-Cloud
to be pointed to the project that I want
to deploy this specific AI agent to.
G-Cloud config set project amarrage
development.
>> The other thing to call out here is if
you're not using u G-Cloud internally,
you can also use the ADK deploy CLA
command um which again will take both
these files that you've created and
deploy it to Cloud Run. Yeah, exactly.
You beat me right to it. I was going to
just paste that command into the
terminal and you're correct. So, there
are two ways of deploying this to Cloud
Run. You can use G-Cloud run deploy or
you can use ADK's builtin tooling for
deploying to Cloud Run. We're going to
use ADK deploy cloud run and then we're
going to give it all the parameters that
we need. So, we use uv run ADK deploy
cloud run. We have to use uvun here
because we're using uv as a package
manager and ADK is installed locally
within our directory. And then we give
it all the parameters that we want in
order to deploy it. So Amari development
for the project. Region is Europe West
one. Service name, expense tracker,
agent, app name, expense tracker. We're
going to say that we want it to have a
UI. And then we're going to point it to
the folder in which the agent lives in.
So expense tracker agent. py. Press
enter. And we're going to give that a
moment to run.
Great. It looks like it's successfully
deployed. If you see this service URL
here, it just means that this is the URL
that we can use to access our AI agent.
So, I'm going to copy this and paste it
into a new tab here. And you'll see
successfully our AI agent has been
deployed and is available and running.
And just to make sure all of the most
recent feature development is deployed
successfully, we can add the same prompt
that we did before. Add a recurring
monthly expense 1,200 and see if it's
able to pull up that tool. Success.
Great. Runs super quickly as well. I'm
not sure if you saw that. And so just
like that, you've been able to use
Gemini CLI, an AI agent, to develop a
new ADK agent that can track your
expenses for you and deploy it with one
command.
>> And also just to call out, this is not
the local host URL that we're running
the uh ADK web from, but this is the
cloudr run endpoint.
>> Yeah, that's correct. You can see it
from this URL here. But more
importantly, if we jump into the cloud
console, you can see the expense tracker
agent running successfully and it has
its own URL. You pop this open, it'll
pop open the same ADK web we were just
in. Uh, and we can play around with our
expense tracker there.
>> Thank you, Emit. That was a great demo.
And to recap what we just saw, first we
used white coding to describe an expense
tracking agent in natural language. And
Jenna actually took our natural language
instructions and created a fully
functional ADK agent.
>> Yeah, that's right. It like scaffolded
everything from scratch,
>> right? And then we followed context
engineering practices to create three
key files. The gemi.m MD to describe our
coding standards, a product requirements
do talk which described the feature and
what we wanted to build today. And then
a third project summary.md which
provided an architecture overview. And
these were super crucial to improve uh
the LLN's coding standards even higher.
>> Yeah. And you can imagine that any
subsequent feature you build, you can
create a PRD for it which acts as its
own documentation in the end and allows
the Gemini CLI to develop the entire
feature from start to finish.
>> Right. And finally, we also packaged the
whole agent using a single CLA command
ADK deploy to deploy to Cloud Run.
>> Yeah, that's right. and we saw it deploy
successfully within a few minutes and we
were able to access it on the cloud.
>> All right, with that we'll move on to
our next section which is the inch talk.
So we've looked at the internet and got
some community questions around the
topics we've discussed today. Are you
ready?
>> Totally.
>> The first question is is rag obsolete
with a million token context windows.
Should I still use rag and vector
databases? Yeah, I think that's a good
point and a very good question. So, it's
important to know that when you are
deciding between using some of these
tools, context engineering and feeding
everything into a context window comes
with its own cost associated with it
too, such as, you know, speed and
effectiveness. On top of that, as well,
rag is really good for proprietary data.
If you do have specific data that you
don't need to be sent to the cloud or
have an agent process through, you can
have your database that just stores that
data and then you can pull it and then
you can pull the relevant chunk and then
feed it in as context for the LLM to
answer.
>> I guess in that perspective, we're also
encouraging to use context engineering
on top of the rack data itself because
then you will only need to extract
specific pieces of information rather
than feeding your entire rack data to
the LLM.
>> Yeah, exactly. And we saw that in
practice today when we went through our
demo, seeing how useful even just
creating a summary file can be for our
agent to sift through many different
files. So as context engineering becomes
much wider and your project becomes
larger, you want to ensure that your AI
agent is kind of dialed in on the to on
the specific files that it needs to
pull.
>> Absolutely. All right. Second question
is how to manage context overload?
Should I use techniques like context
summarization?
Yeah, we saw a little bit of this
through like our to token optimization
technique by summarizing all of our
documents or all of our files into the
one project summary. I think that there
are some really good kind of points in
the right direction and research that a
lot of companies are doing and trying to
solve a little bit about how to avoid
inflation of this context window. But I
think the best way to accomplish it now
is to ensure that your AI agent has a
really good direction. Whether that
means pointing it to the right file
yourself or creating that project
summary file that continues to be right
fine-tuned and updated as time goes on
so your AI agent really knows where to
dial its attention into.
>> And would you also recommend using uh
long form memory and persistence to
handle context window and summarization?
>> Yeah, so I think with long-term memory
there is this ability to ensure that
your data is kind of being persistent
and stored. However, if your data
continues to change as time goes on,
that long-term memory, you just have to
make sure that that's also updated. So,
if a file changes, your directory
structure changes, you delete a file, or
you include new PRDs, for example, you
want to ensure that your database is
also updated. So, if you are, you know,
summarizing some stuff using ADK's
ability to interact with Vert.exai's
XAI's memory bank, then you want to
ensure that whatever you're building, if
it's a summary for a file, can be saved
as a memory inside of memory bank that
can then be pulled up afterwards. But I
guess from what you're saying, it looks
like it's a little bit trickier than uh
usually would do because all of this
context and everything is like rapidly
changing for a development environment.
So, um I guess it's trickier then.
>> Yeah. No, I was just going to say, yeah,
totally. Especially if documentation is
getting updated on an ongoing basis or
new articles are coming out, you want to
ensure that all of those things are
being pulled in in real time so that
your AI agent can perform effectively
for the task that you have in hand.
>> And for the next question, what metric
should I use to track context retrieval?
Or in other words, how do I measure the
efficacy of the returned context? Yeah,
I think this one's a bit tricky, but a
lot of people have been using ragbased
techniques to try to achieve this
because of course you're trying to pull
relevant documents or relevant pieces of
documents to inform your AI agent. So
you can use context retrieval or context
precision, both of which try to aim to
identify the correct documents on a
within a spread of available documents.
Have you picked out the seven best
documents that pertain to this specific
topic when there are eight documents
that exist? And for the final question,
what are some of the tips to do for
token optimization other than the ones
we've discussed today?
>> Yeah, I think this one's a little bit
tricky as well and there's some nuances,
but realistically, if you're building
and working within a project and there's
a lot of vibe coding or vibe
creation happening, you will have and
you will end up generating a good amount
of documentation and resources. the
documentation and resources. You're
going to use them to inform your vibe
coding agent, but you don't want to
throw all of those resources in every
time. And that's where kind of drilling
down into the most relevant documents is
super necessary. So whether that be
including a step for your AI agent
before actually processing what you want
it to by selecting the most correct
documents and or just pointing it to the
right documents may be a helpful
approach in fine-tuning or reducing the
search area of all the documents that
you have so that your AI agent can just
focus on the one or two documents that
are helpful for completing its task.
>> Sounds good. Well, looks like that's all
the questions we had for today. Well,
thank you very much Amit. It was really
fun watching you build something
entirely from scratch.
>> Yeah, thanks so much for having me SA.
And I do want to note that Gemini CLI is
open source and has a very generous free
tier. ADK, the development kit that we
worked with today for our agent is also
open source. And finally, Google Cloud
Run has a super generous free tier and
you can deploy all of your applications
for free because it's a serverless
platform.
>> That was a good call out. And for the
viewers, thank you for watching. Let us
know in the comments if you use any of
these wipe coding or context engineering
practices in your own development
workflows. And for more in-depth videos
on context engineering topics like
memory, check out the videos linked in
the description. And until next time,
happy building.
>> [music]"
iFqDyWFuw1c,"Sundar Pichai: Gemini 3, Vibe Coding and Google's Full Stack Strategy","Logan Kilpatrick from Google DeepMind sits down with Sundar Pichai, CEO of Google and Alphabet to discuss the launch of Gemini 3, Nano Banana Pro and Google's overall AI momentum. They talk about Googleâ€™s long-term bets on infrastructure, what itâ€™s actually like to ship SOTA models, and the rise of vibe coding. Sundar also shares his personal launch day rituals and thoughts on future moonshots like putting data centers in space.

Listen to this podcast: 
Apple Podcasts â†’ https://goo.gle/3Bm7QzQ 
Spotify â†’ https://goo.gle/3ZL3ADl 

Chapters:
0:00 - Intro 
0:51 - Shipping Gemini 3
2:44 - Google's decade-long investment in AI
4:27 - The full stack advantage
5:43 - Scaling up compute and capacity
7:32 - Sim-shipping Gemini across products
9:35 - Nano Banana Pro
12:13 - Monitoring launch day
14:13 - Future model roadmap
16:05 - Launch day rituals
18:02 - The Blue Micro Kitchen
21:57 - Future moonshots
23:26 - The rise of vibe coding
26:50 - Whatâ€™s next

Watch more Release Notes â†’ https://goo.gle/4njokfg 
Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Logan Kilpatrick, Sundar Pichai 
Products Mentioned:  Google AI, Gemini",2025-11-25T23:00:42Z,"Google,developers,pr_pr: AI DevRel (fka Core ML);,Purpose: Learn;,Video Type:Podcast;,ct: AIG;;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/iFqDyWFuw1c/default.jpg,https://i.ytimg.com/vi/iFqDyWFuw1c/hqdefault.jpg,1655,public,59567,1847,207,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,iFqDyWFuw1c,"how it feels like Gemini is this through
line across literally every single one
of our products
>> to see it all come together. It is
really special. I was just reflecting in
the last couple of weeks. I think we've
pretty much been shipping something
every day and so it's a great feeling.
>> The Nano Banana Pro moment. I'm sure
you've spent a bunch of time playing
around with the model. People are going
crazy with it, which is awesome.
>> To me, it showed how much latent
creativity people have. people are going
to express themselves and we giving them
the tools to do it the way they're
thinking it in their head
>> and it empowers me like I feel like I've
actually become more creative in like
the way I think about the world because
of these tools.
>> I think in about 5 years we'll be having
breathless excitement about quantum
hopefully like like we having with AI
today.
Hey everyone, welcome back to release
notes. My name is Logan Kilpatrick. I'm
on the Google DeepMind team. Today we're
joined by Sundar Pachai who is the CEO
of Google and Alphabet. And we're we're
sitting here in Mountain View. Gemini 3
has rolled out. Nano Banana Pro has
rolled out. And the reception's been
super positive. So I I feel like do you
want to sort of frame the moment of this
progress to get to today where we sort
of have state-of-the-art models across
not actually just Gemini and Nano Banana
Pro uh but VO and some of the other
music models and stuff like that across
the board. It's like more more and more
soda the longer the time goes on. Um so
yeah, do you want to frame that moment
for us?
>> Well, first of all, uh great to be here.
Uh it's been a phenomenal week. I would
say you know when you when you're
working on stuff inside you kind of
visualize the moment when you can
actually put it all out and when you
work on products there's nothing more
exciting than that and and that's what
this week is but I think it's based on a
foundation over many many years and of
all the deep investments we built and it
was always clear to me uh you could see
the pace at which we were making
progress but to see it all come together
it is really special I was just
reflecting in the last couple of weeks.
I think we've pretty much been shipping
something every day and so it's a great
feeling.
>> It is. Yeah. Yeah. I I remember back to
um you and I were talking probably a
year and a half ago and I was
complaining about something. I'm sure I
was complaining about something and you
were like uh something to the effect of
like pushing me to sort of see this
long-term perspective. And I'm curious
like I think obviously the to me the
story of like getting to this moment
that we're in now with state-of-the-art
models and all this infrastructure to
scale up Gemini across Google like is
this long long-term perspective and I
think especially in this hyper
competitive moment like what are like
how how are you keeping that long-term
perspective and like because it feels so
much like a like a rat race right now in
some sense to like keep hill climbing 1%
on all these leaderboards but um
obviously that long-term perspective is
super helpful. Now, I think I've always
u you know, I always force myself, you
know, you're obviously in the moment. Uh
we work in an industry where you move
fast, you want to iterate fast. Uh and I
really enjoy that, but being able to
pull back and and making this long-term
bets and being very focused over over
that time period on a long-term bet, I
think it's always super important to do.
I think you know obviously uh in 2016 I
wanted the whole company to be AI first.
A lot of what prompted that moment was
2012 was Google brain the famous cat
paper which had breakthrough in image
classification.
2014 we brought Google deep mind in
January 2016 was the Alph Go moment
>> and now people many people didn't notice
in May 2016 we announced our first TPU.
>> Yeah. So it was clear to me in 2016
seeing all that we are about to go
through another platform shift and and
that was the uh uh bet on a full stack
bet on setting up Google to be a AI
first company you know and since then we
have been making a lot of progress there
so many breakthroughs coming from Google
including transformer we were putting it
in our products in BERT and mum and
making search better launch Google
Photos and so on. But obviously with the
generative AI moment I I realized the
window was even bigger like you know
that that people were ready to use the
technology at scale.
>> Yeah.
>> Consumers, developers and so on. So how
do you respond to a moment like that?
And you know for us it was about we kick
kicked off Gemini as a project and
across Google brain and uh Google deep
mind and as part of that then deciding
to bring the teams together in as Google
deep mind we really ramped up our
investments in our infrastructure
data centers TPUs GPUs and so on. it is
then uh you know getting getting the
company to move at a faster cadence
right you now have the technology and
once the GDM team started shipping
Gemini
uh and you know you can talk about the
series of Gemini milestones we have
worked through and it's been great to
have you
>> for a lot of the journey driving it too
and and and now how do you make sure you
manifest it in all our products there
many many products which touch billions
of users right how do you can search to
iterate with the power of what these
models can do
and and so that's been the journey but
you know you know you can step back and
understand that framework and it's so
exciting because for the first time you
many when you have a full stack approach
each layer when it innovates it flows
through all the way on the top
>> this is what I tell people about
pre-training I'm like the fact that
pre-training for deep mind is working so
well in the Gemini models it's like
post- trainining and RL is like this
accelerant of the underlying capability
And I feel like our infrastructure is is
a similar story.
>> Absolutely. You make your infrastructure
better. Uh you make the models better at
pre-training, post-raining, uh test time
compute, uh where have you or how do you
take those capabilities and then
manifest it in products, right? How does
nano banana show up in your products?
Generative UI in search with AI mode,
right? So you're expressing it at all
these layers and not to mention being
able to take that and give it give it to
developers who are then innovating on
top, right? And that's what creates this
multiplicative effect uh and and it's
always incredibly uh thrilling to watch.
But you know always had this long-term
vision of uh how we can do it. Some of
this took time because because we have a
full stack approach when we had to
respond to this Gen AI moment like I
remember we were short on capacity. So
when we um you know we had to invest to
ramp up all these things to get it to
the scale so that that so you had a
fixed cost around it.
>> Yeah. So if you were on the outside, it
looked like we were quiet or we were
behind but we were putting all the
building blocks in place and then
executing on top of it. We are on the
other side now, right? And which is what
uh you can see the pace at which teams
are moving forward.
>> Yeah, it's uh it's been incredible to
see this. Um you mentioned this like
Gemini showing up in all of our products
and I think I was talking to Josh and
Tulsi about this and Corey about this.
Um, I feel like the actually the
challenge for some of these launches now
is is SIM shipping and and maybe not
even from a product perspective, but
capacity and the sort of how do you make
sure that the models show up really well
across all these different product
experiences. um I feel like has
introduced a new like we've almost Cory
made the comment something to the effect
of like we've figured out how to do the
models and obviously there's more that
we need to do but like deploying them
across all of Google's product surfaces
is extremely hard and it it gets me to
this um and I I sort of had this
realization at IO this year of and I
want to gut check this with you because
I don't maybe maybe you have a different
perspective but historically like other
than maybe your your Gaia or your Google
account there hasn't been there wasn't
this through line across like this whole
suite of different products that Google
has. Everything from cloud to Whimo to
search to everything else to Gmail. Um,
and now it feels like Gemini is this
throughine across literally every single
one of our products which is just such a
it it feels like there's something magic
there uh that is happening. I don't
know. I don't know what your reaction is
to that.
>> I think I think Gemini I know it's a
good observation. I think Gemini to me
is like a much more
uh it's a very clear manifestation of
what what is the AI first strategy.
>> Yeah.
>> Because now you have something tangible
like Gemini which people can uh
understand and Gemini you're right
improves everything from search to
YouTube to cloud uh to VHO. What I loved
about the Gemini 3 launch you talked
about SIM shipping. We shipped it across
many of our products.
But it was fascinating for me to see,
you know, on X, it could be Copilot or
Replet or Figma, you know, people all
coming together. They're also sim
shipping.
>> Yeah. Yeah.
>> Right. And and to me, that's innovation
at scale, right? It's not just us. Uh
it's other other companies in the world.
Uh it's been extraordinary to see.
>> Yeah. It's awesome. One of the other
threads and obviously the Nano Banana
Pro moment. Uh I'm sure you've spent a
bunch of time playing around with the
model. I've the people are going crazy
with it which is awesome.
>> I have to keep asking did we improve the
productivity of the world or or is it
like I'm is it a net progress or not?
The infographics seem amazing.
>> They do.
>> And I think as we move beyond the fund
stage, you know, I just saw OnX I think
Ben Budgerin had tweeted this
infographic of his scorewave analysis.
So you know actually made me look
through that thing to try and understand
it. One of the things PowerPoint many
years ago unleashed is people kept
making more and more slides
like you know always used to so much
information and it kept expanding
like maybe with Nano Banana Pro we are
back to a phase by which we can kind of
compress it and give it to the world in
a more digestible way. Yeah, that that
was going to be exactly where I was
going. Like there's something I've
historically been personally skeptical
about like how useful a lot of the gen
media models would be for the world. And
like obviously it's useful from an
entertainment perspective, but it felt
like Nano Banana Pro crossed the chasm
specifically with infographics and
actually grounding with Google search in
that um of like actually like I I
believe I I can see very clearly how
this is part of Google's mission of like
organizing the world's information
information and actually making it
universally accessible to people with
those infographics. Like it's just it it
blew me away. like it was so interesting
and I feel like it's a great reminder of
like we're going to see those use cases
and I think the Netto Banana team if I
remember correctly from one of the
pieces of content we were doing they
were like yeah we weren't even like it's
not like they were trying to make
infographics work really well it just
happened as the model got really good
and and text rendering capabilities
improved so much which is fascinating
>> fascinating the other thing shows me is
how much latent creativity is there in
the world
>> yeah so one of the other uh beautiful
things we're witnessing is I think like
people are going to express themselves
elves and we giving them the tools to do
it the way they're thinking it in their
head right and so I think I think that I
think if otherwise we've been
constrained by the tools in front of
people you may not have realized it but
we are creating more and more expressive
tools and they're more and more
accessible to you know more and more
people and so watching that you know
it's incredible to see as well. Yeah, I'
I've got another question about that in
a second, but one of uh I'll give Tulsi
credit for this. Tulsi had suggested
when we were talking yesterday to ask
you about this uh because she was
curious what um sort of like as you see
these launches happen in these like
large tentpole moments for Google, what
is your what is your success barometer
for these moments? Is it like the
reception online? Is it like you know
what does the adoption look like on day
one? Or like how are you measuring like
is this thing actually moving the needle
for Google? You know, look, I'm uh I'm
pretty active on launch day trying to
understand what's working. I'm looking
for feedback. Um both, you know, an
example, I'm on X trying to understand
like C is people average users how
they're experiencing the product. I
probably ping people back saying, look,
you know, this is a valid point, we
should address it. So, in some sense,
I'm looking at that trying to assess it.
You know, it's clear to me teams
internally are using Gemini itself to
kind of collect collate. We have great
dashboards and and so I try to take in
across a variety of sources. I'm one of
one of those people. I need to feel it
firsthand, right? So I get reports uh
but I'm out there trying to understand
how people are uh using it, what they
are posting, right? And you know and and
I think that is important. But I walk
walk over to some people who all have
these big screens with multiple
dashboards looking at QPS and you know
understanding how the usage is worried
about capacity but all that gives you a
real sense of what people are uh uh
doing saying but that's how I do it.
It's combination of monitoring stuff
online, talking to people, walking
around, sitting down with people. You
know, I I want to get particularly the
first day. It really helps me get a
sense of what's working, what's not
working well.
>> Yeah. I feel like you can feel the
excitement in the in the office right
now, too.
>> I can't walk out anywhere without some
version of some banana. There's a
million I don't know who did that, but
uh kudos to the events or facilities
team for somehow bringing 100,000
bananas into this building and and
making it happen. The the exciting part
is this is just sort of the first
chapter of or the first page of the the
Gemini 3 chapter. We don't have Flash
yet. We don't have any any of our other
sort of models in the in the 3.0
category. We shipped Gemini 2.5 Pro and
actually as I was looking at a bunch of
the benchmarks like even 2.5 2.5 Pro is
not soda on everything. Um obviously
competitors have caught up but even 2.5
Pro right now is still like bestin-class
at a bunch of capabilities and sort of
taking it a step further with with 3.
Now 2.5 Pro was Google IO. Yeah. And and
you know and you could sense that uh it
was a big step up. I think one way I
feel great is and you know MS Corore and
team the GDM team is on a good cadence
right so we are uh kind of every 6
months or so so pushing that frontier
and it gets harder right because you're
you are yeah you're right 2.5 pro is a
very good model so to kind of clearly
take a meaningful leap from that I think
it's hard but that's what uh you know uh
that that's what makes it exciting
progress. I know you're always excited
about flash, which is working on and is
coming.
>> You're excited about flash because you
it allows you to serve more people.
>> Yeah.
>> And in that parade of Frontier, it
really uh makes a difference.
>> So, I'm excited for 3.0 Flash. I think
it's going to be a very very good model.
Uh might be our best one yet. Uh right.
And we'll see. uh what's great is teams
are internally um you know our
pre-training teams are thinking about
the next version and and so
this culture of uh uh you know
relentlessly uh innovating and and and
shipping uh I think I think makes this
moment special uh special and you know
it definitely
feels like there's going to be a lot of
exciting progress going back to the full
stack at all layers of the stack uh as
we go into 2026.
Yeah, I have a goofy question because
because Josh sort of uh inspired me with
his goofy answer about launch day
rituals uh
>> with his dried Cheerios.
>> It was his dried Cheerios, which is
weird. And I need to keep putting Josh
on blast because I think it's funny and
it's uh
>> I'm like, next time I see Josh, I'm
going to be walking around with a carton
of milk to to
>> to help him so that he doesn't have to
eat dried Cheerios anymore. Do you have
any weird interesting uh launch day
rituals or is it is it just trying to
trying to make it through the day? I
it's um
you know my the the normally I mean the
morning ritual I almost always have is I
kind of wake up and catch up on what's
happening in the world.
>> Yeah.
>> Right. I consume in fact I don't even
check like Google email because the way
I think about it is if something
interesting has happened about Google
it'll be there in the news anyway. So I
kind of like try to step back and take
in the news. That's what I do. So launch
day ritual becomes our products and you
know when we are in the news so trying
to understand uh to your earlier
question about how it's work. So that's
my uh main main routine and uh I kind of
try on launch days to have a bit of a
less structured day so that I can go
spend it time you know I I love walking
to the teams which worked on the
products maybe seeing them seeing how
they feel about what they shipped. So
that engagement to me matters a lot.
Yeah.
>> Yeah. I I have another sort of uh
question along this line which is I have
a interesting observation. I think
Dennis and others have talked about this
maybe internally but there's this uh
micro kitchen inside of the creating
canopy office where a lot of the sort of
action is happening from a deep mind
perspective and every time I'm there it
makes obviously Google is big and huge
and global and there's all this stuff
happening. the the MK that blue MK makes
Google feel small uh to me and I'm
curious if there's like something
interesting there which is like how you
it feels small and intimate um and I'm
curious how you've
>> oh it it so reminds me of early Google
um you know obviously you know I I I I
go there quite often and you know it
could be you know you have Sergey there
and you have people like I mean Jeff and
Sanjay P program still and they're
making their uh espressos.
>> If people nothing encapsulates our
culture better than than watching the
precision with which people make their
espressos in that micro kitchen, right?
I would never dare make an espresso
there, right? you know and I I know a
lot about how to make good espresso but
I feel a bit intimidated uh you know
amongst that group but you know no m
korai oral just last week alone walking
around you know it's very dense in
talent people are exchange constantly
people are visiting there's a very uh
great uh exchange of ideas and so I love
that you know kind of reminds me of uh
how the company used to be in uh in our
early days um you know some of our
serving team uh at Emma and people are
there and you know that's where when I
when I mention I'll probably go look at
the QPS it's you know I'm hovering over
these people's screens trying to
understand what's going on so it's
definitely a favorite part of uh I think
how the company works
>> yeah my my Google feature request is we
need to we need to somehow remake these
MKs across across all the PAs or
something like that I don't know how
that happens
>> I mean you know there are like you know
I uh like you know uh there are other
teams teams which which have versions of
these and I think and it really helps
pull people back into the office uh
because you realize the value for when
you're there that exchange of ideas you
can still go back and have focused time
uh wherever you're working but you know
that that moment really helps a lot I
think
>> yeah a lot of this AI story that you've
talked about so far has been like us
making these really really long-term
investments um and sort of setting the
company up for success l like 10 years
ago um and And I'm curious how you think
about like right now or obviously and
and the the bets were right like cloud
cloud has worked out really well.
Whimo's worked out really well. Quantum
hopefully will work out really well.
Just announced a bunch of other like I
the quantum stuff goes over my head but
I keep vibe coding experiences to try to
understand what they're talking that it
was one of the ways in which I was like
testing
asking Gemini 3 to help yeah
>> understand a layer deeper on these
topics is just fascinating. Yeah, it's
it's uh it brings anything to life,
which I'm happy we went with that uh
that tagline. But how do you think about
like what are is it just like
infrastructure that's like this next
10-year bet or like do we like we've we
know AI is the thing so that's what all
the eggs are in that basket now or like
I'm curious how you think about like
what are the next you know future facing
10 years look like and where we're
making the bets now to set ourselves up
for that ne next layer of success. Oh,
look, I think it's always important,
right? You know, going 10 10 years back,
it was the bet on AI, you know, in a
deep way, in a full stack way. It was a
bet on saying we're going to build other
big new businesses, diversifying the
company, bet on YouTube, betting on
cloud. Uh Google was as cloud native as
a company you can ever imagine, but we
weren't fully providing it outside. So,
that was a deep big scale bet on cloud.
And you know, Whimo, these things take
time. And Whimo is a long bet and I
think we are now seeing that inflection
point around around Whimo. There's
always these future bets, right? Quantum
computing is an amazing bet. I think in
about 5 years we'll be having breathless
excitement about quantum hopefully like
like we having with AI today.
But I'm constantly thinking in that time
frame. One example of this is project
suncatcher
>> two weeks ago where we announced we're
going to build data centers in space.
Obviously, it's a moonshot. Some of it
looks crazy today, but you know, when
you when you truly step back and
envision the amount of compute we're
going to need, it it starts making sense
and it's a matter of time. And so, how
do you make progress on that? You work
back work back and have 27 milestones
and and get get underway. So, in 2027,
hopefully we'll have some TPU somewhere
in space. Uh maybe we'll meet a Tesla
Roadster, which is going around there,
too. So, it'll be fun. But that's an
example of the kind of uh long-term
projects you want to undertake and do
isomorphic with alpha fold uh and wing
with drone delivery.
We are cooking up exciting work on
robotics. Uh so you know so you take
that long-term view and keep keep making
progress. When I saw TPUs going to
space, I pinged uh I pinged Demis and I
said we should we should fund a moon
rover and put Gemini on device and then
have have it sort of explore the moon.
Uh great it'd be a great marketing
campaign even if it's not uh
scientifically super useful. So
>> who knows maybe the project is underway
somewhere.
>> Yeah, I'm I'm sure it is. Um this you
mentioned this thread before which is
the capabilities continuing to go up
sort of like uh raising the floor for
everyone to go and and be creative and I
actually personally feel this way like I
don't feel super like artistically
creative by default and yet I can like
tackle these tasks that historically I
would have had to have been. Um and it
empowers me like I feel like I've
actually become more creative in like
the way I think about the world because
of these tools and like not being
worried about not being able to to do
something. I think vibe coding is this
like massive one and there's this like
moment where this force that is like one
of the most economically powerful things
in in history which is creating software
and being able to code is now in reach
of so many so many more people. Um and
I'm curious uh how you obviously you you
do vibe code sometimes. Um I'm curious
how you've been thinking about that
moment of like AI builders being able to
more than just like traditional software
engineers be able to build stuff. You
know what an exciting uh moment you know
it's almost like how the internet you
know suddenly blogs appeared many more
people became writers if you will right
and what YouTube did uh many more people
uh became creators
and you know you can feel that happening
on the coding side even within Google
just a sharp increase in in a set of
people who have submitted their first
cls
Right? And uh and it's because of these
tools that are making it more
accessible, right? You know, maybe
you're a product marketing person, you
have an idea. In the past, you would
have described it.
Now, maybe you're kind of vip coding it
a little bit and and and showing it to
people.
>> So, you can tangibly see that uh you
know, come to work. I was just speaking
with someone on my team who doesn't code
but was some trying to teach a son uh
Spanish conjugation and just like you
know one-shoted in Gemini 3 a animated
HTML page to describe uh this uh this to
his uh son. See, when you hear stories
like that and and that person is in our
comm's team, right? So, you can you can
kind of see how people are beginning to
do things and so it's very very
promising. uh you know in the limited
time I have and I played around with it
it's almost like you know just not just
swipe coding but just these IDEs now
it's making coding so much more
enjoyable right like of course I'm not
doing large working on large code bases
where you have to really get it right
the security has to be there so you know
that you know those people should weigh
in but
>> but I definitely think I feel things are
getting more approachable
it's getting exciting again and and the
amazing thing is it's only going to get
better now. Something I always used to
tell about Whimo when whenever people
would talk to me about is remember this
is the worst Whimo will ever drive,
right? It'll only get better.
>> Yeah. Yeah.
>> It's a version of all these tools which
we are doing. Um you know uh VIP coding
with Gemini 3 and AI studio uh you know
it's both amazing to see and and it's
the worst it'll ever be.
>> Yeah.
>> Right. Both are simultaneously true. And
so I mean it in a sense that you're
going to see a lot of progress ahead. So
I think it's definitely exciting time
and can't wait to see what people out
there in the world come up with it.
>> Yeah, it is awesome. I think my my last
question is like what's next? What can
we be excited for? Um lots of cool
things in the pipeline but anything top
of mind?
>> I think some folks need some sleep. Uh I
I probably you know hopefully the team
for the teams all of us uh you know uh
get a bit of rest but
>> look I think I'm excited for the uh
Gemini road map. I'm excited at how it's
coming through all our products uh
products. We're also shipping new
things, right? I I I love flow. I've
been playing around with flow uh uh
notebookm
uh you know it has a passionate growing
community and and uh it's amazing you
know seen journalists like work on it,
people doing their PhDs like really
doing all the research in it. Um so uh
there's a lot more to come.
>> I'm excited. Thank you Sundar for
sitting down. Um and thank you everyone
for watching uh release notes. We'll see
you in the next episode."
YwYu1JWvn-8,What is a reliable system? An SRE perspective,"Reliable systems are a way to respect the user and deliver systems that degrade gracefully. A reliable system also anticipates failures and actively defends against them in any other systems it depends on.

Learn more about SRE! â†’ https://goo.gle/sre-resources 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Christine Rafla",2025-11-25T17:51:10Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Campaign: BBDS;,Series: AASRE;,type:G4D SV: Educational ;,ct: AIG; ;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/YwYu1JWvn-8/default.jpg,https://i.ytimg.com/vi/YwYu1JWvn-8/hqdefault.jpg,63,public,3172,52,3,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,YwYu1JWvn-8,"As an S sur, what does having a reliable
system really mean? And how do we build
one? Building reliable systems is like
being a firefighter. You get to rescue
your users from catastrophic failures.
Suppose you have an app that people use
to share cat photos, but the storage
system decides to take a nap. Many apps
will encounter a failure mode and well
fail. You, the user, will see a stag
dyno, a 404 page, or a spinning wheel. A
reliable system anticipates failures and
actively defends against failures in any
other system it depends on. The reliable
system might have a cache or it might
have an alternative path to a different
instance of the storage system. It might
show you an image of a catnapping with a
clear message to users saying, ""Taking a
catnap. Be back soon."" Reliable systems
are a way to respect the user and
deliver systems that degrade gracefully.
For more about designing reliable
systems, see our website
surre.google/re. Google/resources."
fBshJ_ps6WI,How to build reliable AI Agents?,"Join Ivan and Aparna Dhinakaran, CPO of Arize AI, as they discuss the challenges of moving AI agents from prototype to production. This tutorial demonstrates how to integrate the Google's Agent Development Kit (ADK) with Arize Phoenix to enable deep observability, tracing, and evaluation for your agents. Learn how to instrument your ADK agent with OpenTelemetry standards, visualize live traces to debug logic errors, and run evaluations to ensure your agent is reliable at scale.

Chapters:
0:00 - Introduction
0:40 - Why observability matters for agents
1:35 - Callbacks in ADK
2:05 - Arize Phoenix
2:45 - The Financial Advisor agent
3:16 - Installation and setup
4:30 - Demo: Live tracing
5:20 - Error analysis and debugging
7:09 - Scaling debug with evaluations
8:46 - Wrap up

Resources:
Learn more about the Agent Development Kit â†’ https://google.github.io/adk-docs/
Check out the ADK Samples repo â†’ https://github.com/google/adk-samples

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Products Mentioned: Google ADK, Google AI",2025-11-25T17:01:15Z,"Google,developers,Google ADK,Arize Phoenix,AI Agents,LLM Observability,OpenTelemetry,Python,Agent Development Kit,Arize AI",28,en,en,https://i.ytimg.com/vi/fBshJ_ps6WI/default.jpg,https://i.ytimg.com/vi/fBshJ_ps6WI/hqdefault.jpg,622,public,2290,103,5,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,fBshJ_ps6WI,"Hi, I'm Ivan, a software engineer at
Google working on the agent development
kit. Today, we're tackling one of the
challenges in bringing AI agents from a
prototype to a reliable production
application. To do that, I am thrilled
to be joined by a special guest, the CPO
and co-founder of Arise AI. They are
leaders in the Gen AI observability
space, helping teams across the industry
ensure their AI systems perform reliably
in the real world. Welcome, Aperna.
>> Thanks for having me. It's definitely a
topic we're passionate about at Arise.
Today, we'll be walking through how to
observe and evaluate AI agents.
>> Now, before we dive into the code, I'd
love to set the stage. From your
perspective at Arise, why has
observability become such a critical
top-of- mind conversation for every
Genai developer?
>> Great question. For decades, traditional
software had clear deterministic paths.
You could use logs and step through
debuggers to follow the logic, but LLM
powered agents are fundamentally
different. They're non-deterministic
systems with complex emergent behaviors.
These agents can just fail in
unpredictable ways. And you're no longer
just debugging code. You're debugging
tools, prompts, reasoning, and more.
Observability provides the
instrumentation to see the agents
behavior. So you can move from I hope
this works to I know how this works.
This is foundational practice for
building trustworthy AI agents.
>> Now, that's perfect framing. That's also
exactly how we built ADK from the ground
up. We provide plugins and callbacks so
that developers can inspect all the
actions by the agent such as LLM and
tool calls. We were designing for ease
of understanding the what and the how.
Models, tools, and sub aents all have
built-in before and after callbacks,
making them easy to see and control, so
developers can focus on the unique
business logic that matters, not the
boilerplate. And that's where our open-
source tool, Phoenix, will help
developers know more about how their
agents operate. It can run locally on
your machine as well as in the cloud.
And it lets you trace, evaluate, and
iterate on your agent. Teams use it to
debug where their agent is not
performing well. Evaluate both offline
and online, and iterate in minutes, not
hours. So, let's make this tangible. I'm
going to walk you through the entire
developer experience from the agent
built with ADK to the few lines of code
needed to integrate Phoenix and finally
to running the agent and analyzing the
live traces in the UI.
So let's pull up the financial advisor
agent built with ADK. Its purpose is to
help users research stocks. It's built
with a multi- aent system with specific
sub aents for stock and risk analysis.
These sub aents have tools for fetching
stocks and PE ratios. You can actually
check out this example online on the ADK
samples GitHub repo. Now let's say I'm
iterating on this agent and now I want
to see how it's doing under the hood.
How can I use Phoenix for that?
>> Great question and it's actually
surprisingly simple and that's by
design. ADK is a supported integration
with Phoenix. First teams just need to
create an account in Phoenix and then we
just need to instrument our agent with a
few lines of code that you can see in
our native integration here. Uh we just
need to import Phoenix which is over
here Google ADK as well as open
inference which is just the open
telemetry conventions for gen. Then we
configure a standard tracer provider to
trace the application [snorts] and we
just pass this tracer directly into the
ADK agents constructor when we
initialize it. Now we hop into Phoenix
where we can actually see the traces
show up in the platform.
>> I see you're using open telemetry which
is a widely adopted standard. Do you
have an opinion on why building an open
standards is important?
>> Yeah, I think it's super important. Open
standards create a common language for
tools to communicate. We designed open
inference to be OEL compliant for Genai
applications. And because ADK even emits
open telemetry traces, you're able to
send that data to Phoenix easily. Well,
let's show a demo.
So, here we have the ADK financial
advisor agent and I'm going to kick this
off. And the agent says, you know,
provide the market ticker symbol. So,
I'm going to go ahead and ask what's a
good trading strategy for Apple, Google,
and Microsoft. So, the financial
coordinator, it looks like, kicks off a
couple different data analyst agents,
and the agent kind of kicks off some
work to go and answer my question. So,
while the agents working, what's
actually happening under the hood is
that all of the actions it's taking is
getting traced and streamed directly
into Phoenix. So, I can go in here and I
can see the actual live traces coming in
as the agent's kind of doing its work.
Well, that's great. I'm going to try a
different question right now. I'm going
to ask um what's the stock price for
Google? Okay, looks like the agent
responded, but this time it just said it
cannot fulfill that request. H
>> h I I'd love to inspect what the model's
thinking under the hood to see where it
went wrong. My first instinct would be
to add a print statement in the after
model call back and that way I can log
what the model's thinking.
>> That's exactly what Fenix enables for LM
applications. It shows all of the steps
that the agent took. So we can actually
go in and compare the two different
queries side by side. Let me go in and
show the first one. And you can see here
that very clearly all of the different
agent calls that are happening under the
hood and the tool calls, the LLM calls,
all the agents. And I'm going to compare
that to the one where it actually didn't
answer the question at all. And here
basically the agent called the LLM, but
there wasn't actually any subsequent
tool call. Mhm.
>> This process that we're actually doing
right now where we're going through,
we're looking at and inspecting the
traces, this process is called error
analysis. And what's really interesting
is that, you know, there is no bright
red error flag on the screen. Actually,
the code ran without any exceptions. The
bug is actually in the logic. We see the
agent, you know, called the LM, but it
didn't call the tool call. But you know
it's not really a a true error in the
actual execution of the application.
>> Mhm. Now as these applications process
more data and you get more traces, the
developers might not have time to go
through each of them. So how would you
recommend teams debug their agents at
scale?
>> Yeah, great question. I mean this is the
the biggest reason for evals. Evals help
you understand how the outputs of the
application that you're building
actually are. And there's typically two
types of evals. There's offline evals
and then there's online evals. Online
evals are critical once the application
goes live into production. They help
evaluate the output and identify where
there could be improvements. Offline
evals are used during experimentation.
So you can think about it almost like
unit testing. You use offline evals
before the applications shipped. So what
I'm going to do today is, you know,
because our agent's live, I'm going to
go ahead and run some online evals and
evaluate the outputs. So let's go ahead
and do that. So today, what we're using
is actually the answer correctness eval.
>> It's using LM as a judge to evaluate the
output of the application to see if it
actually answers the user's question.
Yeah, I see that we have many rows of
traces and we have labels depicting
whether the judge thought the answer was
sufficiently answered by the agent
according to the user's question. Let's
look at one of the incorrect ones.
Looks like that Phoenix has provided an
explanation which is LLM generated that
tells us the agent was unable to
retrieve the stock ticker data for some
reason. Now perhaps it's missing a tool
to do so. We won't fix it now, but at
least we have a good starting point.
With Arise, what we're really seeing is
that by adding just a few lines of
instrumentation code to your agent, we
get a production grade debugging
experience. I think this collaboration
highlights something important. ADK is
open source and extensible, so projects
like Phoenix can build seamless firstp
party integrations,
>> and we're really excited to be a part of
it. A great framework makes it easy to
build and a great observability tool
makes it easy to iterate and improve.
The two really go hand in hand. This
combination is actually how teams go
from a cool demo to a reliable
productionready AI application that they
can confidently ship to users. So to
bring it all home, Google's ADK gives
you a powerful scalable framework for
building agents and Arise Phoenix gives
you the critical visibility you need to
debug, iterate, and perfect them. You
can run Phoenix locally or check out
Arise AX or Google Cloud Trace. To try
everything we did today for yourself,
check the link in the description to the
ADK documentation, the open source
Phoenix project on GitHub, and the
Financial Advisor agent code we use
today. We can't wait to see what you
build. Phoenix is a communitydriven
project, so please give us a star on
GitHub, open an issue, and let us know
what you think.
>> Thank you so much for joining and
sharing your expertise. This was
fantastic, and thank you all for
watching. Be sure to tell us in the
comments what you're building next.
We'll see you in the next one. And until
then, happy building.
>> Happy debugging."
hk6gwiZmSWA,Nano Banana Pro: Hands-on with the Worldâ€™s Most Powerful Image Model,"Introducing Nano Banana Pro, a powerful model built on Gemini 3 Pro, designed to enhance text rendering, infographics, and structured content generation. Tune in to learn about Nano Banana Proâ€™s advanced visual reasoning and multi-turn generation capabilities, and how this next-gen tool enables complex image edits and real-world applications. In this episode, we discuss how user feedback and continuous benchmarking drive model improvements, ensuring a superior experience for developers.


Listen to this podcast: 
Apple Podcasts â†’ https://goo.gle/3Bm7QzQ 
Spotify â†’ https://goo.gle/3ZL3ADl 

Chapters:
00:00 - Introducing Nano Banana Pro
02:00 - Enhanced world understanding
04:59 - Advanced text rendering
05:49 - Gemini 3 Pro's influence
09:30 - Multi-turn & infographics
14:04 - Text rendering comparison
16:26 - Multilingual text support
18:22 - Infographics for learning
24:00 - Multi-image input
26:38 - Resolution & fidelity
30:07 - Advanced editing & style
32:09 - Practical use cases
35:26 - Future outlook & thanks


Watch more Release Notes â†’ https://goo.gle/4njokfg 
Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Logan Kilpatrick, Abhishek Sinha, Robin Strudel, Yilin Gao, Sherry Ben
Products Mentioned:  Google AI, Gemini, Nano Banana",2025-11-25T01:39:39Z,"Google,developers,pr_pr: AI DevRel (fka Core ML);,Purpose: Learn;,Video Type:Podcast;,ct: AIG;;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/hk6gwiZmSWA/default.jpg,https://i.ytimg.com/vi/hk6gwiZmSWA/hqdefault.jpg,2185,public,8894,239,31,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,hk6gwiZmSWA,"- Hey everyone.
Welcome back to Release Notes.
My name's Logan Kilpatrick, I'm
on the Google DeepMind team.
Today we're talking about Nano Banana Pro.
We're here in Mountain View
with Robin, Yilin, Abhishek, Sherry,
folks from the Nano Banana
team, the the Gemini image team.
I don't know what our internal
code name is for for the team
'cause there's 50 different names.
But yeah, the sort of high
level news is we launched the
Nano Banana Pro model built
on top of Gemini 3 Pro,
which went super exciting.
Obviously folks loved the
original Nano Banana model.
You've got the swag, which
everyone pings me on Twitter
and is like, how do I get the swag?
I'm like, you can't get it exclusive.
Hard to come by but Robin
maybe do you wanna give us the
sort of like high level of for folks
who maybe like missed the
context of this model launch
and then we'll dive in and
actually look into a bunch
of demos and and examples?
- I think so.  Nano Banana
Pro is particularly amazing
at tests like text rendering,
infographics actually like
generating structured content.
I think like we have
plenty of good examples
where we can do tool gold
and actually like generate
content while doing a search on
the internet and yeah,
on visual resume also it's super cool,
like we can do multi turn
generation, you can do edits
where you take multiple people in
and blend this into an image
and yeah, we're excited to
showcase some examples today
live of the, of the model.
- I'm excited too. I, I wanna
make sure we also talk about
this text rendering part
because when we had, we did the,
an episode about Nano Banana one
or the original nano
banana model and Kaushik
and others were talking about how sort
of like text rendering
is actually like one
of the main things from like
an overall image quality,
a model quality perspective
that like we can use
to benchmark the model.
So I'm curious like if, I
don't know if we have like a
public benchmark that we put out
or that there's something we track
but I'd be interested here
but Abhishek, do you wanna show
us some examples to sort
of kickstart the,
how this model is better
I think is the top level
question than the original That bit?
- Yeah, yeah, so we can
drive into multiple examples.
Like let's start
with one common example
which people have been trying
and let me compare against
nano a side by side comparison.
Let me just, so people have
been trying this wine glass
example where you want
the wine glass to be full.
It's a, it's a difficult example
because as you can imagine
like most of the wine glasses,
like the pictures which people take
or the pictures which are online,
like rarely you see like
full wine glass like
because it's served with something
or different And also
like the time, like most
of the time clock images like
people see like always like 10
10, that's the standard set
time which is there in the clock.
And so whenever you ask
these image generative models
to come to show this wine glass
or the clock example,
it always show either shows a
wine glass which is not full
or a clock which is like not
showing the time which you want.
It's either 10:10 or some standard time,
which is mainly a data issue
because that's what exists
mainly in the internet.
But to a surprise like
it's not like something we
optimized our model for.
We trained our model
and then we started seeing
that people were actually
trying out this prompt
and it works like surprisingly well.
You can see in this like
even on my first try like
- Can you clicking on these so
we can see them in full screen?
- Yeah,
- Try to make sure it's really full.
- Yeah, it looks full. Yeah,
it's actually very full.
You can see the time on the clock.
It's also like perfectly depicting 5:30
whereas in the right hand
side if you see like the Nano Banana
output which is it's not
showing the glass which is full,
it's also not showing the
clock which is like accurate.
So this is like some
example of like better
world understanding I would say
just because it's been, just
because it's strained on Gemini 3,
which is a much better
multimodal understanding
that really helps us for image generation.
Whereas all this better world knowledge
comes up to this model.
So this is one particular example
- For the clock one when we
were  pressure testing our
model, not only we can
generate the the clock
with the targeting time but we,
but we can also editing that.
So it's I think with
this joint like text image
and word knowledge understanding
and then also bring that knowledge into
editing the target image.
That's really cool. We
have never seen that
high successful rate for Nano Banana.
- Yeah, that's super interesting.
I wonder, I've been seeing a bunch
of these like new
practical use cases enabled
by the original Nano Banana
model and Veo coming together
and I feel like with like even
more high fidelity editing
you I'm, it'll be
interesting to see like
how many more new use
cases come up with that
but Abhishek check you we
have another example of
- Yeah, so I just took
an example image like I,
I asked a model to show all
the consonants in yellow,
all the vowels in red
and you can see like it perfectly
like generates this image
where all the vowels are
like described in red,
all the consonants in yellow
I can keep like adding
and it's kind of suggesting
that the model is actually able
to reason like it's not
in like an easy prompt.
You have to really dive
into each character
Yeah and I think the fine grain capacity
of like following the text prompt
and the image content it both,
it's both showcase on this example
and also on the wine glass example
where it doesn't output
just a generic image
but really reason about like
what's the specific user input
and trying to match
exactly like the full wine,
the full wine glass for example instead
of like just outputting
a an average wine glass.
- Yeah, I I actually have a,
before we look at another demo,
I have a broad question which is,
and Yilin maybe you can answer
this one, how, how much of
the, the sort of like capabilities
that we're seeing here are
just like coming outta the box
because 3.0 pro is a super
smart model versus like is
it just that like that gives
you all a better baseline
and then it's just like much
easier to hill climb on stuff
or is it like actually
it's, you know, some
of these things were free versus
some of them you really had
to go deep on or actually
some of them it was harder
because 3.0 Pro has its own biases
or trying to do things
like I'm I'm curious.
- Yeah, I think it's, it's both.
I would say like first
of all the mountain,
the image model is trained
based on the Gemini 3.0 model.
As Abhishek said it brings a
lot stronger word and knowledge
and like multimodal
understanding capabilities.
we also put a lot of work
into preparing the data
and we have a much larger
dataset data collection
that we use to train this pro model
and like have much
better synthetic captions
that we generated for the images.
And I believe like that
was a important part
of like bringing a lot
of the capabilities such
as you know like or knowledge
and like description
of the clock for example
and also like full wine, gas
and also maybe some of the
infographic capabilities
that we're also really proud about. Yeah,
- Yeah this this story of the fusion
between image understanding
and like how as we continue
to push the frontier from an
image understanding standpoint
it actually translate back
to the image generation I
think is really interesting
and you sort of get this unique flywheel.
One of the, one of the use cases
that I think this is
independent of image generation
that we have a ton of customers
that I think are looking
into this is around robotics
and like if you can get high fidelity sort
of synthetic captions of
like the the spatial world,
it potentially is one of the unlocks
for robotics which is a a sidebar
but also really interesting.
- Yeah. Another side
is the generation they
how always the understanding
because right now we could see the visual
reasoning of our model.
They can do segmentation,
they can do bonding box
- Collection
- And also can do a little bit
planning of robotics.
We actually have robotics
expert testing our models
so it's accurate to see the mixture
of all the experts coming together.
So first pressure testing our model
and then figure out a way if we can
further improve the model.
That's really cool about like working at
Google with all the experts.
- Yeah, so want to add, so
because like we use Gemini
itself to, it's leveraging,
its multimodal understanding ability
to generate almost all
the captions we have.
So right now like we are
also having a much closer
collaboration between the generation team
and also like the
understanding workstream so
that like we can really
have like as you said,
a flywheel about what our needs are
and like what feedback we have
and like how that can be used
to improve multimodal
understanding in Gemini. So
- Yeah, we've had a long thread
waiting to get JB on the,
on the podcast and and a
few of the other folks.
So he is, yeah he's on our
list of folks to talk about
- And specifically about understanding.
I think it's also interesting
like we were talking about
robotics and you have plenty of tasks
where actually you would
rather segment an object doing
image generation rather than like use text
to describe like exactly where
this object is in the image.
So I think you can really
think about like generation
and looking like key understanding
capabilities of the world
that wouldn't have
through text for example.
- Yeah, that is super interesting.
I don't know if we have a robotics
example but I'd love to see it.
I don't know, I wish I, do
you wanna maybe as we go, not
to a robotics example
but if you want to go to, we can look at
our, our next example.
- Yeah, don't have a robotics example
but just adding on
to things which we
improved over Nano Banana.
I think one thing which
is like we saw a lot
of like user complaints
where the model was not good,
the Nano Banana was not
perfect for multi turn.
So we have tried to improve
the model a lot for multi turn.
So in this case you can see
like, like I just asked another,
another reasoning example
like arrange all the words
alphabetically and it then
rearranges everything into alphabetical.
So I think the model
would feel much better
for such multi turn conversation.
It can, you can go up to
like 5, 10 conversations
and you won't, hopefully you
won't see much issues which we,
which people saw before.
So that is like one particular example.
Then some other examples like
this thing I'm really
excited about which is like,
so I had, I took
my GitHub repo, I wanted
to understand my previous code,
which I wrote using TensorFlow.
So, so, and this is something
which I've been using
exclusively at Google, which
is I paste someone's code,
I don't want to really read
all the code that is written
or ask Gemini to summarize it.
I just instead just ask my model to say
generate some infographic
poster explaining this image
and since our model can support like 2K
or 4K resolution so that
that's another positive,
let me try generating something at 2K
and yeah you can see it's a code.
It's like we can, I've
been able to play with
code which I even like 500 lines of code
and I just feed everything to model.
It just simplifies my daily life so much.
Oh I have once I, it started
explaining, let me just say
to explain generate an
image corresponding to it.
I wonder does the ordering
of the prompt change this?
Like is the, I don't know if we have,
I assume we have best practices somewhere,
but like do prompt first context first
or what is there a preference on ordering?
- Justtify the prompt. Okay.
The explicit and say generate an image.
- Yeah,
- Abhishek didn't say please that's why
- I did not say to generate
an image I just said
to generate an infograph
and started writing text.
Now I've explicitly said like
infographic image. Got it.
So now it's generating the image
- To your key question also like
how do you organize the
information, the answer
do you send an image back
or text And I think Gemini
has been like text first
and there is three that know
that we have like really
good like text rendering
and like detailed generation
we can really think about like
how could, can we use Gemini
to actually like generate image
where where it's actually
like when it's best than text.
- Yeah, I feel like this is an interesting
and I think the, the image
output is an example of this,
the GenUI stuff, which a
bunch of the products with now
with Gemini 3
actually has this as well.
It's just like when do you output text?
When do you actually write code
to make an interface for users?
So it feels like as Gemini
gets smarter it's also learning
like not not just text, like
what are the other modalities
that when is tool calls
as another example.
So this looks awesome though.
- Yeah, yeah, yeah. It's
- Like
like can we zoom in a bunch?
I'm trying to, yeah so it
started, does it look right? Yeah
- It looks like
- I haven't looked at TensorFlow code in a
long time so it looks convincing to me.
- Yeah it looks quite convincing.
Like it, it analyzed
from my code that I'm,
I was reading in some
data first I trained a teacher
network, it start, it
wrote all the parameters
of the teacher network, like
what were the con layer sizes,
the different layers which were used.
And then once teacher is
trained we train the student
network which is whose
architecture is perfect.
- Just to clarify, this
is not the architecture
for Nano Banana Pro, this is open source
- project. So
- Yeah, don't, don't get your hopes up
- And yeah this is the distillation loss.
You get teacher outputs
and then you comp you, you get
student ha distillation loss, normal loss,
you have all the hyper
parameters and everything.
Described how many steps
you train the teacher for
what optimizer was used
the same for a student
and yeah, it's
- Like that's crazy.
- Perfect. Like
- I feel like it's perfect.
Like I don't see any, I mean
I, I think I don't have context
to make sure that the like the,
it is like factually
accurate from an architecture
standpoint but the text rendering
and overall setup looks awesome.
Can we do the original net OpenNet model
as a side by side of this too?
Yes. I'm just curious
because I think this is one
of the places where
there's lots of domains
where like single turned editing,
I feel like the previous model
was actually really good at
that and continues to be good
but I feel like this
is one of the use cases
where it's like clearly night
and day difference from a text rendering
capability standpoint.
- Yeah so
and you can see the whole
like flow diagram like Abhishek
actually didn't ask for it
and it came up with this through thinking
and like organized in a very like clear
way the information. It's
- Crazy.
And have you noticed that
this has poster aspect ratio?
So so
- Yes.
Yeah so Nano Banana always
generates a square image.
Our model like is adaptive.
It kind of understands that
oh this seems like a prompt
for which I have to generate some,
a different aspect ratio you made.
So it's more intelligent in that way also.
- Yeah, I
- Love that.
- Can we zoom in on this?
- Yeah,
so this is Nano Banana I think
firstly like you can see the text
for instance if not perfect
it made some mistakes in the
text for Nano Banana.
Yeah the text rendering is not perfect
but if you compare
against Nano Banana Pro,
like we were hardly able
to find any mistake in
the text. In the text.
- Yeah this was better
than I remember though.
I honestly for Nana Banana 1.
I like the the, I like the pro model
but it is interesting that yeah,
- Yeah I want to say that
for for text rendering,
this is quite a hard test test
to measure in terms of metrics.
So during the last like
launch phase we actually have
a small team with very proactive
engineers like proactively
working on like defining the prompt set
with very simple short
text prompt, just measure
that you can character by
character, verify the metrics,
verify the successful rate
and also whether like long under short,
under specified text
prompt and long prompt
and the prompt with like
different language space
and they come up, they come
up with like human eval,
silver reader and then
eventually we also assemble team
within our work stream people
on different languages like
French, Chinese and Japanese, yada yada.
So we actually came up with a wide range
of rich metrics in other
to measure the success
of pro model.
- Yeah
- So that's quite impressive.
- This was one of the threads
that was most surprising
and I think we got lots of
feedback from the original model
about sort of like i18n
non-English languages
and I feel like I saw a bunch of, I forgot
what the metrics were specifically,
but this model is like
now SOTA across a bunch
of the other languages that
are not English specific.
Which is actually great because
I think the original model
was like super popular
all like all these like
country geography specific
trends and like Indonesia
and other places where
like people were trying
to do really cool things with the model.
So being able to render,
I assume rendering text in those languages
as well is Yeah, is state of the art.
- So we have been te
we tested other models,
we don't have any external benchmarks
but we tested different models as well
as Nano Banana on a variety
of different languages
and we are almost better in almost all the
languages I would say.
And it's,
this is a behavior which we did not like
explicitly add in the model.
We did not explicitly add
training data for each language.
We just targeted a general improvement
and then in the end, like when
we evaluated all different
more languages, it was very
surprising for us to see
that okay everything improves,
which was like super good too. Yeah
- And I think to piggyback on
what Yilin was saying earlier,
it's really a matter of like
having a very like wide variety
of data and improving the
amount of like concepts
and like data with a lot
of world knowledge we train
on compared to Nano Banana.
- Yeah, we, we should have,
we should have just gone
and benchmark max the clock
and the, and the wine stuff.
No, we, we didn't do that.
Which I think folks will feel
like as they use the model
and I, I do think it
would be easy to like go
and try to hill climb a bunch
of the narrow stuff especially
because I, we got lots
of feedback from n one
of what folks wanted.
So it's cool that we have
made general progress.
Do we have more more examples? We,
- Yeah I have some.
So on that side of text
rendering, like we are excited
for people to try out like infographics
where people can use the model
to explain different concepts
and in an infographic fashion,
like for instance, let me
show me an infographic
explaining photosynthesis
- And is this grounded with search
as well when you do these infographics?
- Yes, we have an option
here to ground with search.
So for a lot of queries if you're asking
for some real time queries
like show me infographic about
the weather for the next seven days.
- Yeah - It'll use search
to be grounded with search.
- That's awesome. And
can we talk as, as it's,
or as you're typing this prompt Abhishek
and as it's generating this image,
the reasoning piece is new for this model
and maybe the model was
doing some reasoning sort of
behind the scenes before,
but is there, is there any like
interesting part about this?
Like is that part of the story of
how we're hill climbing quality?
And again is it reasoning in like,
I assume it's not actually like
generating an image is just
like generating like a really
similar to how thinking
works on the text models,
it's just generating text
and then it uses the like reasoning
text to then make the image.
- Yeah, exactly. Yeah
and I think that's where
again like the fact
that the image model is so good
at following the text that's
given as input really
plays nicely with thinking
because like the thinking
trace is really long
and still like the model is
able to reason based on it
and like generate an accurate image.
- That's interesting. My,
my intuition would've been
that would've actually
like degrade model quality.
'cause you have like way more
like actually again then one
of the challenges for Nano
Banana originally was like sort
of you had to have like
more short terse requests
and like not give as you
gave it more context.
Like it sort of overloaded the
model at least in my, a bunch
of tests that I was doing personally
- The other way around.
- Really.
- Yeah. Yeah.
- Okay. See my intuition
doesn't match the,
the way that the model is trained, which
- Is why Yeah, and also the thinking,
the model also does self-critique
so it generates results
and then try to compare like
see the response comparing
with the content and see if actually
achieve the user intent.
If not the model will redo it
and try to improve on itself.
That might also be the reason why for some
of the challenging prompts
we can actually see a
higher successful rate.
- Oh that is interesting. And
Robin, just to clarify it is
what you're saying is
true for the pro model
and the original Nano Banana model
or just for the pro model?
- It's true in general.
Like it's good to basically
like give a lot of context
and define clearly what you want
and the longer the prompt the
more detailed you can specify
what you actually want to generate.
- Okay, cool. This looks,
this looks impressive. Can we,
- Yeah, so I just ask you
to explain photosynthesis
and detail, like I'm
not a biologic expert,
but it's like talking
everything in detail.
Like it talks about all the,
the photosynthesis equations,
it talks about all these things in
chloroplast, Grana, everything.
And I think you'll have
to really zoom into find any text mistake.
Like I don't see anything
on top of my mind,
but I feel it's going to
be really useful for people
to really understand deeper topics.
Yeah. Much easily in an informative
fashion instead of going
through all their texts,
we like visualize all this
information via an infographic image.
- This, this passes my like
seventh grade science vibe test.
- Yeah, yeah, yeah. We can also like, is
- This a 1K or 2K generation?
- This is like 1K but I can ask you
to impress make this it simpler for,
- Because we had the same,
same impression with Abhishek.
It felt amazing, but we're not bio experts
so it wasn't clear
to us like was it actually
like accurate or not.
And it was a bit too complex,
- But we sent it to, we
sent it to biology experts
and yeah, it's, it worked it's correct.
We were seventh graders
who are looking at this and
- We, we have biology professors
and assist researchers helping
us to grade those responses.
- Nice. Awesome. Yeah,
this is like much simpler.
How, how well does this
work for stuff like,
like obviously like photos,
photosynthesis is like very
in distribution I would assume
for like the knowledge of the model.
Is it able to sort of grok
the, the nuance of like topics
that are like not like
I don't, I don't know
what a good recent
example of this would be,
but like a net new research
paper about like something
that's like far afield
that like maybe isn't
super like well represented
in, in training data.
- Yeah, like so I tried like
once the Google earnings were
out, like I just asked it like
show me the latest Google earnings in
an infographic fashion.
It, it went through Google
search since it's Google search
grounded it, it found
all the Google earnings
and then generated infographics.
So I think since it's
Google search grounded,
like you can really test
it with very recent things,
which are for sure out distribution also
and the model should be able
to do Yeah, it quite well.
- That's awesome. I love it.
I feel like folks are gonna
get a lot of value out of this
infographic use case.
I feel like I'm guessing,
I don't know if it's gonna
be available in NotebookLM,
but this feels like a
very NotebookLM.
Bring to life experience.
- Yeah, it's available
in NotebookLM like,
and then the NotebookLM team
have integrated like lots
of key new things.
So it's really awesome to
play it in NotebookLM also.
- Yeah, I feel like audio
overviews plus this plus I'm like,
it's, yeah, I feel like
that's gonna be awesome.
I wanna take, can you
do, is it just text and
and image inputs or can I also
put in like audio and video
and stuff like that as well?
- Right now it's only image and text.
We, we hope to support video also,
but yeah, currently
it's image, text input.
- What, what about like native
audio in? Is that possible or
- Not right now?
Not right now. It it, it's great
that we launched everything
simultaneously like
Nano Banana Pro or simultaneously
launched in NotebookLM.
There are other few exciting
changes which people will see
all like simultaneously getting shipped.
So it's great to be in Google
and see like all
of these things all
simultaneously being launched.
I have right now all the
four presenters and Logan.
So let me just say show them celebrating.
- Tell - Tell it to put us
in a Google office and see.
- Oh yeah, yeah. I will put, put
that in the follow prompt then.
- And, and so from a user
perspective, would I expect, like
is there any difference in from
like a model inference time
perspective like
putting in all these images
and fusing them, does that
like generally on average like
take longer for the model to be able
to bring the references
into a single image
or is it like basically the similar time
to image generation across all of them?
- It's a bit longer, right,
but like the difference is not that much.
I think like mainly the different,
the bottleneck is image generation.
- Yeah.
- Yeah,
- This looks good.
Can we, can we zoom in and
- Yeah, but, but I think
we can make it, it,
it did not generate a photo realistic one.
It can generate a photo realistic one.
- I wish this is what was
happening where bananas
and balloons and yellow champagne.
- Shall we try again?
- Yeah, yeah,
- Yeah.
And in terms of latency,
I don't think there
will be much difference
between inputting one image versus
inputting multiple images.
It should be roughly the
latency should be close
to like 30 seconds
or something, which is yeah,
a bit higher than Nano Banana.
But yeah, quality comes at a price of,
- Yeah, I feel like, and
and so that's one of the,
I think the like world
knowledge grounding stuff.
If you have more references,
I'm trying to, I'm trying
to accumulate my list of
like when I should be going
to the pro model versus the
original Nano Banana model.
Other things it likes, it likes
doing us in this comic book.
What, what is the prompt?
I can't see it. She's,
- It's just show them
celebrating in a Nano Banana
pro launch party.
Let me just say show
a photo realistic
image. And you, you're
- Not saying please.
That's why I'm also excited about
Is there any like interesting
technical detail about like
what the, in my mind I
think of like  1K and 2K
and 4K as like I, I don't sort
of think deeply about this
and I just imagine there's some magic
happening behind the scenes.
But like from a model
perspective, is there like,
is this actually like
technically difficult if we
make, can make 1K work?
Like the, what's the difference?
Is it a model understanding
piece that's different
between 1K and 4K
or something like that?
Or is it just mainly about
- Infra and actually if you
have like serving cost in mind,
of course like generating 2K
or 4K is going to be quite more
expensive compared to 1K.
So you really want to make sure
that if you start generating
2K, it's actually going
to be worth it and the image quality is
going to be much better.
So in a sense you could just
like up sample a one K image
and say it's 2K, but
that's not going to bring something new.
Whereas like when you generate 2K,
you really want like
small text to be perfect.
- Yeah. - And that's the kind
of thing we think about when
we propose like higher
resolution for example. Yeah.
- And also in, I assume in
addition like to train like
for example 2K or 4K model
and there's also requirement on the
training datas resolution.
- Got it.
- Like it has to be higher
- Accordingly.
Yeah. We need AK now, now
that we zoom in on this,
this looks, this looks very good.
- Yeah. I think, yeah, it like perfectly,
almost perfectly captured
all of our faces. Like
- I'm looking at the hands I'm looking at.
I thought aha, I thought our hands,
we were holding the same.
- Yeah, I also
- We're not, we're not.
We have, we have separate hands
and we have our own glasses.
That's two glasses. Yeah.
- Yeah. Well the character
consistency is a big thing
for the, oh, there's also a label.
Oh where not a label, the
x wondering in natural image
setting. Yeah. So great.
- What are these little things that
are sitting on the table?
- I'm not sure. I think,
I think the model figured
that we are launching some new product,
which looks a banana looking thing.
- Electronic - Banana erase
electronic gadgets. I love it.
That's awesome.
- Abhishek, do you want to talk
about like how we were fighting
to make sure that character
consistency is also good in the
pro model because it was so
good in Nano Banana.
- Yeah, yeah, yeah.
- I feel like it's a high bar.
- Yeah. Yeah. It, it was
a very high bar for us
and I think this took us
maximum amount of time
to hill climb, burn because
when we trained the model,
we were super happy with
almost all other capabilities,
but consistency was
something which we found
that people really
loved about Nano Banana.
We were initially like
struggling to come to parity
with Nano Banana, but now if
you play like you'll find it's
even better than nano
banana for consistency.
But it took a great deal
of time both from data
better curating better evals
and changing our
training strategies a bit.
But it did take us quite a bit
to help climb on this
correcting consistency.
But we are now really excited for it.
It, it should work much better
and it should support
multiple people input
and yeah, it's a new capability
I think I, I love playing
with, I love imagining myself
in Times 30 under 30 poster,
- Hey, if Nano Banana
Pro goes well who knows?
Who knows? Maybe, maybe
they'll make it happen other,
so character consistency,
taking a major leap, the sort
of like real world knowledge
taking a major leap sort
of some of these vibe tests
start to work. The the
- Class text rendering, infographics,
- Text rendering.
Any, any other things top of mind from a,
from a model capability standpoint?
- I would say these are the
main things we definitely
improved also on style transfer
compared to Nano Banana.
- I think because of this work knowledge
and better grounding,
we are better at a bunch
of editing capabilities
like chart editing.
Imagine you have a very
complicated graph with pie chart,
a different format of the layout.
- I want this 'cause I
can't make, I'm incapable
of making slides, but slides are horrible.
But then I, I see beautiful
visual slides and I'm always
jealous
- you can just talk to the model
to improve the style, the
arrangement of the text
and the, the bar chart.
Yeah. So and that's awesome
part chart into pie chart
and some other things.
It can also do some mass
computations directly from the
numbers in your image.
- Yeah.
- And the make a process
through result into the edited images.
So that's, I see example
that you presented
with a compute Confucian matrix.
That's that's a common
thing in statistics.
And then you ask the model, okay,
tell me the percentage each
entry of the Confucian matrix.
- Yeah.
- And that's pretty accurate.
- Can can I do,
maybe this is a  too on
the nose question, can I do
transparent backgrounds?
No, darn it. I got, I got
a ton of feature requests.
Everyone was like, when is
transparent backgrounds, what,
what's the, what stops the
model from being able to do,
I don't, I don't have
a good understanding of
how transparent backgrounds
actually work from a
technical perspective, but is it
- Yeah,
- Possible to like,
- It's possible, but it's a matter
of like getting the right data
because you don't have
much data out there
that has like the transparency channels
and it's also like you
need to actually like
change a bit the way you train your model.
We, while making sure you're not going
to regress on like all
of the tasks you're currently good at.
So it's a bit tricky to get right.
I we're definitely going to get there,
but like not for this actual version.
- Okay. People want it so
we need, yeah, we need,
I feel like getting transparent
back on images is the most
painful thing anytime you're
trying to do something.
Yeah. So we can nail that and land it.
I feel like folks will love it.
- Yeah. Yeah. But in
general, like I, I think
even though like these
are the new capabilities
that you're talking about,
but even in simple like text
stream and editing,
the model should feel much superior. Like
- Yeah, - Because that's the
main metric we climbed on.
These all capabilities kind of emerged,
but simple text image queries,
simple edit image editing,
the model should feel much
better than what it was before.
Like this, we can even
try some examples of like,
I like I I was trying
to play with it like,
let me just simply show
me a funny pie chart
around some random topic
and then we can ask it to even
edit the image from pie chart
to 3D power plot or something.
- Yeah. And I think it really
relates to this capability
of like, the model is really
good at multiple generation.
- Yeah. I feel like multi-turn
generation has started,
I'm just remembering back
to the original, I forgot
what we even called the
Gemini two flash image model.
And you'd sort of like
actually in some contexts
and that model was great
and we were the first
ones to ship that sort
of like native image
generation editing capability,
but you'd like visibly be able to see some
of the regression over time.
Yeah. And I feel like now
it's sort of like crossed the
uncanny valley of like
you can just like continue
to do the editing in
generation that doesn't it,
it like seemingly gets better
over time, which is really cool.
- Yeah, yeah, yeah. Like for
instance, for this image,
like you can see like this is
a normal text image query,
but you can see that
all the labels are like
perfectly accurate.
Like 40%, 30%,
all these like percentage
labels on a pie chart is pretty
accurately depicted here.
And it came up and
- Actually add up to be a hundred percent.
- Yeah, yeah, yeah.
- I gotta, I gotta
measure, we gotta get that,
we gotta get the true a hundred
percent overlay we should
run like code execution and
use like a, a visualization.
I'm curious how, how close
the, the actual gap is.
But my, my vibe check is I
feel like it looks right, so
- Yeah.
- Awesome. Looks
- Quite good, know. Yeah. Yeah.
Looks quite good. And now let me say
and yeah, it did some
reasoning to like make it funny
- Going through memes
I've already seen. True.
- Staring into the fridge
and hoping for a miracle is a real one.
I open up my fridge all the time,
like maybe there's something
I really want in here
and every time I'm
disappointed or these MKs
- Yeah.
- In, in the office.
- I also remember it's just
in general I think there was a
use case, you know,
like on our team, a lot
of us there were in us
and some of the, our team
and the rest of them,
they're like in Europe,
like for example in London.
- Yeah.
- And every year when the daylight time,
daylight saving time changes,
it's mismatching in two countries.
So there's a week when our
meetings gets misaligned.
So on that week our helpful
TPM send out a message
to remind everybody there's
a change in the meeting time.
And someone else on our team
used that just chat message
and send that into a Nano Banana pro.
And it made a very, very cute,
like a reminder poster, like
with all the information
and like even has London
bridges and stuff.
So it's, it's really nice.
Like it's just nice to
see high visual quality
and like for just our
daily tasks now you maybe,
like now we don't have
to send chat messages,
we can just generate a good
looking cute image for that
and to communicate what
we want to tell others.
- Yeah, there's something about
images too that like, you,
it is just so easy to like grok Yeah.
Information and I feel like
we're so text-based and working
and all this other stuff
that like, you just sort
of resonate so deeply with image,
which I think is why this is so powerful.
And also just like letting anyone I, the,
the pro models leveling everyone up
and I feel like Nano Banana did this
where like anyone can go and edit images
and start to generate
and I feel like now the,
now this model, taking that even further I
think is a, a cool capability.
think is a, a cool capability.
This was a ton of fun. I'm
super excited for folks
to get their hands on this model.
I feel like across the Gemini
app NotebookLM AI studio,
the APIs, and I'm sure a bunch
of other products folks will be able
to experience this model.
So thank you all for the hard work.
Thanks for sitting down and
chatting about this stuff
and excited
for Gemini Nano Banana Pro 2
or whatever we call it in the future.
I wanted Giga Banana, but
that, that did not happen.
So Nano Banana Pro, again,
thank you all, this is a ton of fun.
Thanks for watching Release Notes.
Hope you enjoyed and we'll
see you in the next episode."
fXtna7UrL44,Koray Kavukcuoglu: â€œThis Is How We Are Going to Build AGIâ€,"Join Logan Kilpatrick and Koray Kavukcuoglu, CTO of Google DeepMind and Chief AI Architect of Google, as they discuss Gemini 3 and the state of AI! 

Their conversation includes the reception of Gemini 3, the ongoing advancements in AI research, and the role of benchmarks in pushing new frontiers. They explore critical areas for Gemini's focus, emphasizing instruction following, tool calls, and internationalization, alongside Google's collaborative approach to AI development.

Listen to this podcast: 
Apple Podcasts â†’ https://goo.gle/3Bm7QzQ 
Spotify â†’ https://goo.gle/3ZL3ADl 

Chapters:
0:00 - Intro 
2:00 - Gemini 3 launch reception
4:16 - Continuous progress and innovation
6:47 - Key areas for Gemini improvement
11:45 - Product scaffolding for model improvement
13:56 - Chief AI architect role
17:04 - Engineering mindset and collaboration
18:37 - Future growth areas for Gemini
20:33 - From research to engineering mindset
23:22 - The rise of generative media
27:22 - Nano Banana Pro capabilities
29:31 - Towards unified model checkpoints
36:26 - Organizing for AI success
38:26 - Balancing exploration and scaling
41:40 - DeepMind's collaborative culture
45:21 - Innovating at Google
48:37 - Closing


Watch more Release Notes â†’ https://goo.gle/4njokfg 
Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Logan Kilpatrick, Koray Kavukcuoglu
Products Mentioned:  Google AI, Gemini",2025-11-24T22:37:26Z,"Google,developers,pr_pr: AI DevRel (fka Core ML);,Purpose: Learn;,Video Type:Podcast;,gds:Yes;,ct: AIG;",28,en,en,https://i.ytimg.com/vi/fXtna7UrL44/default.jpg,https://i.ytimg.com/vi/fXtna7UrL44/hqdefault.jpg,2925,public,12513,339,39,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,fXtna7UrL44,"Gemini 3, we're sitting here. Reception
seems super positive. The vibes of the
model are good.
>> I'm very excited about the progress. I'm
excited about the research.
>> We had actually pushed the frontier on a
bunch of dimensions. This is how we are
going to build AGI. We want to do it the
right way and that's where we are
putting all our minds, all our
innovation.
>> It's not like it's this purely research
effort that's off in a lab [music]
somewhere like it's a it's a joint
effort with us in the world.
>> This is a new world, right? There's a
new technology that is defining a lot of
what users expect. We are [music]
in some sense like co-building AGI with
our customers. [music]
>> So all of a sudden you enable a lot more
people to be builders.
>> Bring anything to life.
>> Bring anything to life. Right.
>> Yeah. I feel like the next 6 months are
going to be probably just as exciting as
the as the last [music] 6 months and the
previous 6 months before that.
>> We are lucky to be living in this in
this age. [music] It's happening right
now. It's very exciting.
Hey everyone, welcome back to release
notes. My name is Logan Kilpatrick. I'm
on the DeepMind team. Uh today it's an
honor to be joined by Cororey Kavachulu
who is the CTO of Deep Mind and the new
chief AI architect of Google. Cory,
thanks for being here. I'm excited to
chat.
>> Me too. Yeah, very excited. Thanks for
inviting.
>> Of course. Gemini 3, we're sitting here.
We've launched the model. Reception
seems super positive. Like I think we we
went out and we obviously had a hunch
about how good the model was going to
be. Leaderboards looked awesome but I
think putting in the hands of users and
actually getting out is like
>> that's always that's always the test
right like I mean we have been like
benchmarking is the first step and then
we have been doing tests we have been
like with trusted testers with pre-
release and everything. So you get a
feeling that yes, it's a good model.
It's capable. It's not perfect, right?
But like I think um I'm quite pleased
with the reception really. People seem
to like the model and the kinds of
things that I think we found
interesting, they also found
interesting. So like that's good so far.
Like this is uh this is good. Yeah, we
we were talking yesterday and the the
thread of the conversation was just
around like appreciating this moment
that the progress isn't slowing down,
which I think resonates with me. And as
I was reflecting back to the last time I
sat next to you, uh we were at IO as we
launched 2.5 and we were listening to
Dennis and Sergey talk about AI and all
that stuff. I feel like the progress has
not slowed down, which is really
interesting. Like when we launched 2.5,
it felt like a state-of-the-art model
and it felt like we had actually pushed
the frontier on a bunch of dimensions
and I feel like 3.0 delivers that again.
>> Yeah.
>> And I'm curious what the the scaling
conversation of can it continue
continues to go. Uh what's your sense
right now?
>> Yeah. I mean um look I'm very excited
about the progress. So I'm excited about
the research like when you are actually
um there um in the research there are a
lot of excitement in terms of like in in
all areas of this right like I mean from
data pre-training post training
everywhere um we see a lot of excitement
we see a lot of progress a lot of new
ideas at the end of the day like this
whole like this this this whole thing is
really running on innovation running on
ideas right the more we do something
that is impactful that is in real world
that people use you actually get more
ideas because like your surface area
increases, the kinds of signals that you
get increases and I think like the the
problems will get harder, the problems
will get more variet and with that like
um I think like we will be challenged
and these kinds of challenges are good.
>> Yeah.
>> And I think that is the driver for like
going towards building intelligence as
well, right? Like that's that's that's
how it's going to happen. I feel like
sometimes like if you look at one or two
benchmarks you can see squeeze but I
think that's normal because benchmarks
are defined at a time when something was
a challenge you define that benchmark
and then of course like as the
technology progresses that benchmark
becomes um not the frontier it doesn't
define the frontier and then what
happens is like you define a new
benchmark. It's very normal in machine
learning, right? Like benchmarks and
model development is always hand in
hand. Like you need the benchmarks to
guide the model development, but you
only know what the next frontier is when
you when you when you get close to it so
that you can define with the new
benchmark.
>> Yeah, I I feel this way and I there was
a couple of benchmarks like HLE
originally all the models were horrible
on and doing like one or two% and I
think now the newest with deep think uh
is like 40 something% which is crazy.
Yeah,
>> RKGI2 was originally all the models
could barely do any of that. It's now
like 40 plus. So, it is it is
interesting and then it's also
interesting to see and I don't have the
context on why
>> the benchmarks that are static that do
have a little bit of like uh the test of
time if you will and like I I think
there are probably close to saturating
but like GPQA diamond as an example like
continues to stick around even though
we're ekking out 1% or whatever. It's
like there are there are really hard
questions there like I mean and and and
like those hard things we are still not
able to do.
>> Yeah.
>> Right. And and they still test something
but if you think about like where we are
with GPQA like it's not like oh there's
a there's like you're at 20s and you
need to you need to go to 90s right like
so you're getting close there. So the
number of things that it defines as
unsold is like of course like
decreasing. So at some point it's good
to find new frontiers, new benchmarks
and defining benchmarks is really really
important right like because if you're
going to think about like if we think
about benchmarks as the definition of
progress which does not necessarily
always align
>> right like there's this thing between
like there's progress and then there's
the benchmarks in an ideal case it's
100% aligned but it's it's never 100%
aligned
>> like to me the most important measure of
progress is like We have our models in
real world and scientists use them,
students use them, like lawyers use
them, engineers use them and then like
like people use them to do like all
sorts of things, writing, creative
writing, emails, easy or hard, right?
Like that spectrum is important and
different topics, different domains. If
you can actually continue delivering
larger value there, I think that's
that's progress and these benchmarks
help you quantify that.
>> Yeah. How do you think about and like
and maybe even there's a particular
example from like 2.5 to 3 or like
whatever we could choose whichever model
version change you want where are we
hill climbing and like like how actually
how in a world where there's like a
zillion benchmarks now actually of you
know you could choose where you want to
hill climb how are you thinking about
for just like broadly Gemini but also
maybe the pro model specifically like
where do we try to hill climb for that
>> I think like there are there are several
important areas right like one of them
is instruction following is important.
Instruction following is where like the
model needs to be able to understand the
request of the user and to be able to
follow that, right? Like um you don't
want the model just like answering
whatever it thinks it should answer,
right? So that instruction following
capability is important and that's
that's what we always do. And then like
um like for us internationalization is
important. Google is very international
and we want to reach everyone in the
world. So that that that part is
important
>> and I feel like 3.0 No pro at least
we're talking I was talking to Tulsi
this morning and she was remarking about
how incredible the model is for like
languages that historically we haven't
been really good at which is awesome to
see. So like continuously you have to
put the focus on some of these areas
right they might not look like like okay
it's the frontier of knowledge but they
are really really important because like
you want to be able to interact with the
users there because as I said it's all
about getting that signal from the users
and then like if you come to actually
like a little bit more technical domains
function calls tool calls agentic
actions and code these are really
important right like function calls and
tool calls are important because I think
it's a whole different like multiplier
of intelligence that comes from there.
Both from the point of view of the
models being able to just naturally use
all the tools and functions that we have
created ourselves and then use it in in
in its own uh in its own reasoning. Uh
but also the model writing its own
tools, right? Like you can think that
like the models are in a way the models
are tools in themselves as well. So that
one is is is a big thing like um
obviously like code is because like not
just because we are all software
engineers but also because like like we
know that like with that you can
actually build anything that is
happening on your on your laptop and on
your laptop it's not just software
engineering that happens
>> bring anything to life
>> anything to life right so a lot of the
things that we do right now happens in
digital world and code is the basis for
that to be able to to integrate with
anything that happens pretty much
anything that happens in in your life
not everything but like a lot of things
that's why these two things together I
think like makes up for a lot of um a
lot of reach like for users as well I
give this example of like wipe coding
right I like it why because um like a
lot of people are creative they have
ideas and all of a sudden you make them
you make them productive right like
going from creative to productive in a
way that like um you can you can just
write it down and then like you see the
application in front of you and like it
is like I mean most of the time it works
and when it works it's great right like
I mean so cool and that loop I think is
great so all of a sudden you enable a
lot more people to be builders like
building something like I mean like it's
great
>> I love it yeah thank you for this is the
AI studio pitch I appreciate we'll clip
this part out we'll put it out online um
one of the interesting threads that you
mentioned is like how um and and
actually the as part of this Gemini 3
moment we launched Google Anti-gravity
new agent coding platform. Um, how much
do you think about
>> like the importance of having this
product scaffolding to hill climb on
quality from a
>> from a model perspective obviously?
Yeah. Tool calling and coding.
>> Yeah. Yeah. It's I like to me it's very
very important and I think like um like
anti-gravity as a product itself yes
it's exciting but like from a model
perspective if you think about it um so
it's like double-sided right like let's
talk about first the model perspective
from the model perspective like being
able to have this integration with the
end users in this case software
engineers and learning from them
directly to understand where the model
needs to improve is really critical for
us I mean it is important for in areas
like Gemini app is important for the
same reason, right? Like I mean
understanding users directly is very
very important. Anti-gravity is the same
way. AI studio is the same way. So
having these products that we work
really closely with and then
understanding and learning that getting
those user signals I think is really
massive and anti-gravity has been a like
a very critical launch partner. um it
hasn't been long that they have joined
right but in the last um two three weeks
of our launch process like their
feedback has been really really
instrumental the same thing with search
AI mode right like I mean AI overviews
have been that we get a lot of feedback
from there so like to me this
integration with the products and
getting that signal is the is the main
driver that we understand like of course
we have the benchmark so like we know
how to push the STEM the sciences the
math that kind of intelligence but it's
really important important that actually
we understand the real world use cases
because like I mean this has to be
useful in real world.
>> Yeah. How in in your in your new chief
AI architect role um you're now
responsible for also making sure that we
don't just have good models but the
products actually take the models and
implement them and sort of build build
sort of great product experiences across
Google. how like how much obviously I
think this is the right thing for users
like getting Gemini 3 in all the product
services on day one isn't is like an
awesome accomplishment for for Google um
and I think even more so hopefully more
product services in the future how much
additional complexity from the deep mind
perspective do you think it adds to try
to do this in some sense life was
simpler a year and a half ago
>> true
>> but like we are building intelligence
right a lot of people ask me you have
these two rows like I mean I have these
like two titles in a way but like they
they're very much the same thing.
>> If we are going to build intelligence,
we have to do it with the products
through the products connecting with the
users. With the Kairo, what I'm trying
to do is make sure that the products in
Google have the best technology that is
available to them. We are not trying to
do the products like we are not product
people. We are technology developers,
right? like we develop the technology,
we do the models and of course like I
mean just like everyone is opiated on on
anything like I mean people are opiated
but like the most important thing for me
is like making the models making the
technology available in the best way
that is possible and then work with the
product teams to to enable them to build
the best product in this AI world like
because this is a new world right
there's a new technology that is
defining a lot of what users expect and
how how the product should behave that
information that they should carry over
and all the new things that you can do
with this new technology. So to me it's
about enabling that across Google
working with all the products. I think
that's exciting both from the product
perspective from what users getting
perspective but also from the point of
view of like as I said that's our main
driver right like it's really important
for us to be able to feel that user need
to be able to get that user signal
that's critical for us so that's why I
wanted to do it that like this is how we
are going to build AGI this is how we
are going to build intelligence like
with the products
>> that's that's how I think it's going to
happen
>> this is a great this is a great tweet
for to put out at some point because I
do think it's I do think it's
interesting. It's I I share this
perspective that like we are in some
sense like co-building AGI with our
customers with the other PAS like it's
not like it's this purely research
effort that's off in a lab somewhere
like it's a it's a joint effort with us
in the world
>> and and and I think like it is actually
a very trusted tested system as well
that like it's a very engineering
mindset that I think we are adopting
more and more and I think it's important
to have an engineering mindset on in
this one because like um when when when
something is like like nicely engineered
you know that it is it is robust it is
like safe to use so like uh we are doing
something in real world and we are
adapting all the trusted tested in a way
ideas of how to build things and I think
like that's reflected in how we how we
think about safety how we think about
security right like we try to think
about it again from that engineering
mindset of think about it from the
ground up from the beginning not
something that comes at the end right
like we don't like so when we are doing
post training models when we are doing
pre- training when we are looking at our
data we always have this like everyone
needs to think about this like do do we
have a safety team obviously we have a
safety team and they are bringing in all
the technology that is related to we
have a security team they're bringing in
all the technology but enabling everyone
in Gemini to actually also heavily be
part of that development process that is
that is taking this as a first principle
and those teams are themselves part of
our post-raining teams
Right? Like so when we are developing
these when we are doing these iterations
release candidates just like we look at
like GPQA hle those kinds of benchmarks
we look at it safety security measures
as well like that's that's I think I
think that is a very like that
engineering mindset is important.
>> Yeah I completely agree with you. I
think it also feels natural to Google
which is also helpful like because of
how collaborative and like big it how
big the effort is now to ship Gemini
models out the door. I mean with Gemini
3 I think like we were just reflecting
on this like to me one of the important
things is like this model has been a
very team Google model.
>> We should look into the data. I might be
like one of the I mean some of the like
maybe the Apollo NASA programs had a lot
of people but like it is I think this
massive Google global also global effort
across all of our teams to make happen
which is crazy. Every Gemini release
like takes people from like this
continent, Europe, Asia, all around the
world like we have teams all around the
world and they contribute and like not
just like uh not just GDM teams, right?
Like all teams across Google.
>> Yeah. Like it's a huge collaborative
effort and like we sim shipped with AI
mode, we sim shipped with Gemini app,
right? These are not easy to do um
because they were part of they were
together with us during our development.
That's the only way that on day one we
can actually go all together out at the
same time the model is ready and we have
been doing that when we say across
Google it's not just like people
actively trying to build the model all
the product teams they doing their parts
as well. Yeah, I I have a maybe this
isn't a controversial question, but you
know, uh Gemini 3 were sort of soda on
many benchmarks, a lot of benchmarks.
We're sort of sim shipping across, you
know, across the Google product
surfaces, our sort of partner ecosystem
surfaces. Uh the reception is very
positive. Sort of the vibes of the model
are good. If you sort of fast forward I
knock on wood. Uh if we sort of fast
forward to like the next major Google
model launch like are there things that
you are like still on your list of you
wish we were doing X Y and like how does
it get better than than the G or should
we just enjoy the moment of Gemini 3 and
I think we should do both right like we
should enjoy the moment because like one
day of enjoying the moment is a good
thing like this is the launch day and
like I think people are appreciating the
model so like I'd like the team to enjoy
this moment as well
>> right but But at the same time, every
area we look at, we also see gaps,
right? Like is it perfect in writing?
No, it's not perfect in writing. Is it
perfect in coding? It's not perfect in
coding. I mean, especially I think on
the on on the area of like agentic
actions and coding. I think like um like
I think that there's a lot more room
there. Um that's one of the most
exciting growth areas and like um like
we need to identify where we can do more
and we'll do more, right? Like I think
we have come a long way. The model is
like I would say pretty much like maybe
90 95% of the people who will engage
with coding in some ways are their
software engineers or these like
creative people who want to build
something.
>> Yeah,
>> I'd like to think that this model is the
best thing that they can use, right? But
there are some cases probably that is we
still need to do better.
>> Yeah, I have another sort of pointed
question for coding and tool use. What
do you think historic has it just been
if you sort of look at the history for
Gemini and obviously we had like a very
multimmodal focus for 1.0 and I think
for 2.0 know we started to make some of
the like agentic infrastructure work
like do you have a sense of like why we
>> and I'll make the caveat that like I
think the rate of progress looks really
strong but like why has it just been
like a focus thing why we haven't been
like state-of-the-art and agentic tool
use from the get-go but for example
multimodal we have been literally Gemini
1 was state-of-the-art and multimodal
and we've sort of held that for a long
time
>> I like I don't think it was a deliberate
thing I think it was like I mean
honestly I think like if anything when I
reflect back I tie it to using the
models the development environment being
closely tied to real world. The more we
are tight then we are more better
understanding these like real
requirements that is happening
>> and I think like in our journey in
Gemini we started from a point where of
course like I mean like the AI research
in Google is a huge history right like
the amount of amazing researchers that
we have and the amazing history of AI
research that has been done in Google um
I think it's great but like Gemini is
also a journey of moving from that like
research environment into this like as
we talked this engineer mindset and
getting into a space where we are uh we
are really connected with the products
right like when I look at the team like
I have to say I feel really proud
because like this team is still majority
formed by people like including me right
like for five years ago we were writing
papers like we were researching AI and
here we are actually we are at that
frontier of that technology and that
technology you are developing it via
products with the users it's a
completely different mindset that we are
building models every 6 months and then
we are doing updates every month, month
and a half. It's an amazing shift and
like I think like we walk through that
that that shift.
>> Yeah, I love that. Uh Gemini 3 progress
has been awesome. Another thread that
was top of mind is just generally sort
of how we're thinking about like where
the gen media models which I think
historically have like not been a huge I
mean not that they haven't been a focus
they were they've al always been
interesting but I feel like we've had
with V3 VO3.1 with the nano banana model
we've had like so much success from like
a product externalization standpoint and
I'm curious how you think about in this
like pursuit of we want to build AGI
sometimes I think sometimes I and
convince myself that like a video model
is like not part of that story. I don't
think that's true. I think in general
you can sort of you should understand
the world and physics and all this other
stuff. So I'm curious how you see all
these things intertwining together.
>> If you actually go back like 10 15 years
ago, generative models were mostly on
images, right? Like because like we
could we could much better inspect like
what is going on
>> in terms of and also this idea of
understanding the world understanding
the physics was the main driver of doing
generative models with images and and
and and so on like some of the exciting
things that we have done with generative
models like date back to like 10 years
ago like way 10 years ago
>> feels like 20
>> right 20 years ago we were still doing
image models right I mean that's why I
was stating a little bit but during my
PhD we were doing like generative image
models, right? Like everyone was doing
those
>> at that time. We woke through that like
I mean we had uh we had things called
like pixel CNN's, right? Like they were
like image generative models. In a way
what happened was um I think it is also
a big realization that text actually was
the uh better domain to have very fast
progress. But I think it is very natural
that the image models are coming back
and like at GDM we have had really
strong um image video audio models for a
long time. I think that's what I'm
trying to explain. Maybe bringing those
together I think is natural. So where we
are going right now is we have always
talked about this multimodality
right and of course naturally like we
have always talked about like input
output multimodality and that's where we
are going right and when you look at it
as the technology progresses the
architectures the ideas in between those
two different domains have been merging
with each other it used to be that these
architectures were very different
>> right but they are getting together
quite a lot so like it's not like we are
forcing something in what is happening
is naturally the technology is
converting ing as the technology is
converging. It is converging because
everyone understands where to get more
efficiency from where the ideas are
evolving and we see a common path and
that common path I think is is getting
together well. So nana banana is one of
those first moments right like where you
can iterate over images you can talk to
the model because what happens is like
text models have a lot of world
understanding right like from the text
they have a lot of world understanding
and then the image model has the world
understanding from a different
perspective so like when you merge those
two I think you get exciting things
because like I think people feel that
this model understands the neons that
they want to get through
>> I have another question about nanobas do
you think we should just have goofy
names for all of our models. Do you
think that would help?
>> Not really. Look, I mean, like I think
we didn't do it on purpose.
>> Gemini 3. If we didn't name it Gemini 3,
what would you what would we have called
it? Something ridiculous.
>> I No, I don't know. I'm not good at
names. I think I like I like I mean um
it was RiftRunner, right? Like it was
Rift Runner. Like we actually use Gemini
model. Those are code names. We use
Gemini models to come up with those code
names too. And Nano Banana was not one
of those. Like we didn't use Gemini,
right? There's a story about it. I think
like it's published somewhere. Yeah. Um
I mean as long as these things are
natural and like organic, I think I'm
happy because I think the teams who are
building the models, it's good for them
to sort of like have that connection.
>> Yeah. And then um when we released them
like I think that that just like I mean
that happened because we were testing
the model with the code name right on LM
Marina and people loved it and I think
like I don't know I'd like to think that
it was so organic that like sort of it
caught on. I'm not sure if you can
create a process to generate that.
>> I agree with you. That's my feeling.
>> If we have it we should use it. If you
don't have it it's good to have standard
names. Yeah, we should talk about uh
Nano Banana Pro, which is our new
state-of-the-art uh image generation
model built on top of Gemini 3 Pro. Um,
and I think the team I think actually
even as they were sort of finishing,
Nano Banana sort of like had early
signal that potentially doing this in a
pro capacity like you could sort of get
a lot more performance on a bunch of
like more nuance use cases like text
rendering and world understanding and
stuff like that. any anything sort of
top of mind for I know we're we're a lot
of stuff going on but I think like this
is like probably where we see this like
alignment of different technologies is
coming into play right like I mean
because with Gemini models we have we
have always said like every model
version is a family of models right like
we have the pro flash like this family
of models uh because at different sizes
you have different compromises in terms
of speed accuracy cost those kinds of
things as these things are coming
together of course like We have the same
experience on the image side as well.
Yeah. So I think it's natural that the
teams like thought about okay like
there's the 3.0 pro architecture. Can we
actually tune this model more to be like
generative image using everything that
we learned in the first version and
increasing the size and I think like
where we end up with is something a lot
more capable understands really complex
like some of the most exciting use cases
are you have large set of really complex
documents you can feed those in. We rely
on these models to ask questions. You
can ask it to generate an infographic
about that as well and then it works
right. Um so this is where this natural
input modality input output modality
just comes into play and it's great.
>> Yeah, it feels like magic. I I don't
know uh hopefully folks will have seen
the examples by the time this video
comes out but I think it's just it's so
cool uh seeing a bunch of the internal
examples being shared around. It's it's
crazy.
>> Yes, I agree. Like it's exciting when
you see that all of a sudden, oh my god.
Yes. Like that's sort of huge amount of
text and concepts and like complicated
things explained in one picture such a
nice way.
>> Like uh when you see those things like
it is it's nice, right? Like you you
realize the model is capable
>> and it's Yeah. And it's it's the there's
like so much nuance to it too which is
um which is really interesting. I I I
have a parallel question to this which
is uh probably December of last year uh
December 2024.
>> All right.
>> Tulsi was promising how we were going to
sort of have these unified Gemini model
checkpoints. Um and I I think what
you're describing is like actually that
we've gotten really close to that now
where like the architecture historically
was done
>> unified in terms of like image
generation and Oh, I see. I see.
>> Yeah. Yeah. And and I'm curious. Do you
think like that I assume that's like a
goal is like we want these things
actually mainlined into the model uh and
there's like natural things that stop
that from happening and I'm curious like
if any like context or sort of high
level
>> look I think as I said the technology
the architectures they are aligning
>> right so we see that happening at
regular intervals people are trying but
it's an hypothesis and like you can't be
ideology based in this right the
scientific method is the scientific
method like we try things we have an
hypothesis and you see the results
sometimes it works Sometimes it doesn't
but that's the progression that we go
through. It's getting closer. I'm pretty
sure near future we are going to see
something getting together and I think
gradually it's going to be more and more
like one single model out but it will
require a lot of innovation right like
it is hard like if you think about it
the output space is very critical for
the models because that's where your
learning signal comes from right now our
learning signal comes from code and text
that's the most of the driver of like
that output space and that's why like
you are getting good at there now being
able to generate images is like like we
are so tuned for the quality in images
like it is it is a hard thing to do
right like generating really like the
quality of the images the pixel
perfectness is hard and then images are
also conceptually it has to be very
coherent like every pixel both the
quality matters but also how it fits
with the general concept of the picture
like it matters right it is harder to
train something that does both the way I
look at this is to me I think it's
definitely possible it will be possible
it's just about finding the right
innovations in the model to make it
happen.
>> Yeah, I love it. I'm excited. Hope it'll
hopefully make our serving situation
easier too if we have uh if we have a
>> Yeah, that I don't know
>> a single model chart.
>> It's impossible to say.
>> It's impossible. I agree with you. the
sort of interesting thread as we sort of
sit here and you know deep mind has a
bunch of the world's best AI products
hopefully vibe coding and AI studio
Gemini app anti-gravity uh and sort of
across Google that's happening now we
have a great state-of-the-art model with
Gemini 3 we have now banana we have vio
we have all these models are sort of at
the frontier the world looked very
different like 10 years ago uh or even
like 15 years ago and I'm sort of
curious like for your personal journey
to get to this point. You when we were
talking yesterday, you had mentioned
which I had no idea. Um and I mentioned
this to someone else and they also were
like I had no idea of this. Uh you were
the first deep learning researcher at
DeepMind. And I think taking that thread
to this place that we're at now feels
feels like it's a crazy jump uh to go
from just like the fact that people
weren't excited about this technology. I
don't know how long ago you started at
DMI like 10 years.
>> Uh 2012.
13 years.
>> Yeah,
>> that's crazy. 13 years ago, people
weren't excited about this technology to
the place or I guess deep mind was
excited about this technology to the
place now where like it is literally
powering all these products and is like
the main thing and I'm curious as you
reflect on that um what comes to mind uh
well
>> is it surprising or like was it
obviously
>> well I mean I think this is the hopeful
positive outcome scenario case right
like um the way I say it is like like
when I was doing my PhD I think it's the
same for everyone doing their PhDs you
believe that what you do is important or
is going to important right like you are
really interested on that topic you
think that it's going to make a big
impact and I think like uh I was in the
same mindset that's why I was really
excited about deep mind when like Dennis
and Shane uh reached out and we talked I
was really excited to learn that there
was a place that was actually that was
that was really focused on building
intelligence and deep learning was at
the at the core of it and u it's
actually like um like me and like uh my
friend Carl Greger actually like uh we
were both in Jan's lab in NYU. We joined
we joined deep mind at the same time. So
just to be very very specific and then
at those times it was very unnatural
that you would have a deep learning
focused and AI focused uh startup even.
So like I think that was very visionary
and an amazing place to be like it was
really uh it was really exciting and
then like like I started the deep
learning team it grew. I think one of
the things that I like I mean my
approach to deep learning has always
been that like a mentality of how you
approach problems and uh the first
principle it's always learning based
that's what deep mind was about
everything is bet on learning it was an
exciting journey to start from where we
were at the days and then RL and agents
and um everything that we have done
along the way like you go into these
things at least like this is how I think
I go into these things hoping that a
positive outcome happens but I reflect
and I say that like we are lucky right
like we are lucky to be living in this
in this age because I think a lot of
people have worked on AI or the topics
that they are really passionate about
thinking that this is their age and and
and this is when it's going to pan out
but it's happening right now and like we
have to also realize that AI is
happening right now not just because
machine learning and deep learning but
also because it's like the hardware
evolution has come to a certain state
like internet and data has come to a
certain state, right? So there are a lot
of things that align together and I feel
lucky uh to be actually be doing AI and
sort of like working up to this moment.
I think it is a like when I reflect
that's how I feel that like yes they
were all choices that like we worked on
AI and we we made and I made like
specific choices to work on AI but also
at the same time I also feel very lucky
at this time we are in this position.
It's very exciting.
>> Yeah, I agree too. I love that. What um
I'm curious like what are some of the
and I was watching the the thinking game
video and sort of like see like learning
more about like I hadn't wasn't I wasn't
around for AlphaFold. Uh so that's the
only context that I have is like reading
about it and seeing people talk about it
and I'm curious like as you reflect and
having lived through a bunch of that how
things are different today versus what
they were before. And I'll sort of tee
you up with one example which is what
you kind of alluded to off camera right
before this which is uh and this is not
exactly your words. You were like we've
kind of figured out how to make these
models and bring them to the world was
like sort of an essence of what you're
getting at which I agree with. Um and
I'm curious if that felt like yeah how
that
>> is similar or not to how things were for
some of the previous iterations. how to
organize or the cultural traits of what
is important to be successful to turn
hard scientific and technical problems
into successful outcomes. I think we
learned to do that a lot with many of
the projects that we have done starting
from DQN, Alpha Go, Alpha Zero, Alpha
Fold, all these kinds of things have
been quite impactful and in their ways
like we learned a lot on how to organize
around the around a particular goal,
particular mission, organize as a as as
as a team, right? Like I remember in the
early days of deep mind like uh we would
work on a project with like 25 people
and we would write papers with 25 people
and then everyone would say to us like
surely 25 people didn't work on the
>> I would say yes they did they did right
like I mean we would organ because in
sciences and in research that wasn't
common right
>> and I think like that knowledge that
mentality I think is is is key we
evolved through that I think that is
really really important at the same time
I think like with the latest like the
last two three years as we talked right.
Um what we have been merged like what we
have merged this with is like the idea
that now this is more like a like an
engineering mindset where we have a
mainline of models that we are
developing and we learned how to do
exploration on this mainline how to do
exploration with these models. The good
example where I see this and I like
every time I see this or think about
this I feel quite happy is our deep
think models. Those are the models that
we go to the IMO competition with with
to the ISPC competition, ICPC
competition with and I think that's a
really really cool and good example
because like we do the exploration you
pick these like big targets like am
competition is really important right
like u it's really hard problems and
like um like kudos to every student out
there who's competing in those
competitions amazing stuff really and
like being able to put a model there of
course like you you have the urge to do
something custom for that
>> we sort of what we try to do is use that
as an opportunity to evolve what we have
or to come up with new ideas that are
compatible with the with the models that
we have because we believe in the
generality of the technology that we
have and then that's how things like
deep think happen and then like we come
up with something and then we make it
available for everyone right so everyone
can use a model that is actually the the
the one that is used in the IMO
competition
>> yeah just to draw a correlary between
what you said the 25 people in the
paper. I think now the today version of
that is you look at like I'm sure there
there's a Gemini 3 contributors list
that will come out or is already
>> 2500
>> and there's like 2500 people and then
I'm sure people conservatively
>> yeah I'm sure people are thinking
there's no way that 2500 people
contributed to actually but they did
which is crazy and it it is fascinating
to see like the how large scale some of
these problems are now. Yeah, we did and
I think it is important for us and
that's that's one of the like the great
things about Google. There are so many
people who are amazing experts in their
areas. We benefit from that. Like Google
has this full stack approach, right?
Like we benefit from that. So you have
experts at every layer from like like
data centers to chips to like networking
to how to run these things at scale,
right? It's all it come to a state again
going in on this like engineering
mindset. come to a state that these
things are not separable, right? Like
when we design a model, we design it
knowing what hardware it's going to run
on and we design the next hardware
knowing where the models will probably
go. But this is beautiful, right? Like I
mean, but coordinating this, yes, of
course, you have thousands of people
working together and contributing. And I
think we need to recognize it and that's
a beautiful thing. That's great.
>> Yeah, it's not easy to pull off. Um, one
of the one of the interesting threads is
around back to this sort of like deep
mind legacy sort of doing all these
different uh scientific approaches and
like trying to solve these really
interesting problems and today where we
actually like sort of know that this
technology works in a bunch of
capacities and we truly just need to
keep scaling it up and like and
obviously there's innovation that's
required to keep doing that but I'm
curious like how you think about deep
mind sort of in in today's era balancing
you know purely doing scientific
exploration versus like we just trying
to scale up Gemini and maybe we can use
my favorite example for you which is
Gemini diffusion as an as a sort of like
an example of like that decision-m come
to life in some capacity
>> like that is the most critical thing
right like finding that balance is
really important even now when people
ask me like what is the biggest risk for
Gemini and of course I think about this
a lot um the biggest risk for Gemini is
running out of innovation
>> because I really don't believe that like
we figured out the recipe and like we're
just going to execute from here. I don't
believe in that. If our goal is to build
intelligence and we're going to do that
of course like with the users with the
products but the problems out there are
very challenging. The like our goal is
still very challenging and it's out
there and I I don't feel like we have
the recipe figured out that it's just
scaling up or executing. It is
innovation that is going to enable that
and innovation you can think about it as
at different scales or at different
tangential directions to what you have
right now like of course like we have
Gemini models and inside the Gemini
project we explore a lot like we explore
new architectures we explore new ideas
we explore different ways of doing
things we have to do that we continues
to do that and that's where all the
innovation comes from but like also at
the same time I think Deep mind or
Google deep mind as a whole doing a lot
more exploration. I think it is an it is
it is it is very critical for us like we
have to do those things because like
again like there might be some things
that like the Gemini project itself
might be too constraining to explore
some things. So like I think the best
thing that we can do is both in Google
deep mind also in Google research right
like we would explore all sorts of ideas
and we will bring those ideas in because
at the end of the day Gemini is not the
architecture right Gemini is the goal
that you want to achieve the goal that
you want to achieve is the intelligence
and you want to do it with your products
enabling goal of Google to really run on
this AI engine in a way it doesn't
matter what particular architecture it
is
>> we have something currently and we have
ways of evolving through that and we
will evolve through that and the engine
of that will be innovation. It will
always be innovation. So finding that
balance or finding opportunities of
doing that in different ways I think is
very critical.
>> Yeah. Yeah, I have a parallel question
to that which is at IO I sat down with
Sergey and I made the comment to him
that sort of when and I I personally
felt this at IO which is you bring all
these people together to launch these
models and and have this innovation you
sort of like feel the the warmth of of
humanity as you do that which is really
interesting and I I was referencing this
because of you know I was sitting next
to you also listening to them and I sort
of uh was feeling your warmth and I I
mean this very personally because I
think this translates into like how deep
mind sort of as a whole operates. I
think like Demis has this as well where
it's like this deep scientific roots. Uh
but also it's just like people who are
like nice and friendly and kind um and
there there is something interesting
where like um I don't I don't know how
much people appreciate like how much
that culture matters and like how it
manifests. And I'm curious like as you
think about like like helping sort of
shape and run this um how how that yeah
how that manifests for you. Like first
of all like I mean thank you very much
you're embarrassing [laughter]
uh but like I think it is important to
be I believe in the team that we have
and I believe in giving people like
trusting people giving people the
opportunity and that team aspect is
important and I think like uh this is
something that at least to my part I can
say I've learned through uh working at
deep mind as well like because like we
were a small team and of course like
like it's like you build that trust
there and then like um how you maintain
that as you grow. I think it is
important to have this environment where
people feel like okay like we really
care about like solving the challenging
technical scientific problem that makes
an impact that matters for real world
and I think that is still what we are
doing right like Gemini as I said is
about that like building intelligence is
a highly technical challenging
scientific problem we have to approach
it with that way we have to approach it
with that humility as well right like we
have to always question ourselves like
hopefully The team feels like that too.
And I'm like that's why I always keep
saying I'm really proud of the team that
they work together amazingly well. Like
we were just talking upstairs at the at
the micro kitchen today, right? Like I
mean I said to them, ""Yes, it's tiring.
Yes, it's it's hard. Yes, we are all
exhausted."" But this is what it is. Like
we don't have a perfect structure for
this. Everyone is coming together and
working together and like uh supporting
each other. It is hard but like what
makes it fun and enjoyable and also like
what makes you tackle really hard
problems is I think to a big extent like
having the right team together working
together the burden is the way I see it
is more like be clear about the
potential of the technology that we have
>> I can't definitely say that 20 years
from now it's the exact same LLM
architecture I'm sure it won't be right
so I think pushing for new exploration
>> is the right thing to do as as we talked
about like GDM as a whole together with
Google research we have to be doing with
the academic research community like as
a whole we have to push many different
directions I think that's perfectly fine
like what is right what is wrong is a
like I don't think that it's the
important conversation I think like um
like the capabilities and the
demonstrations of those capabilities in
real world is the real thing that that
that that should that should speak for
itself.
>> Yeah. I have one last question which is
and I'm I'm curious to have your
reflection on this as well. I feel like
the for me personally like my first year
and a half plus at Google felt like uh
which I really liked actually this like
uh Google underdog story um to a certain
extent which you know despite all the
infrastructure advantage and all that
like for me personally showing like
working
>> when when did you join?
>> Uh April 2024.
>> 2024. Okay.
>> Yeah. Yeah. So like for and also like
the AI studio context. So like we were
building this product and like sort of
>> Oh yes. Now now I remember
>> we had no users. We had or we had 30,000
users. We had no revenue. We had sort of
very early in the the Gemini model life
cycle. And I think fast forward to today
and like it's obviously not like I was
getting a bunch of pings earlier as as
sort of the the last couple of days as
this model has been rolling out and you
know from folks across the ecosystem.
I'm sure you got a bunch of these as
well. people like very I think they're
real finally realizing that like this is
happening but I'm curious from your
perspective like what did you feel that
like again I had belief that's why I
joined Google that like we were going to
get to this point but like did you
>> did you feel that underdogness too and
I'm curious like how that how you think
the team will that man manifest for the
team as we turn that corner
>> I definitely did even before that
because like um when LLM's really like
became apparent that they're really
powerful right like I felt like very
very honestly I felt like we were the
frontier AI lab right like in deep mind
but also at the same time I felt like
okay like there's something that we
haven't investment invested as much as
we should have as researchers and that's
a big learning for me as well right like
that's why I'm always very careful about
like we need to cast a wide net that's
really important that exploration is
important
>> um it's not about this architecture that
architecture and I've been very I've
been very open with the team that when
we started taking LLMs a lot more
seriously and starting with like with
the Gemini program like two and a half
years ago I think we have been always
and I've been very honest with the team
that like like we are nowhere near what
is state-of-the-art here like we don't
know how to do a lot of things there are
a lot of things we know how to do but
like we are not at that level yet and
it's a catchup and it has been a catchup
for a long I feel like nowadays we are
at that leadership group.
>> I feel really good and positive about
the pace that we are operating at. We're
in a good sort of rhythm. We have a good
dynamic. We have a good rhythm.
>> But like um yeah, we have been catching
up. You have to be honest with yourself,
right? Like when you are catching up,
you are catching up. You have to you
have to see what others are doing and
like um learn what you can learn, but
you have to innovate for yourself. And
that's what we did. And that's what I
feel like um it's a good underdog story
in the in in a sense in that way, right?
like we innovated for ourselves and like
we found our own solutions both like
technology wise model wise process wise
and how we run right and it's unique to
us right like we run together with all
of Google like look at like what we are
doing it's a very different scale I
never saw these things as like sometimes
people also say oh Google is big and it
is hard
>> I I I see that as like we can turn it
into our advantage because we have
unique things that we can do so like I'm
quite I'm quite pleased where we are but
we had to learn through and innovate
through that. That's a good way to
achieve uh what we have achieved right
now and like there's a lot more to do
>> right like I mean I feel like we are
sort of just catching up we are just
getting there's always comparisons but
our goal is to build intelligence right
like we want to do that we want to do it
the right way and that's where we are
putting all our minds all our innovation
that way
>> yeah I feel like the next uh the next
six months are going to be probably just
as exciting as the as the last six
months and the previous six months
before that. Uh, thank you for taking
the time to sit down. This was a ton of
fun. Um, I hope we get to sit down again
before IO next year. Uh, which feels
like forever, but it is going to sneak
up. I'm sure there's going to be
meetings like next week that are like IO
2026 planning to make everything happen.
So, uh, thank you for taking the time.
Congrats, uh, again to, uh, to you and
the Deep Mind team and everyone on the
model research team for making Gemini 3,
Nano Banana Pro, everything else happen.
>> Yeah, thank you very much. It's been
amazing having this conversation. It's
an amazing journey as well and glad to
have all the team but also like sharing
with you as well. It's it's great. Thank
you very much for inviting me.
>> Uh we got a special a special little
gift. Thank you to congratulate you and
the team for making this happen. Uh
>> oh, nice. Thank you very much. Very much
on point.
>> 1500 point club. First first model,
right? 1501 for
>> first model. Very kind. Thank you very
much.
>> [music]"
Yil8Gpd6dLg,What's happening under the hood with C's type conversion rules? Go!,"Today's C dev challenge explores a common source of bugs. At first glance, -1 is clearly less than 1, but C's type conversion rules have something to say about that. What's happening under the hood, and which branch actually executes? 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speakers: Laurie Wu",2025-11-24T14:01:07Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Campaign: G4D: Puzzles;,Video Type:G4D SV: Educational ;,ct: AIG;;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/Yil8Gpd6dLg/default.jpg,https://i.ytimg.com/vi/Yil8Gpd6dLg/hqdefault.jpg,21,public,4518,69,19,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,Yil8Gpd6dLg,"Here's [snorts] a C puzzle that looks
perfectly safe until you [music] notice
one small detail. Pour some hot cocoa
and take a look. This one may be
trickier [music] than it looks. It may
seem obvious at a glance which branch
runs, [music] but C might see this
comparison different than you would
guess. What's the issue here? Drop your
[music] answers in the comments."
9oYHU1hdDog,"Agents, AI & The Next Wave: Mike Clark on Vertex AI at DevFest Silicon Valley","AI just reset the developer playbook. In this DevFest Silicon Valley sit-down, Mike Clark (Google's Director of Product Management, AI Agents) sits down with Frank (Puf) Van Puffelen (Google Developer Expert and Engineer), sharing how agents are moving from demos to production with Vertex AI Agent Builder, Agent Engine, and the open-source Agent Developer Kit (SDK). They also discuss what â€œbuild â†’ scale â†’ governâ€ means for real teams.

In this video we cover: 
* Why AI is finally accessible (and why community matters) 
* â€œOutcomes over lines of codeâ€ and the new definition of quality 
* Citizen developers, vibe coding, and what pros should build next 
* Continuous PRs, observability, OpenTelemetry, and open agent protocols (A2A, A2UI) 
* Practical entry points, prebuilt agents, and deployment choices (Cloud Run, GKE, etc.)

TL;DR: Stay curious. Start small. Ship agents with guardrails.

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Event: DevFest 2025 

Products Mentioned: Google AI",2025-11-21T00:00:10Z,"Google,developers,pr_pr: Google Developer Groups;,Purpose: Learn;,Video Type:Interview;,ct: ;,gds:N/A;",28,en,en,https://i.ytimg.com/vi/9oYHU1hdDog/default.jpg,https://i.ytimg.com/vi/9oYHU1hdDog/hqdefault.jpg,1602,public,2031,52,14,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,9oYHU1hdDog,"Hey there everyone and welcome to DeFest
Silicon Valley. I'm Puff and I'm sitting
here with Mike. Right.
>> That's correct. Mike Clark.
>> Mike Clark. See that's the thing I
didn't even know. And Mike is new to
Google and um also new to Deathfest. I
heard
>> that is correct. I've been at Google
just like about 120 days. And so great
to be doing a keynote so fast and
shipping products so fast. It's it's
exciting. I get to attend to a lot of
these events now and they're awesome.
What was your first impression today?
>> My first impression today was the line.
I was super excited by the number of
folks that were coming dealing with
Silicon Valley traffic to come hear my
keynote, but also to come participate in
the day. You could obviously tell folks
that work in big companies, but also
like college and just all kinds of ages,
all kinds of everything. just the just
the diversity of folks that were here
was really exciting walking in. Um, and
then the energy I think everybody
grabbing breakfast, people introducing
themselves, the community that really
mattered here and then just watching how
engaged people were the moment they
walked in the room. Like people couldn't
wait to break down those doors and get
in get in their seats and be at the
front. And that's just a good good
energy, good vibe and everything.
>> I get that. And this is pretty much what
I've found at all fests. I've I've done
these quite a few and it's everywhere,
right? It's the local community
organizing events about Google tech.
Like, can it be any cooler than that?
And and it shows, right? So, if you are
somewhere where there's a Google
developer group, you should definitely
check out if there's a local DeFest
also. We're right smack in the middle of
Death Fest season. So, it's it's an
amazing opportunity to meet great people
like Mike and also to just hang out with
fellow geeks like myself. I I also think
there's a new energy. I think AI has
brought a whole new energy to DevFest
like this, I think it's been a real
reset for everybody. Um, talking to
product managers and engineers and
researchers, unless you were in
academia, you weren't working on
generative AI more than a couple years
ago. And so, it's a great reset, but
everybody wants to be a part of it.
>> Yeah.
>> And it's accessible. And that's not
something that happened in the past
because not everybody had access to a
supercomputer. Not everybody had access
to these things. And so I think those
communities today are just that much
more important in making things
accessible in accelerating everybody's
ability to do it and educate them across
engineering, across product, across all
the disciplines.
>> I love how you're saying right how it's
accessible for everyone now, but it was
it wasn't always like that because I
think we've both been in this domain for
a while, right? We've been in tech for a
bit. Tell me a bit about your
background.
>> Not quite punch cards, but
>> no also not 8 in floppies. I started
with 5 and a/4 in.
>> Um, I started out working for Philips
Electronics while I was still in school.
>> Yes. Dutchies represents.
>> Um, but it's a very interesting time in
tech because the reason I got hired was
all the people that they had working on
tech. Nobody wanted to work on these new
fangled things like Unix and Linux, the
internet and all kinds of fun things
that today is really funny to say. Um,
but rolling out email across 300,000
employees, rolling out these, you know,
First Access, building security products
in its early days, building out
telecommunication systems, being at
Citrix in its early days, and making
access to like lots of things. Um, I've
gotten to be at a lot of interesting
spots, but the big shift for me was I
met the founders of Photobucket and they
were growing so fast. They could only
turn on registrations for 20 minutes a
day and they would fill the servers they
put in the data center the night before.
And it was the first time that I was
watching this like community of sharing
content, of helping others do things.
and I joined them to help raise money
grow from hundreds of thousands of users
to a couple hundred million users in 18
months. We got acquired by News Corp.
Um, sister company to MySpace
and at some really wild times after
watching everything there and running
Photobucket for a little bit, I left and
started a cloud-based security product
to help parents figure out what their
kids were doing online.
>> Okay, it was a really fun and exciting
time. Um, and then spent the last 10
years I was at Visco helping creators
get be better and working on AI there on
the image side. went to Meta, worked on
privacy and bot detection and scraping
and privacy war infrastructure and lots
of projects, but spent the rest of my
time there working on Llama and Llama 2
and Llama 3 and the last year helping
figure out what you could do with agents
and working with researchers there. So I
was exciting to come here because that
connection to enterprises, the
connection to consumer products and just
all these things that were there but
needed to be combined together. Google
just had all the right pieces together
in an open way to help people out.
>> Yeah, it's an amazing place here, right?
Like the the amount of talent that's
around you at Google is a thing I I
definitely noticed after leaving, right?
How different it is. That's why you come
back to talk to us.
>> Yes. No, I love being at events like
this, right? It's also quite clear,
right, why Google would be interested in
having you, right? Because you have the
right background and there's so much
shifting with AI. I love how you just
said, right, where people here are
curious about AI. I notice in my
experience at DeFest also that lots of
especially people that are more junior
to the field are just afraid, right?
Because there are so many headlines of
it takes your job, right? Are there
places that AI is going to affect
employment? Yes. And that is
unfortunate. But there are places that
telegraph, telephone, printing press,
like we've watched these observations
happen over and over. I grew up on a
farm and I watched like a tractor that
could plant four rows, go to eight rows,
go to 24 rows and take away and change
rolls.
But it actually created other
opportunities. And the companies that
are that are rolling out AI in both
responsible ways, but are also the most
successful are the ones that are
building new products, unlocking things
that they've always wanted to do. And
the people that are there aren't
disappearing. The people that are there
are the ones enabling and helping like
finish and fulfill the rest of these
visions. I think it's it's in every
aspect too. It's from how we measure
success as businesses going through
shifts. We've always used metrics, but
metrics are just little mechanical
measures of how fast I'm going or how
much gas is in the car. But what really
matters is outcomes and did I get to
where I wanted to go safely within or
near the constraints of a speed limit
and at the, you know, right boundaries
that I set. And when I look at agents
and I look at AI, it's that same kind of
thing. It's that mental paradigm shift
which has people fearful because anytime
we shift paradigms like it creates
cultural shifts it creates we don't have
language to talk about these shifts and
it creates fear but what I'm observing
is in focusing on the outcome businesses
are wanting to do more and accomplish
more and in doing so I need the best
people to do that and it's the people
that are there and it's the people that
come join afterwards. I love how you're
framing that because right what you're
saying is essentially short-term right
roles are changing which is a thing we
see right and I actually um I I did the
keynote at another death fest and one of
the things I tried to tell people is
like your best opportunity is to stay
curious and just today Addios Osmani
right very senior um um engineering
manager here at Google like they had
done a interview with him and in the
cliff notes this was one of the quotes I
was like yes Adi agrees with me I love
that right and it's so important right
because We don't yet know how these
tools are best used, right? We know
plenty of ways how they don't work
because you can find lots of
non-believers, right? And there's plenty
of people that say they can do anything,
right? And it's like there's probably
some middle ground there. And we're all
discovering the middle ground, right?
Like you might know more than I do about
it, but
>> no. So curiosity, I think, is so key and
an optimism about I'm right. the current
or the new generation of software
developers might not be doing what I
have been doing for decades.
>> Yeah.
>> Right. But that doesn't mean that
there's no no new software developers
need.
>> I let's go back 10 years. If I needed to
accomplish something that was hard and
new, I had to understand a software
language or I had to be friends with
somebody or I had to hire somebody and I
had to have capital to do that. And I
had to have a deep understanding of what
the technical capabilities were and not
go beyond those capabilities. and syntax
mattered and all these things that I got
lost before I ever got to solving the
problem. Today, creativity matters as
much as skill and the nuance is
shifting. I think we're we're watching
folks that were amazing at that
contextual understanding of code
now hit a wall on, okay, I've replaced
all these workflows with agents. What
should I build now? But then I'm
watching the citizen creators, the
citizen developers come out of the
woodwork with like, I've always wanted
to be able to do this, but could never
explain to anybody how. And the LLM gets
me. This vibe coding agent gets me. We
talk about career growth.
>> Uhhuh.
>> But we don't talk about like the
development behind that a whole bunch.
The the like maturity of my
understanding of a skill, my maturity in
as a creator from the first time I take
a Polaroid to where I'm a professional
photographer and going to school for it.
Like there's so many things that I can
do in this maturity of knowledge that I
can now do with AI too. I could start
with um I I could start with AI Studio.
I could start with Lovable. I could
start with all these products that are
very friendly for initial creators, but
then I can very quickly overnight be
working on ADK and like doing something
that could run in my companies like
every employee could work or I could be
using some of the most complicated uh AI
tools out there to solve real core
problems to the business and move it
forward where I wasn't empowered to do
that before.
>> Oh, and I I love that about AI. So um
when when I left Google, I was sort of
like trying to figure out it's like
okay, what do I still want to do? And I
come up with like a personal mission
statement, which is help more people
build more apps.
>> Yeah.
>> And I spent a year at a startup where
initially we were very focused on help
the same people build more apps. And
then right we we also switched to an AI
based product and that allows more
people to build apps and that has always
been right in software
there's never been a prolonged period of
time where there were too many software
engineers in the world right and and so
when I always joke when Visual Basic
became popular it allowed everyone to to
write apps that they couldn't write
before of course most of those apps were
horrible but most of the apps I make are
horrible right? Like the thing is it
empowers them to do something that they
couldn't do without a tool. And we're
now seeing a new world and
>> I I worked on Firebase for a decade. Um
>> right when when Firebase started we gave
you a cloud hosted database and then we
said you can talk to that directly from
your web and mobile application. So we
essentially told every web and mobile
engineer that hey you're now a full full
stack developer
>> which was for them like ooh we can build
so much and then of course I spent a
decade fixing all the problems that
introduced right like because they're
going to make all the mistakes.
>> Yeah. And we're seeing the exact same,
right? So all the the right so so much
is done with VIP coding by people that
couldn't do it without those VIP coding
tools.
>> Y and of course there's going to be
mistakes in there there that you and I
would not make. That's not the point.
Mistakes are fine. The the thing they
built would not exist without it. And
that for me is so key about these about
these tools, right? We're bringing on a
whole new generation of
people that would not have been an a a
builder, I would say, without the tool.
>> Yeah.
>> But now also, right, we still need the
people that have the technical chops to
check the output of the tool.
>> Yes and no.
>> I I think I think there's a couple
things to that. I think as we start to
like think of what is the future of how
we measure quality, how we define
quality
is like is an is an eval based way of
thinking about quality the way to do it.
Does the code have to be good if it
solves the outcome? And so so what if
the code isn't the best? Is it achieving
what the business outcome is? I think
more and more companies I talk to the
number one lines of code or the the
teams that deploy the most code per
person
>> are marketing teams
>> and surprised. Yeah.
>> It's not the best quality.
>> No, exactly.
>> But it solves the biggest problems for
the business. And so if the quality
question is what's having the biggest
impact on the outcome intended, it's the
best code in the world, but it is not
code from an art perspective. And that's
what's that's what's hard. I think as a
as a as a person that wrote and deployed
a lot of code in my life as looking at
friends that are very senior architects
and senior tech leads,
the folks that are doing the best today
and finding the most success
understand that that craft has changed.
There are places to go look in and these
days it's like it's on walls of like
beautiful code with tabs or spaces. We
can have that debate another time. But
that's not what it's about now. It's
about those folks are trying to figure
out how do we become conductors of all
these different types of code and make
them work together and still protect the
boundaries of the business from the
things we care about security and and
you know is it going to run today? What
happens from a continuous deployment
perspective?
But can the marketers just go deploy all
their code and we be even more
successful as a business and unlocking
those capabilities? These are the
paradigm shifts. Like that's where it's
in every single way how we think, how we
operate is just different.
>> Yeah. No, and I totally get that indeed.
Right. Where I work from lots of jokes
usually, right? Where I always joke
where if you're watching one of those
far out sci-fi movies, nobody's ever
reviewing a PR. It's just not a thing.
And and we're moving towards that now.
You're completely right about about
that.
>> I some ways I disagree. I think like I'm
I'm I'm being contentious today.
>> I love that
>> because the nature of what a pull
request is is here's a chunk of work
that I've done. Does it match what the
intended plan was? And can it integrate
in a way? If I look at what an agent
does, a lot of times a coding agent or
other things, we have our definition of
what pull requests are inside of our
process. But when I look at a 10step
agent,
is it accomplishing some of those pull
pull requests just along the way on not
necessarily in code being checked in and
being deployed into a production base,
but there are pull requests that are
happening all day long on like is this
meeting these boundaries from a safety
and a responsibility perspective? Is it
meeting these other boundaries? And so
we live in a world of continuous pull
requests. And in your sci-fi movies,
like whether you're Star Trek person or
Alien or Dune or whichever, behind the
scenes, is that just continually
happening? And is there no longer just
the big celebratory moment because
things are just being released
continually in real time?
>> That's it's a really good point. I
notice when I'm interacting with an uh
agent or or a simpler LLM variation,
right? I often notice that I'm
I'm not sure anymore if code is the
correct format for it to deliver what I
want to review.
>> Yeah.
>> I just don't know what the right format
is. Right. Right. But as a very
traditional software developer,
sometimes a bit saddened about like,
hey, we're moving beyond me typing text
into a box that tells the computer what
to do. That said, I'm excited about the
opportunities, right? And I'm excited
about, oh, where is this going to go?
What is the thing going to allow me to
do? Right? When can I do the um the
Captain Picar computer tre
I always had to like he always wanted
the same tea. Why couldn't he just say
computer tight?
>> I I don't think he had context
management and memory memorization and
personalization and you know I think we
we've evolved beyond the next generation
already. We have all those things
>> built in.
>> I get that. No, I think I I think the
point you're talking about is
fascinating because a year ago if you
were working in agents, you were trying
to figure out how to build code to get
access to things that didn't have API
for computer use for all these other
things. And then if you look today,
we have things like A toA, the agent to
agent protocol, where maybe instead of
needing to generate a bunch of code to
then go execute and operate in this
traditional way, that other person's
going to have an agent and I could just
talk to that agent natively. And if I
can do that, can I have a contract? Can
I do commerce? Oh, well, those protocols
are out there. user interface like being
able to generate user interfaces on the
fly changes things and it's I think like
what's funny is like even what you and I
were just talking about with code and
agents and writing code a lot of our
agent actions don't require generation
of code to solve the things that it
would have 9 months ago or a year ago.
>> Yeah, we could be spending all time on
this but we also need to cover a few
more topics. So I want to still get uh
so this was like lots of context around
AI right but I right you said you've
been at Google for since June right so I
want to know a bit more about your your
role here and then right we're at defa
Silicon Valley where you gave the
keynote and you were talking about the
traffic here in Silicon Valley I was
stuck in traffic so I want you to uh
give me the keynote of the cliff notes
of your keynote
>> cliff notes on my keynote I'm working
inside of Google cloud I'm working on
the cloud AI team um vertex XAI agent
builder is the platform that I work on
specifically. Somebody the other day
called it the tip of the spear for
agents in many ways because it's we are
the team that builds out um the products
we talked about in the keynote and the
products that they're talking about in
the sessions next door which include ADK
which is the agent developer kit open
source tool for people to be able to
develop and so if we go back to what is
an agent it's a model it's access to
tools and it's orchestration
it it is that orchestration and
development and access to tools so that
on my laptop in the cloud somewhere I
can use ADK to build agents. I can build
multiple agents. I can do other things.
Then I need a runtime in order to run
those agents that I build. And so we
have agent engine as a managed runtime
that you host or can can host what you
build um from an agent perspective. And
then we take all of those pieces and we
add on top of it quality features. How
can I understand all the traces? What's
what's the history of what happened?
What's the performance? How do you
evaluate an agent? And how do I build my
own evals? How do I govern them? How do
I build security? How do I build
baselines of identity for agents?
Because we didn't have that last year.
We just treated agents as a piece of
code running here. And in reality, they
need to authenticate. They need to be
able to have permission set against
them. And so, um, we're the team that
builds all these. We build them for
Google. There's thousands of products
inside of Google that are built on top
of ADK running um running agents for all
the products that you can think of. Um
we then support products like Gemini
Enterprise in cloud that just came out
recently. Um which is the it's the entry
product for AI for enterprises.
Everything from deep research, notebook
alm, all these major features plus a
platform for employees to build their
own agents are all built on top of ADK
agent engine and all these same pieces.
We then have other cloud products that
are built on top of it and then we make
these products available directly to
customers. And so um we span all these
customers for what we build. Um and
we're a mix of a bunch of product
managers here. We've got engineers that
are spread all across uh in Poland, in
Kirkland, in Seattle, in New York, here
locally. Um, and then lots of folks
across the company contribute. We work
very closely with Deep Mind. We couldn't
be where we are without Deep Mind. Um,
and we like have so much respect and
depend so much on that team, but then we
work with other teams like Cornell, Core
Dev, all those teams that you're
familiar with as well. So, it's it's a
very interesting role to play and a very
non-traditional role at the same time.
>> Yeah, I get that you're sort of sitting
between those those researchers from
DeepMind almost, right? And then tools
like Notebook LM, which is like this
beautiful piece of functionality that
still needs to be built on top of
something, right? And you're sitting in
between those.
>> Yeah,
>> I love that you were talking also that
you're like the agents, right? You
provide a hosting environment for it.
And I immediately always have to think
like how does this relate to Vertex, for
example, Vertex AI? So, Vert.Ex AI um we
have Vert.ex AI. So, when you build an
agent with ADK, one of the options you
can choose is I want to talk to Vert.ex
AI to choose the model, which model um
and also what runtime do I want to put
that in? Do I want to put that in cloud
run? Do I want to put that in in on top
of Kubernetes and GKE? Um or like the
internal systems as well. And so um we
support all those pieces as well or we
make it possible for people to go deploy
raw on those too. I got it. I love that.
Okay. Sounds like a fascinating product.
I definitely need to read read more on
this and do more research.
>> Yeah. And so the keynote was all about
this week. We launched a whole bunch of
new products. Um ADK developers started
on it a year ago in October and launched
it publicly in March or April and Agent
Engine was very similar. And what we
took is all the feedback we got from
customers both individual developers and
large enterprises on things that they
needed and things that they wanted. And
we launched this kind of new paradigm
shift where we're not just focused on
building because that's where we've been
for the last 6 months, but how do we
build and then how do we help you scale?
How do we help you deploy? How do we
help you know what's going on,
understand performance, understand
evaluation? And then how do you govern
it so that you can run it in ways that
you can go hands-off because I'm a small
developer and I just want the base
levels of protections or if I'm an
enterprise and I care about every single
aspect and so like color health that was
in talking they build products to help
women get and know when to get um breast
examinations and and it is such a like
helpful product but deals in healthcare
and so like lots of regulations and lots
of governance requirements and so they
use that full stack and so build scale
govern is like where we think developers
are in that life cycle. We've all played
and we've all been curious. Now how do
we turn that curiosity
into careers for ourselves into products
that help our businesses or into new
businesses that I want to go build.
>> You talked a lot now about the enhanced
observability of the agents, right? for
me a very Googlebound term but right
like all the the analytics outputs that
you get out of that of like is the thing
operating which is very much a DevOps
type thing right versus how do you build
a product right how do you build the
agent which is very much a
>> essentially the early phases of classic
software engineering right and then in
between there there's like yeah the
magic of using AI for all
>> and doing as much of it open as we can
>> ADK is open source we're building on
open telemetry the protocols that we've
built along the way from A2A for agent
to agent protocols to the payment
protocols ATP A2P along with AUI didn't
say that on stage but A2UI which is a
new agent to user interface that
announced yesterday um these are all
things that we are providing open source
we're donating to Linux Foundation in
those protocol cases
because we want everybody we want to we
want to we want to rise all ships not
just not just ours because we think
that's what's important for agents to be
to win.
>> I love that. I love your reference to
one of my favorite quotes indeed. Right.
A rising tide raises raises all the
ships or the other way around. A rising
tide raises all the ships.
>> Yeah. No, exactly. I love that. Okay.
Any other big announcements that you did
in the keynote?
>> In the keynote, we talked about um the
entry point. It's been kind of
challenging and so it's it's a big
hurdle like do I have a weekend or a
week I'm going to take vacation to go be
curious about agents which I think for
you like you probably remember doing
that where code language like across the
holidays I'm going to spend two weeks
learning this me like not having typed
professionally writing code for many
years wrote an agent built an agent and
did so in 30 seconds and had it running
up in cloud
>> crazy
>> and could interact with it and yolo
modeed with Gemini CLI I'd have it
edited on my behalf. And so, like, these
things are right there. We have a bunch
of pre-built ones. One of them was a
travel agent. And I hit deploy and went
back a few minutes later after it
deployed. And I was able to ask like,
where are the Moai and how can I get
there? And it was able to tell me about
Rapanui and the history and the hotels.
And he's like, also, if you've never
been, you need to go.
>> I need to travel more is what you're
like. I like that. Okay, Mike. I think
that's all we have time for. So, thank
you so much. I think we're on a mic
base. Right. First name.
>> That's There we go. We are. Thank you so
much. This is awesome.
>> And everyone there remember this is what
DeFest is all about. Finding out these
all these new technologies, right? Main
thing if you're just getting started in
this domain, don't be afraid of AI,
right? It's just a tool to help you
figure things out. Lesson for today,
look at what Mike's team is doing and be
curious. See you at the death festival."
9EGtawwvlNs,Build an AI Agent with Gemini 3,"Gemini 3 is our most intelligent model yet that helps you bring any idea to life. In this video, youâ€™ll learn how to build an AI Agent with Gemini 3 Pro and the Agent Development Kit (ADK). Youâ€™ll explore how to build a single agent with access to Google Search as its tool, and interact with it using a web UI. We will also learn how Gemini 3 Pro uses Thought Signatures and we will explore its chain of thought when the agent receives a prompt. 


Resources:
Getting started with uv â†’  https://goo.gle/4a7M7KG 
ADK Google Search Tool docs â†’ https://goo.gle/3LGFKUR 
Github Repo for the AI Agent built with Gemini 3 Pro â†’ 
https://goo.gle/4pq0pep 

Timestamps:
0:00 - Introduction to Gemini 3 Pro
0:32 - Initializing the Project & Installing ADK and GenAI libraries 
1:25 - Setting up Google AI Studioâ€™s API Key & Creating Agent Scaffolding with ADK Create
2:00 - Implementing the Google Search Tool 
2:33 - Configuring the Gemini 3 Pro Model 
3:14 - Deploying the Agent to ADK Web 
4:29 - Testing the Agent
5:24 - Thought Signatures and Analyzing Chain of Thought
6:40 - Conclusion

Speaker: Smitha Kolan 
Products Mentioned: Gemini, Google AI, Agent Development Kit (ADK)",2025-11-20T21:45:49Z,"Google,developers,pr_pr: AI DevRel (fka Core ML);,Purpose: Learn;,Video Type:DevByte;,ct: ;,gds:N/A;",28,en,en,https://i.ytimg.com/vi/9EGtawwvlNs/default.jpg,https://i.ytimg.com/vi/9EGtawwvlNs/hqdefault.jpg,383,public,78822,2280,106,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,9EGtawwvlNs,"Hi everyone. Gemini 3 Pro just launched
and I'm super excited by all the new and
improved updates. The model is really
great at advanced highle reasoning and
things like following along with really
complex instructions which make it ideal
for building agentic operations and
ultimately building great AI agents
with. So in this video I'm going to show
you exactly how you can go about
building your very first AI agent with
Gemini 3 Pro and the agent development
kit. To start off, let's create an empty
project folder where our AI agent will
live. I've done that and I've named it
Gemini 3 Pro AI agent and I've opened
terminal. We're then going to initialize
our project folder with UV. UV is a
Python package manager. And if you don't
have it installed, you can find
installation instructions in the
description box below.
And next, we'll add the two libraries
that we need for this project, which is
Google ADK's library and Google's JDI
library.
Now, it's time to add your Google AI
Studio API key. To find this API key,
all you have to do is navigate to Google
AI Studio and get your API key there. Uh
once you have that, you can export it
here in this type of command. Now we
will activate our Python environment.
Once we're in our Python environment, we
can now create our ADK agent scaffolding
with just one simple command. All you
have to do is type in ADK create
followed by the name of your agent.
At which point you'll be prompted to
select a model for your agent. Since
we're going to be using Gemini 3 Pro,
I'm going to select option two.
And you can see the files which have
been created as a result of that
command. Now we can hop into Visual
Studio Code and start writing in our
Asian. py file. Once you navigate to VS
Studio and you open your Asian py file,
you'll notice it already comes with some
boilerplate code. Let's remove all of
that. Let's go over to ADK's
documentation and copy a piece of code
from there. We're going to be making use
of this code right here, which comes
under the Google search tool in ADK's
documentation. The link for this will be
in the description box below, as well as
the link to the repository of our final
agent as well, because we are going to
be making some changes to this code. So,
let's go ahead and copy this. Head back
to VS Code, and then let's paste it.
Now, we want to make some changes to
this code. First off, we want to use
Gemini 3 Pro instead of Gemini 2.0
Flash. So let's make that change.
This is the model name for Gemini 3 Pro.
Next, I want to change the instruction
set. So instead of this single line of
instruction, we want to make it a bit
more detailed. We also want to give
instructions to the agent to site its
sources implicitly by providing the
answers clearly based on search results.
And for this basic search agent, we are
giving it one tool which is access to
Google search. The final thing we want
to do is make some changes here.
And we also want to import async io.
And that's our basic agent which is
built with Gemini 3 Pro and it has
access to Google search. Now back in
terminal, we are going to be deploying
our agent to ADK web.
You can now use this link to access your
agent.
We can now start interacting with our
agent in ADK web. I've been meaning to
take horse riding lessons in my city and
but I just don't know where to get
started. So, let me ask about that.
So, now we're getting some detailed
response from our agent on exactly where
I can find these lessons. It has also
listed out places and locations where I
can get started and also categorized
them best in terms of how serious or how
basic they are.
Let's explore the model's chain of
thinking to get to this response.
To do this, let's explore the events tab
and click on this tab right here with
the text. So this text is what is being
returned by the model.
And then if we go over to the next
event, we get to see something very
interesting which is the thought
signature of Gemini. And the thought
signature in Gemini 3 Pro helps the
model to understand the context in the
conversation. We also see some really
interesting data which is the websites
that Gemini 3 Pro actually searched. So
horseport.com, the best toronto.com,
Reddit and many many more. And then if
we scroll further, we actually see the
web search queries that Gemini 3 Pro
does and in what order. So first it
starts off by doing a generic search for
horse riding lessons Toronto. And then
it digs deeper and does uh horseback
riding near downtown Toronto and then
followed by best horse riding schools
Toronto. And at that point, I'm sure it
determines some of the names of the
horse riding schools available in that
area. So you see the searches actually
start to get way more specific. And then
it even looks up if it's accessible by
public transit, which is very
interesting. And then it also looks up
on the status of whether these stables
are actually open. So from just a single
prompt of where can I learn horseback
riding in Toronto, Gemini 3 was able to
go through this chain of thought
thinking in order to arrive at this
response. And I think that's pretty
cool. There's so many more amazing
agents that you can build with Gemini 3
Pro and ADK. If you're interested in
learning more, check out the links in
the description box below. And happy
building."
sCXuCczPE8A,What's on the newsdesk for Google Developers this November? Take a peek!,"From new updates to Google Play game to unveiling ADK Go, see whatâ€™s new for you, a Google developer!

Watch more Google Developer News â†’ https://goo.gle/4e8Rysd  
Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Meghan Mehta
Products Mentioned: Google AI, Agent Developer Kit (ADK), Google Play",2025-11-20T20:00:55Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Campaign: ;,Video Type:G4D SV: Educational ;,ct: AIG; ;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/sCXuCczPE8A/default.jpg,https://i.ytimg.com/vi/sCXuCczPE8A/hqdefault.jpg,34,public,3685,47,4,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,sCXuCczPE8A,"on November's episode of Google
Developer News. We're already feeling
thankful for all the amazing updates
[music] we have for you this month.
First up, Google Play's latest updates
are designed to streamline the developer
experience [music] and help apps connect
with a broader, more diverse audience
worldwide, and ADK has a whole slew of
[music] announcements to unveil this
month. But you'll have to head over to
our YouTube channel to find out more on
this month's Google Developers News
show.
Like go right now. It's happening right
now."
hunRGj4Bpb0,Keep your code fresh and clean with YAGNI!,"Every extra line is more to test. The solution? Keep it simple, and build what's needed now.

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Crystal Endless",2025-11-20T19:30:53Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Campaign: TIPS;,type:G4D SV: Educational ;,ct: AIG;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/hunRGj4Bpb0/default.jpg,https://i.ytimg.com/vi/hunRGj4Bpb0/hqdefault.jpg,34,public,8078,205,9,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,hunRGj4Bpb0,"Guess what? Yagney. Yagy stands for you
aren't going to need it. And it's a
great rule of thumb to help keep your
code fresh and clean because unnecessary
code stinks. That extra base class with
only one subclass, yagney. That flag
that's always false. Yagney. That public
method no one calls. Yagney. Every extra
[music] line is more to test, more to
document, more to confuse your
teammates. It all adds up. So keep it
simple. Build what's needed now, not
what future you thinks might be [music]
nice to have. Your code and your
teammates will thank you.
>> [music]"
L0QHMLx02l4,"Introducing ADK Go, User Simulator, and more! - Google Developer News November 2025","Welcome to this monthâ€™s Google Developer News for a rundown of whatâ€™s new for developers! Dive into the new Google Play Console overview pages that provide a live review of your app's performance against key goals like user growth and monetization. Discover how Gemini-powered language translations are making localization easier for app bundles. And ADK Go? Watch this episode to be caught up on the latest, low-latency news for Google developers. 

Chapters:
0:00 - Intro
0:31 - Gemini-powered app localization
0:55 - Streamlined deep link validation
1:08 - ADK user simulator for AI agents
1:39 - Introducing ADK Go for Go developers
1:53 - Conclusion 

Resources: 
Google Play â†’ https://goo.gle/4ih9DHw 
User stimulator â†’ https://goo.gle/3LR1IEv 
ADK Go â†’ https://goo.gle/4rcg0Qm 

Watch more Google Developer News â†’ https://goo.gle/4e8Rysd  
Subscribe to Google for Developers â†’ https://goo.gle/developers 

#GoogleDeveloperNews 

Speaker: Meghan Mehta
Products Mentioned:  Google AI, Agent Developer Kit (ADK), Google Play",2025-11-20T17:02:19Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Video Type:DevByte;,ct: ;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/L0QHMLx02l4/default.jpg,https://i.ytimg.com/vi/L0QHMLx02l4/hqdefault.jpg,132,public,5225,140,4,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,,
uzFOhkORVfk,Google Antigravity: Hands on with our new agentic development platform,"Explore Antigravity, Google DeepMindâ€™s innovative new AI developer coding product, with Varun Mohan on Release Notes. This episode dives into Antigravity as a powerful agent development platform, integrating a familiar IDE experience with browser verification and Gemini 3.0 capabilities. Discover how developers can orchestrate complex agentic workflows, leverage artifacts for task communication, and balance AI automation with human collaboration. Learn about the philosophy behind building next-gen agentic experiences, the platform's multimodal strengths, and its role in accelerating software development at scale.


Listen to this podcast: 
Apple Podcasts â†’ https://goo.gle/3Bm7QzQ 
Spotify â†’ https://goo.gle/3ZL3ADl 

Chapters:
00:00 - Introducing Google Antigravity
04:02 - Evolution of AI in coding
04:53 - Beyond writing code
06:21 - Ideal Google Antigravity user
09:48 - Evolving user personas
11:46 - Agents versus the IDE
14:46 - Human-agent collaboration
16:43 - Local versus server-side
18:50 - Self-improvement and knowledge
21:29 - Generalizing agent capabilities
24:20 - Naming Google Antigravity
27:04 - Integrating Google's AI models
27:59 - Demo: Airbnb for dogs
28:48 - Understanding artifacts
29:51 - Asynchronous user feedback
32:16 - Agent manager workflow
33:17 - Browser actuation demo
34:36 - Browser for research and testing
36:45 - Parallel agent conversations
41:04 - Agent task best practices
42:51 - Future of Google Antigravity

Watch more Release Notes â†’ https://goo.gle/4njokfg 
Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Logan Kilpatrick, Varun Mohan 
Products Mentioned:  Google AI, Gemini",2025-11-20T04:40:52Z,"Google,developers,pr_pr: AI DevRel (fka Core ML);,Purpose: Learn;,Video Type:Podcast;,gds:Yes;,ct: AIG;",28,en,en,https://i.ytimg.com/vi/uzFOhkORVfk/default.jpg,https://i.ytimg.com/vi/uzFOhkORVfk/hqdefault.jpg,2690,public,22578,555,73,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,uzFOhkORVfk,"[music]
Hey everyone, welcome back to Release
Notes. My name is Logan Kilpatrick. I'm
on the Google DeepMind team. Today we're
joined by Verun Mohan, who's the co-lead
for Google Anti-gravity. Uh Verun, thank
you for being here. Let's dive in and
talk about anti-gravity. Do you want to
sort of give us the TLDDR and like the
high level of what the product is and
all that stuff?
>> Yeah, so Anti-gravity is an agent
development platform. Obviously, it has
an IDE, but there are a couple sort of
key differentiators we've we've come to
market with uh to kind of start with.
So, in the very beginning, one of the
things it has is it has a familiar
editor that all of us know how to use.
Um, but it also has this agent manager
that allows you to orchestrate many many
agents that can operate over your
codebase and in addition a brand new
surface which is the browser. Agents now
with the advent of Gemini 3 can also
actuate the browser um in a very very
capable way. On top of that, the agents
are able to interact with the developer
in these kind of indivisible sort of
tasks that are and units called
artifacts that sort of give the
developer uh verifiable steps that the
agent is kind of going through and the
user can kind of go and back and forth
with these artifacts to kind of complete
longer and longer tasks. kind of the
inspiration here is and a lot of it was
by seeing Gemini 3, we basically saw
that models are able to now go longer
and longer with less human intervention
and we want to take maximal advantage of
this to to kind of give the best
possible experience to developers.
>> Yeah, I love that and I'm excited to
deep dive on like all the nuance of of
anti-gravity and some of the innovation
from a product perspective. actually
just to sort of set the stage um you
came over to DeepMind joined Google to
start focused on building this product
do a bunch of agentic coding stuff just
to sort of like that uh experience
[laughter] of like coming over and like
how it's been actually now seeing and
obviously I think you all were you know
sort of training models in some capacity
before but like now being sort of super
close to like the frontier foundation
model perspective I'm curious how that's
like influenced your sort of like level
of ambition but also excitement and like
what you're thinking about from a
product experience and like also just
generally what has what has it been like
to come and and join deep mind and do
all this stuff.
>> I think first what you said I think that
resonated the most is the level of
ambition. I think the level of ambition
has increased tremendously and that's
kind of because we we don't need to
reinvent the wheel in a lot of different
pieces, right? The model is like an
incredibly important piece to this
experience, right? Uh we can we can see
this if you have a bad model and it's
not able to go and follow instructions
properly, not able to take many many
steps in a row, it doesn't really
matter. You're kind of dead on arrival
in a lot of ways. And I think one of the
benefits uh of coming here that we've
sort of seen is how much we are pushing
the frontier of models not only for
coding but across many different axes
like multimodal capability UI control
all of these different pieces I think
are really really critical to deliver a
great experience. When I look at the
kind of industry over time, you can kind
of see how things have transformed,
right? If you go to 2021, you see the
advent of GitHub copilot, right? And and
it had kind of basic autocomplete and
that was already kind of groundbreaking
at the time, right? And then after that,
you had some of these chat experiences
with Gemini and Chad GBT. Um, and that
started to make its way into the ID as
well, right? A chat panel on the very
side uh that kind of understood your
codebase. It was very personalized to to
all the private data inside a company or
inside an organization. And then after
that you sort of had these agentic ids
and agentic experiences. And I think the
next version of this it can no longer be
the case that you understand code only.
They look at the browser. They kind of
do some research, right? They might read
some papers. Um on top of that they
might look at their bug their bug
reports to see what what tasks to
ultimately solve. And I think that's
what got me and the team very excited
about the fact that our models are
getting very capable not only in the
silo but across many many dimensions.
This is one of the threads that I I
personally had not sort of deeply groed
until I started thinking about this like
coming up to these launches of just like
how important but also different and
impressive it is to bring those like non
sort of coding developer
even though developers are doing
research and like non-traditional like
developer tasks into this experience and
I'm curious like how you sort of like
think about finding the balance because
obviously like developers do a whole lot
of like their own you know, you know,
meetings and they have calendar needs
and like where do you sort of draw the
line of like what is like stops being a
core part of that experience and you
would sort of like outsource out of that
main product experience versus like what
do you pull in and like maybe research
is one of those things and you know some
like QA testing or deployments or stuff
like that like where where's that line
in your head right now?
>> Yeah. So when I think about what does a
developer ultimately do, writing code is
maybe a very small chunk of what they
ultimately do, right? They write code,
they review code, they test code, they
debug code, they deploy code, right? Um
there's many many aspects of this. And
for each of those, they might do they
might actually have a completely
different surface, right? Um I'll give
you an example, right? Inside inside
Google, we use Google Docs, right? And
we might draw write up system designs
inside Google Docs. And it's quite
important actually to understand the
system design um if you're going to go
make a large scale change in the code or
you're going to review code right for
that matter. And in that case I think if
all we did was looked at the code only
um we'd be operating in a very narrow
silo. We wouldn't be able to solve these
very hard problems for the developer.
And the way I like to look at it is it's
almost like you're chopping wood right a
developer has 100 units of time. Some
units of time is spent writing code. And
we can if we all we do let's say that's
20%. If all we do is we make it very
easy once you know exactly what to do to
write the code I think there's a limit
to how much more we can accelerate
software development
>> right um like I think I think basically
if you look at the hierarchy of needs
and what we're able to provide for
developers it's like what what should I
build how should I build it and building
it and I think the models if we're only
able to do code they help with the
building it part but how should I build
it it's actually a very very interesting
question that I think AI can help
accelerate tremendously and I think
anti-gravity has a lot of the pieces to
kind of push in that direction as well,
right?
>> Who who is actually the ideal user for
anti-gravity? Like is and both like
actually there's lots of folks using it
internally. It'll it's available
externally now. Like who do you think is
going to be like the most successful
user persona today?
>> I think it's very important in all of
these products in general to be a dog
foodter of the product, right? Um
otherwise you're kind of just like
implementing a bunch of features and you
kind of see these products where all
they have is a bunch of features and
you're unsure exactly what's going to
get used or what what's not going to get
used. So internally at Google um a lot
of Google DeepMind actually does use uh
does use the product. Um this gives us
like a lot of rich feedback about how we
can iterate even before we ship
something to the public. For folks who
don't have context, the internal Google
codebase, all this infrastructure is
extremely I would guess on average much
more complex than other environments
like anywhere in the world. I think
that's a really unique requirement and
like challenge of doing this actually
inside of Google. It' be easier to like
build something completely external that
doesn't have to touch Google stuff and I
think the internal stuff is extremely
difficult.
>> Yeah, exactly. Um I think this is
definitely hardening our our our product
uh to work with the largest enterprises.
I think if any kind of talks to us and
is like hey is anti-gravity working on a
very large code I think very few
companies have a larger codebase than
Google surprisingly yeah we you know
because of the product and you know
targeting all these developers folks
like Sergey use the product all the time
um he's he's shipping a lot of CLS and
PRs now which I guess is a is like a
really good thing uh for him to be dog
fooding the product as well uh but I
think at a high level sort of we we're
trying to make sure that a lot of the
researchers inside the company are are
able to use the product and the benefit
here is they can kind of also see the
the capabilities of the Gemini models um
in real world scenarios. It also gives
us a lot of rich feedback of what is it
like for anti-gravity to be useful for
solving very very complex tasks. Right.
I think there's one thing for if we're
viating very ephemeral applications, but
I guess the real persona and you've got
to it is the developer.
>> Yeah.
>> Right. We want developers to be able to
build their dreams, right? We want them
to be able to build uh very very complex
apps that solve really hard business use
cases. We'd like singular developers to
build entire companies, right? I think
this is like a line that's floating
around um a lot in the ether right now.
But I think as a as a whole kind of
working with the with a lot of the
researchers um we've also learned a lot
right about about the style of task that
we need to be able to solve. And this
dubtales back to the conversation of the
model only being good at code. It's very
clear if we want to accelerate a lot of
research which we're very excited to do.
um the model will need to do a lot of
things beyond just write code, right? A
researcher might fire off a training
job. They might kick off an eval. They
might need to analyze the eval. And this
is like a lot of boilerplate work that
they're doing, a lot of which doesn't
sit inside the editor, right? And sits
on many, many other surfaces. And this
also was the inspiration for why we want
the browser, right? It's not just for UI
testing kind of simple web applications.
It's for actually actuating um the like
kind of the browser for very complex use
cases that we'd like we'd like for for
the general developer.
>> And how do you think about the do you
have a sense that this user persona will
change over time? Like obviously the
whole sort of like tailwind of like who
is a developer is changing what it means
to create software is changing. I do
think the experience today like feels
very developer centric and do you think
like over time you'll sort of like add
abstraction layers on top of that? So,
one interesting thing we've already
found is folks that are not technical
are already using anti-gravity
internally, uh, which is which is very
cool to see. Um, I think the way I would
like to for us to think about it is we
should be constantly trying to raise the
ceiling of the product, but also making
it really really easy for folks that are
just technical adjacent to be able to
get value from the product, right? Um,
but I think my worry about us opening up
the floodgates today is we kind of
become a product that is not good for
either persona, right? And that's like a
big worry that I sort of have. Um, like
in my mind, if I'm looking at a pure
vibe coding use case where you want to
quickly build an app and then have all
the bells and whistles of AI infused in
it, AI Studio is an awesome platform to
go and do this. And I think we should
not be trying to replicate that
experience, right? We would love for
people to use AI Studio for those
experiences. And if they have an
application that they'd like to kind of
kind of continue to iterate on over time
or add more features to over time, we
would like anti-gravity to be the right
platform for that.
>> This was you and I were going back and
forth on this and I think it's actually
an interesting uh it's an interesting
point to make mention of like we will
actually have the AI studio to
anti-gravity path. Um we we're still
figuring out the details and making sure
that it works well and all that stuff.
Like I do think like for a lot of folks
they start even technical folks they
like start with this vibe coding journey
and they're like okay great I sort of
can now start to materialize my idea I
am a software engineer I am a developer
I really want to go deep on this and
make sure I've made all the right
technical decisions to scale up etc etc
and I think being able to like take
things that you vibe coded out of AI
studio go to anti-gravity I think is
going to be this like magical experience
so I'm I'm excited for that to land
hopefully the team
>> I think I think that's going to make
these apps like more econom ically
valuable to the people that are building
them
>> 100%. Um I'm also curious like this um
this sort of like broad industry trend
of like agents versus the IDE and
obviously you sort of like have both of
these. Do you think there's a world
where like you stop
like the IDE actually goes away and you
sort of are truly orchestrating agents?
And I think I think and I don't know how
much you want to talk about, but like
there's like literally like design
product decisions that you all been
making the last few days about like what
is the default experience? Like what do
we drop developers into? And I think
there's actually an this is an
interesting thread of like what how do
you meet developers where they are today
versus like also showing them this
future of what you could do. Um and
yeah, I'm curious like that tension
actually as you as you build this. Yeah,
I think for the for like the foreseeable
future um the editor is not going away
and that's purely because I think
developers need the control to
ultimately see the underlying code and
edit it um right at the lowest level
like taking that away is going to be
like catastrophic in a lot of cases,
right? But I think what ultimately does
happen is maybe the time you spend
actually writing singular lines of code
in the editor doing autocompletes and
all these things may drastically go down
over time, right? And then what
ultimately ends up happening is the
editor looks like any other work surface
for a developer. Right? If you think
about it, there are many places that a
developer generates a work product.
Editor is one. Maybe inside the their
code review tool is another. Uh maybe in
a Google Docs or Docs or Sheets is
another one. And I guess all of these
become like important services um that
kind of an agent can operate on top of
and collaborate with a human, right? Um
but I don't think it is going to be the
case that the editor itself is going to
go away. I think the entry point of how
much time people spend looking at
individual lines of code in the editor
instead of looking at artifacts which is
a new new kind of paradigm that we
brought together with I think the
trade-off will be artifacts will the
amount of time people spend looking at
artif art artifacts and verifiable sort
of units u from the agent is going to
increase right and I think the nature of
why that's going to be the case in
general is let's imagine the AI is going
to write most of your code then actually
a lot of it is going to be the AI needs
to figure out and the agent needs needs
to figure out a way to make it easy for
the developer to review the code without
kind of kind of like making them write
every piece themselves, right? So there
does need to be this higher level
abstraction that lives between the the
kind of developer and the agent. Um and
uh and yeah, the editor may not be the
the like the editor or the lines of code
themselves may not be that interface.
>> Yeah, I I totally buy that to be honest.
I feel like it is this like evolution of
like what are and like today you're
reviewing PRs uh or CL's if you're
internal to Google. Um and like in the
future maybe it literally is you're
reviewing like the implementation plan
or like the execution plan or like the
the final test suite or something like
that. Um that that brings me to another
thread around sort of this like agent
human collaboration and how like
verification and autonomy and all that
stuff is like baked into the product
experience. Obviously today like models
can't run for at least you know reliably
in a lot of cases aren't running for
days on end or even like hours on end.
It is a little bit sort of like faster
of a feedback loop. So I'm curious how
you all are thinking about in
anti-gravity today like when is the
human in the loop versus like what
decisions can you just like fully
delegate delegate and let the model run
with. So actually we we give you a way
in the product to kind of have this
thing called agent assisted development
which is you can actually specify the
level of autonomy you want to give the
agent um and the agent itself can decide
hey like for these kinds of terminal
commands that it runs or for these tasks
does it want to notify the user does it
think the threshold is high enough uh to
notify the user and obviously we have
like evaluations internally to make sure
that these are reasonable but is this
reasonable um are these commands that
are being run something that it should
tell the user and I think you do need to
kind of kind of trust that the AI will
do this, but at the same time, you need
to give kind of the escape hatch for the
developer to kind of provide feedback.
And that's why kind of in the artifact,
you can asynchronously provide feedback
anywhere. You can provide feedback
almost like comments on a on a Google
doc, right? Um in in in on in on the
artifact um also on the chat panel, you
can always write a comment and it will
be folded into whatever the agent is
doing. The agent will decide when the
right time is to fold in this, right?
So, I think there's going to be a mix of
everything because I I see a world and I
already see this right now at Google
where folks are going and grabbing a
coffee and coming back and they're
looking at the work that the agent has
done and they're like, ""Hey, actually, I
want to tweak it this way and and it
will go and take that feedback and and
go and continue working for the next 30
40 minutes."" Uh, actually, yeah, it's
kind of remarkable that this has
happened in such a short period of time.
>> Yeah, that is super interesting. I have
a a very tactical question which is this
um and we actually see this across the
ecosystem for a lot of these coding
agents today is what's happening um on
server side versus what's happening
locally and I think this is an
interesting distinction and I think
today if my understanding is correct is
gravity is running locally on my device
um obviously it's connected to a server
a model that's hosted on a server
somewhere but like the code is being
generated and stuff and it's visible on
my computer um how do you think about
like that and obviously that means like
I have to have my my computer has to be
on in order to continue to make
progress. How do you think about that
experience today versus like the server
side background agent sort of running in
a container somewhere and like the
trade-off between those two and like do
they converge at some point? Is it the
are they completely different use cases
long term as well?
>> No, I think that they do converge. Um I
think that's because this isn't like you
have to imagine a model or an agent that
is able to go for a very short period of
time and complete a task. Um or can go a
long period of time can also complete
the task in a short period of time. The
model is not going to be somehow able to
only think long but not think short
right it would be a weird property and I
think that's what all of us here at
Google and Deepbind are really pushing
for right models that can go for longer
and longer and execute tasks and more
and more complex tasks right and you're
totally right. I think I think these
tasks outliving the the sort of
developers machine is going to be quite
important. But I think all the
principles that we just talked about
about being able to provide feedback and
all these different pieces is very very
important. And I think the reason why
it's important and why some of these
asynchronous experiences haven't taken
off nearly as much at least in the
market um I think is actually because I
I don't know about you but I'm really
really bad about communicating my
intentions to the model.
>> Right? When I tell it hey do X in the
back of my head I have a hundred other
requirements. Yep. But I'm not very good
at specifying them. Like I could
actually tell all of our users, hey,
specify every requirement up front, but
some things you only learn after you see
the code.
>> Yeah, it's a very iterative journey.
It's kind of like why you can make a PRD
or or uh a system design, but ultimately
when rubber meets the road and you
actually build this thing out, there's a
there's a gap between the two. You
realize, hey, there this piece um
actually didn't fit in the way you
thought it did. And because of that
actually despite having a very
asynchronous system you need mechanisms
to allow the user to kind of understand
the progress so far and iterate on it.
So I think all the pieces we have built
into anti-gravity are going to really
really like help in that environment as
well. I think you really want that
there. Right. I find it very hard to
believe I have a very complex codebase.
I say dox
and specify a sentence. It is going to
come back with exactly the right thing I
want. Right? Because one approach that
it could take is delete all my code and
rewrite it all from scratch and then do
X. But I wouldn't really want this,
right? And I actually think this
dovetailes into one last thing that we
do have in anti-gravity that I think
helps with this experience, which is
that I think one of the things that I
love about about Google as a product as
a whole, and my goal is not to just say
only positive things about Google is it
almost feels like it does it does read
your mind and understand context, right?
Um like for instance, if I just did if I
just sort of like searched a lot about a
basketball game and then I wrote curry,
it's going to show me Steph Curry. It's
not going to show me chicken curry.
Yeah. Right. Um, and I think you want
the same experience also when you're
using the Asian and because of that
actually self-improvement is kind of
built as a key sort of primitive into
anti-gravity actually. So as you're
>> knowledge panel,
>> this is the knowledge panel. So the idea
here is like as you're using the
product, it is actually building up
knowledge of your usage. So the next
time you use it, it is not rederiving
everything, right? Because you can
imagine these conversations that you
have with the agent, they're very rich
in information. They're not only they
they teach the agent things that are
outside of the codebase because a lot of
knowledge it's kind of like what you
said right there's a lot of meetings
that we do have and that builds
intrinsic knowledge that doesn't live
inside the codebase and the codebase
might be a little bit out of date on
what the actual intent of the user is
and I think that kind of helps you
bridge the gap but despite all that I
don't think that this is enough I do
think you want an interactive experience
even if the model is able to go out for
days weeks and months right imagine the
idea of of uh of someone telling you hey
Logan go implement this feature and
You're like, ""Okay, uh, the agent says
it's going to take 3 weeks."" You look at
it at the end of 3 weeks and it's wrong.
>> You're going to be like, ""It's it's bad,
right?"" Like release date needs to keep
getting bumped at that point.
>> Yeah.
>> Yeah. And I there's so many u human
analogies where like um if you were an
engineering team like the worst outcome
is I as a product person say like I need
X Y and Z feature and then I show back
up in 3 weeks after they've completely
done everything and then I'm like this
actually isn't what I want. Like you
check in there's an process. So I think
I think it like mirrors the what what
humans do today as as a best practice
>> as as you have been talking and I think
more broadly in this moment of like um
you know the anti-gravity product coming
together and sort of agents coming
together it is interesting one of my
reactions is just like how or the
questions in my mind is just like how
much do some of these like agentic
coding experiences generalized across
domains that have nothing to do with
coding like as you're describing like
you know the human in the loop
components I want to be able to like
have knowledge. I want to be able to
like do tasks in the browser and
research. Like I start to imagine I'm
like this is just like all like all work
fits in. And obviously some work you're
generating code for, some work you're
creating other artifacts. And I'm
curious like um yeah maybe there are a
bunch of bespoke things that will
continue just for software. But have you
thought much about like how much this
generalizes to just an agent overall?
>> I guess you know the hard part about
this is this goes to show the power of
these models.
>> Yeah. just that they are like generally
very intelligent.
>> Yeah.
>> Right. Um and that enables you to solve
many tasks. And I think you're totally
right. I found myself using anti-gravity
for very different things, right? Um I
think recently I asked it to analyze a
bunch of uh a bunch of metric data,
right? Um and this metric data had
nothing to do with the code or or
anything, right? Actually was on some
website. I wanted to analyze some sports
stats.
>> Yeah.
>> And actually wrote like a very local
script. It did some stuff generated me
an artifact, a plot. I iterated on the
plot a little bit in image space and
then after that I got a brand new plot
and now I've been kind of like using
this internally just to kind of like
analyze I guess maybe maybe to give you
an example like I I just started getting
to baseball and show Otani seems like
kind of otherworldly figures. I've been
just analyzing other baseball players to
see is there anyone remotely close
surprise surprise no one is remotely
close but I just wanted to kind of get
get some metrics on that and um I think
I think it's very very easy to see how
this generalizes beyond just uh beyond
just software development. I guess the
the reason why we are not talking about
this as if it's a a very general purpose
tool is I always have kind of a deep
worry that hey if we go and try to make
the product great for everyone we will
make the product great for no one.
>> Yeah.
>> Right. And that's like a deep worry that
I sort of have and I think we deeply
understand the developer. We're all
developers um ultimately. And I think I
think one of the things that is true
that I think is exciting is the number
of developers in the world has been
growing, right? Um so maybe maybe that's
the right way to kind of formulate this
is the number of people that are
builders and developers is going up,
right? And and that's just because the
ease of of building has been going up,
right? You look at the very beginning,
people were writing assembly.
>> Yeah. very very hard to we made assembly
is horrible. If you haven't written
assembly, don't write assembly.
>> Exactly. And then after that you had C
um right and then you had C++, Java,
Python and Python if you just look at
the gap in what a Python developer needs
to know and what a assembly developer
needs to know. It almost looks like they
are not the same thing.
>> Yeah. Yeah.
>> Right. Um and the barrier to entry is
now definitely reducing over time. And I
think anti-gravity I think will will
serve a need for people that want to
build right. Um but also at the same
time for the people that are just
experienced developers it will continue
to sort of increase the ceiling. I think
the commitment that we have is we are
not just going to make it so that it is
very easy to build these apps. We want
to make it so that if you're operating
over the hardest code bases, the most
complex code bases that provide the most
business value, we will accelerate sort
of development as much as we can.
>> Yeah, I love that. I have a bunch of
very narrow questions. Uh, one, what got
you into baseball?
>> Um,
>> just like just the world.
>> I'm not gonna lie. I Yeah, I um
>> I think just this idea of a I guess this
is maybe like a childhood dream. You're
like, can someone or like I I I guess I
played I played uh little league for
like a year um in uh in elementary
school. And you know there was like
someone who was very good and you he
both could could pitch and bat and uh
you know you kind of like watch hear
about baseball and it's like people are
very specialized and uh somehow show is
just doing both at a world class level
and you're like this doesn't make sense.
Like it doesn't make sense that this is
even possible. Like probabilistically
this does not make sense and that's like
a childhood dream of having someone so
so overpowered that exists out there.
>> It's like it's like Gemini 3 Pro.
>> Yes. This is Shouldn't exist. Um, where
another fast follow is like where does
the anti-gravity name come from?
>> So, I think as developers we're
remarkably bad at sort of naming things.
So, we went externally uh for for this
um to someone we we think is like uh
like genuinely amazing. And I I think I
think the reason why this fit the reason
why we thought this fit was um like we
want to create an experience where
you're basically not limited at all,
right? and your imagination is kind of
the only constraint. And I think this
idea of anti-gravity um and and the
space theme and the fact that there's
basically no limits was sort of very
attractive to us.
>> Yeah, I love that. I also feel like it
ties to this like Gemini 3 narrative of
like helping you bring any idea to life
and it can the model is really capable
of doing that. So I feel like there's a
lot of dovetail. It also will we've been
trying for a long time to have somehow
get the first people vibe coding in
space. So I think maybe anti-gravity
will be the first sort of
>> even doing that.
>> I I don't know. We got to figure it out.
Supposedly, it's like $500,000 for
launch tickets or something. I don't
know. We'll we'll see if it happens, but
we got to get uh anti-gravity as the
first uh developer coding product on the
moon. Let's figure out how to make it
happen. I don't know.
>> Let's do it.
>> Let's do it.
>> Yeah. Another great property is kind of
the uh like a shortened acronym um is
kind of like AGY. Yeah. Uh which I think
is both short catchy. Um obviously wink
wink to to AGI.
>> Yeah. This is the I think this is the
best uh one of the best.
>> Logan seems to think that itself has
justified the entire naming.
>> The whole the whole naming for Agy makes
sense. Every time I see it, I'm like, I
get it. This is tongue and cheek. This
is great. Uh so kudos to to you and to
the marketing team for coming up with
that. It's a great uh yeah, it's a great
acronym. One of the things that um gets
me most excited about the anti-gravity
experience is it's not just uh like a
single. It's not just Gemini 3 Pro
that's that's coming to the experience.
It's actually like the whole suite of
Google's models including our latest
generative AI models or generative media
models rather. Um you want to sort of
talk through that at the high level and
like how you're thinking about like what
models actually make it into the
experience?
>> Yeah, I think that's one thing that's
particularly exciting to the team um at
least being at GDM and then also at
Google is that we have state-of-the-art
models across many realms, right? Um
image generation, the latest and
greatest nano banana sort of
capabilities are embedded into the
product. um all the future capabilities
on day one we envision will also be in
the product as well. We want to be at
the cutting edge of what Gemini has to
offer. The benefit here is we're we're
kind of at the forefront of what the
model is and we're also able to then
push the models not only on the product
side but the research side in terms of
here's what frontier agentic
capabilities are and here's how we need
to be making the models more capable in
many many different axes. Um, as you
pointed out, UI controls also baked into
the model as well. And super excited
about about the progress we made in a
very short period of time.
>> Yeah, I feel like this ensemble of all
the models coming together is super
powerful. So, maybe we actually let's
let's do the experience. Let's try the
demo and we'll see the models come
together.
>> Awesome.
>> Let's do it.
>> This is an example of sort of a very
simple app. I think it's a I think the
benefit of showing this is to kind of
see what the brand new capabilities
anti-gravity brings to the table. It's
going to be Airbnb for dogs here. It's
not the most complex web. very hard to
show a demo of something working in a
complex codebase in a very short period
of time. Yeah.
>> Um, so for now, what I'm going to do is
I'm just going to ask, uh, I want to
build an app for Airbnb for dogs.
>> Can you just show me a couple designs?
Um, and let me review them.
Right. So now it should just go out and
and as it's going and doing work, it's
going and thinking, right? Our model has
like dynamic thinking. It's able to
think through things. Um, and it's gonna
it's gonna kind of formulate a couple
images for me to for me to take a look
at. Um,
>> and maybe we can talk about uh images
are one of the things that fall into
this artifacts category. Um, so you want
to give a sort of quick TLDDR on like
what artifacts are and how folks should
be thinking about them.
>> Yeah. So, as you can see, what it did
very quickly was it generated a task.
Um, and a task is itself an artifact
that kind of in this case it's trying to
explain to the user uh what it is about
to do, right? And an artifact is
basically the mechanism for the agent to
communicate with the user and some work
product of like maybe the steps it's
going to take or the steps it's already
taken so far. So in this case it has
already generated a couple nice uh sort
of images for me to kind of take a look
at. Uh one is called K9 concierge and
other one is called pup stay. Um K9
concierge sounds very stateful. So maybe
what I should do is um I want to make a
little bit of an edit here. Uh maybe
what I can uh what I can what I can say
is um I want this to be called pup stay
instead. So I'm going to take the best
of one and another one and I'm going to
I'm going to asynchronously provided a
review. As you can see there's now a
grade out bubble here that kind of shows
hey I did provide a review. It was
almost as you can see almost in a Google
doc style format uh that I did um and I
added a comment here. And as the model
now trucks goes through, it's going to
pick up that comment. And we'll we'll
kind of see it do that um as it does
this. So it it updated its its task.md.
It said, ""Hey, it did review the design
with the user."" And it's now picked up
my sort of comment as well, right? Um so
we'll see that we'll see that come
about.
>> Is it is it common? Would you want to
like ceue up like four or five of those?
>> Yeah, I can do that. I can queue up four
or five of these messages to the AI.
It's kind of similar to how, you know,
if we did this and we actually went and
grabbed coffee, I might see something
and I might not like what I saw and I'll
just ceue up a bunch of tasks. So, it
generated an implementation plan for me.
Honestly, I I, you know, because it's a
demo, I'm not going to read it that
carefully, but I feel very strongly
about adding documentation. Similar
similarly, Google style, Google Doc
style, I can like add a comment here and
I can say, please make sure to document
the HTML file. And we we had talked
before about this sort of like notion of
the knowledge panel and is is this maybe
one of the inputs if you're like
consistently saying like I want
documentation for my uh for my app or I
want it to use X Y andZ style format or
you know library etc etc. Those are the
types of things that will like
automatically be picked up and then
distilled into this like knowledge panel
so that I won't have to ideally continue
to feedback.
>> Yeah. So this this is purely an
artifact. Actually the artifacts that
you generate can be used in the future.
they're actually something that can be
so let's say I did build an artifact and
I continue to use the repository it can
pull this up knowledge is something that
the model is given the opportunity when
it goes through some amount of work to
kind of save some state
>> and once again this is it's crazy that
that this is not like some hacked up
thing or heristic this is the model
deciding I did a meaningful chunk of
work that is very specific and I'd like
to remember this going forward and
potentially retrieve it in the future
>> and our product is capable of kind of
retrieving these these pieces of
knowledge. So one of the things that
just happened by the way along the way
is it kind of wanted to install
something and actually a lot of terminal
commands we've run in auto mode but for
installation where it changes something
on your local machine we come back to
the user and ask it to kind of make the
change as you can see for that I just
plopped open the editor here what we
have open is the manager and I was able
to kind of accept the change there so
this is one of those cases where I don't
need to commit to one way or another I
can take a look at the code if I really
want and you can see that a bunch of
code was actually generated and I could
open this and kind of read it. I don't
want to right now. I'm being lazy. Um
but um but yeah, it went and created a
sort of implementation. It's going to
continue sort of operating and we'll
kind of see what it what it comes out
with. Do you think do you see
practically do folks like and this is a
very nuanced question but like do they
like split screen like IDE and then and
then agent man or is it literally like
most of the time you're like just from
folks internally and externally we've
been testing with is like mostly in the
agent or mostly in the ID or like how do
you see the split right now?
>> It's interesting I think for tasks where
you want to do something very very high
level. Um by the way once again you can
see creating documented HTML structure
you can see that it already took my
piece of VBA. Yeah,
>> you might live entirely in the in sort
of the the agent manager entirely,
right? And I'll show you cases where you
can pop open the inbox and kind of see a
bunch of tasks queued up like at a much
higher level and you can operate on it.
But if you're going to make a very
nuance change and it's going to edit a
lot of code, you probably do want to
look at the code uh kind of ultimately
as well. It really depends on what
you're trying to do. I found myself
actually using the agent manager to do a
lot of deep research style queries and
for that actually I can live entirely in
sort of the agent manager um because I
actually trust the summary that the
model is kind of giving me. Yeah, if
>> that makes sense.
>> No, that makes a ton of sense.
>> So actually interestingly what it did
was it it um it created the app. It is
now capturing screenshots of the running
app. So this is actually the beautiful
part of anti-gravity able to actuate the
Chrome browser itself. This is a pretty
beautiful website honestly. Yeah.
>> So, hey, the agent is doing some work
here. So, I'm going to go back because
we want to let the user know that
something like this is happening. And
it's going through kind of a
verification step. And the beautiful
part about this is it even provides a
screenshot of what it did in the
verification step or like a kind of a
kind of walkthrough of what ultimately
ended up happening. So, this is one of
those cases where actually if I walked
away and grabbed a coffee and came back,
I could actually be like, ""Okay, how did
you know that the website looked good
and it's actually that okay, I had a
screenshot. I actually recorded stuff
and clicked some buttons along the way
and here is actually the proof of work
that what I had was actually correct.""
>> I love that in at the at the very end.
>> And we were talking off camera right
before this, but um there's a
anti-gravity chrome. This just like
plugs into your existing Chrome and then
>> plugs into existing Chrome installation
uh necessary. Um it just opens up a
Chrome profile and is able to operate
right there. So you can just take over
from there and continue using it as if
it's your own browser.
>> Nice. Awesome. And is it mo is the
browser actuation mostly right now
testing or will it also like for the for
the research that you were explaining
before like those types of tasks will it
like go to Google search and like start
like browsing around and doing that type
of stuff or is it
>> 100% do that?
>> Oh, interesting. We found it as well and
we actually evaluate the model on all
these capabilities and in the product
itself to be able to actually go and do
deep research and all these other things
and the reason is fundamentally because
you can imagine one of the use cases I
guess like we've sort of tried doing is
hey find me my top priority bugs
>> and then sort by them. So this actually
requires you to go on a brand new
website um maybe use the search
functionality, click a couple buttons to
research to kind of like filter and and
sort through things and ultimately come
back with hey here's the task you need
to do.
>> So this is this is much more than just
open an app an app and just click a
couple buttons. It's actually deeply
understanding like a page
>> and needing to use search and a bunch of
other tools on there.
>> That's crazy.
>> Yeah. And by the way, this uses the the
the latest and greatest like UI control
capabilities of the Gemini models uh to
basically be able to do this
>> state of the art. Yeah. So maybe maybe
just to walk through a little bit about
about this, you know, just through the
course of doing this, you can kind of
see the model has given you uh and the
agent has given you kind of the proof of
work of everything it did. It provided
all of its design concepts that got you
here, right? Um it also gave you like a
nice implementation plan, the set of
tasks that it accomplished and a final
walkthrough um about this, right? And
implemented like you can actually see
that it took all my feedback. It
implemented the K K9 concierge aesthetic
with the pop state branding. So it
actually like provides all this. And if
I want, I can even pop open the files in
the manager itself. This is one of those
cases where you don't need to jump out.
Once again, we're not trying to say the
editor is not useful, but we want to
make it possible that this becomes your
command center in a lot of ways for all
these agents. Um, and you can even see
all of the videos that I can now comment
on and continue to sort of iterate on,
right? Um, let's take a look at the app.
One interesting thing about the app is
it very clearly only tried to make the
pages I cared about which is the main
page. If I actually hover over
experiences and become a host, it is
very clearly pointing to nothing. So
what I'm going to do now is I'll
actually showcase a new capability which
is that um I can actually create many
many sort of agents uh that kind of
operate over operate over the same piece
of code. In fact, I can even do it
across many many pieces. Uh but let me
just start a brand new conversation,
right? Um, so I'm going to start a brand
new conversation about Airbnb. I wanted
to
>> and just to clarify, how do you how do
you know when you go to the inbox that
like it's you're connected to it's
because you like have the workspace
selected and that's how it's like
attached to a specific project.
>> Yeah. So the the inbox and I'll show it
to you just shortly. It actually has
basically all the agent sort of um work
streams that I've had are in the inbox
regardless of what sort of workspace I
was operating on top of. So this is a
brand new workspace called Airbnb.
because it was empty when we started.
Um, so right now I'm just going to kick
off a couple things and let's just do
that first. So I'm going to say uh the
experiences tab is tab is empty
fill it in. Right? And I'm going to do
that and then I'm also in parallel going
to start another conversation. It's very
very quick to do this. I'm going to now
say the become a host host tab is empty.
Fill it in. And actually I want to start
a brand new another conversation here. I
have another project called baseball
stats. I don't really know what it does.
Um, what does this do? Right. Um, I
think it was made uh a while ago. Um, so
now I can I can kind of see a bunch of
these running sort of tabs here and one
of them has become idle and I can pop in
whenever I want and I can search for
these conversations. I can search for
only the pending ones. You can kind of
see three or four of them are kind of uh
pending right now and running.
>> Pop in whenever I want.
>> Yeah. And idle means like the task is
done and it's like ready for you to
>> ready for me to jump in if I really
want. Got it. Right. And now it's made
me a brand new implementation plan for
this one that I can take a look at. I
really don't want to take a look at it
that closely right now. So I'll just
I'll just wait for it. This is like our
coffee moment. So I can pop open the
inbox and I actually see a couple of
them have finished or one of them has
actually finished, right? Um
correspondingly. So uh one of them has
actually gone idle. So it's actually
done a proper kind of deep research for
me here uh of what this does. So it said
based on the analysis, it's a React web
application that breaks down some data
from some sources and here's here's
something and I can dig deeper if I
really want. But this is like one of
those things now that it's kind of
changed my development model. The cost
of asking a question, making changes is
so low. I'm just firing off a lot of
things in parallel, right? And I have I
can now orchestrate many of these
agents. And the beautiful thing about
this is this actually operated on a
brand new workspace. In the past, you
would need to open up an entirely new
editor window. This is why these two
have been decoupled, if that makes
sense. like why we've done this and if I
really want there's always a shadow
editor available even for this workspace
called baseball set. So I can press
command D and I'm right there.
>> Wow.
>> So if I want I can have it but if I
don't want and I want to be decluttered
it's gone. So um so we have that and now
we have interestingly for the file
experiences it really wants me to go and
run something to run a command. So I'm
going to press accept. It's actually
going and modifying my the browser too.
So um and going and doing things. So, I
guess like the agent is going and doing
its own thing here. Um, it opened up a
browser. Because it opened up the
browser, it should hopefully give me
some sort of verification of what it
did. Um, it's going and ripping through
a bunch of tool calls in parallel.
>> How much do you find that like
personally for you as you're as you're
using anti-gravity that you're sort of
going with the flow of model suggesting
versus like, and I guess this depends on
like the model quality overall versus
like course correcting like no actually
go and do these things as well. Do do
you just just spend more time, you know,
again, firing off thoughts and trying to
like try different things versus like
over correcting models in certain
directions?
>> Yeah, I I feel like I find myself
understanding the hills and valleys of
the model. In other words, I feel like
at every given point, I'm always trying
to push the capabilities of the model
more and more. And the benefit of sort
of doing that is is um as I understand
that more, I kind of give it more
autonomy. Mhm.
>> It's like, okay, I specified the task
well enough. I'm just going to kind of
let it go do what it's doing, and I
don't really want to review what it's
doing.
>> I love that.
>> So, it basically has implemented the
become a host page. So, I can see this.
It created this beautiful become a host
page. Um, I have a walk through and the
beauty of the walkthrough is actually
shows me clicking the model, clicking
become a host, and scrolling. So, once
again, I could live here entirely. If
the browser was closed, I could totally
do that.
>> Yeah. And and just from uh this is tied
to the model piece, but also just like
the product experience story is your
suggestion for developers as they're
sort of like putting anti-gravity
through its paces like should they be
you know taking like you know here are
the 25 features I want in in this app
and like just dropping those into like a
single workspace or like a single like
model turn and then just it'll do the
implementation plan, it'll test
everything or is it like break it up
into smaller pieces and kick those off
independently or like what's the best
practice? I have found myself really
enjoying breaking it off into smaller
pieces and and kicking them off
independently.
>> And the reason is just because I think
the implementation plans, the
verification is much more digestible
>> in this case. Now granted, this does
mean that it is like quite important
that what you're looking at is disjoint.
>> So you do need to at this point be a
little smart about it. But ultimately,
if you're doing something that touches
many different pieces, you do need to
think about the coordination of it. you
want 10 different features and they all
kind of do 10 different things. It's
actually important that you understand
what's going on.
>> Yeah. No, that's I feel like it's very
much like
sort of a collaborative partner. Like it
really is like you're there's a human in
the loop. Like you in some cases you can
sort of let the model go off and run
wild. But I feel like the way it sounds
like the way you're saying is the best
the best experience is doing it with the
model and you're sort of continuing to
give the feedback iteratively.
>> That's right. What one of the big
questions is like obviously sort of just
at the macro context like this is and we
were chatting about this in ping over
the last couple of weeks but like this
is v one of the anti-gravity experience
>> and like to say it's v v 0
>> v 0 v 0.1 of the anti-gravity experience
and like it's awesome and folks I think
are going to enjoy it and be super
excited but like what's the um what's
the sort of like path to follow along
with obviously you all are going to be
shipping a ton of stuff and like the
pace of progress on the model side the
product side is going to be really rapid
like how do people stay in the loop. Uh
or do we have to do a podcast every time
you you all go and ship a bunch of
stuff?
>> Yeah, I think what is what how we think
about it is the model capabilities will
continue to increase and as the model
capabilities continue to increase, there
will be brand new product form factors
that take advantage of the model
capabilities. Yeah. And that's kind of
what you've seen in the past, right? As
we said, autocomplete, um it was very
quick. You just had some ghost text,
right? And then chat, you had a chat
panel on the side. The agent now it's
able to kind of like make changes in the
codebase and you're allowed to review
them. Now with the agent manager, you
can now orchestrate tens or hundreds of
agents in parallel um that are each not
only operating on top of your ID but a
browser, right? And as you can see each
of the times the model capabilities
increase. And why is the agent manager
possible? It's because the model can go
for much longer. The model could only go
for one step. What's the point of
running 10 or 20 of them in parallel?
You just run one at a time
>> cuz you need to babysit it. And I think
the way we'd like to think about it is
over time as the model capabilities
increase we are going to have brand new
ways for the human to kind of like
interact with the with the model with
the changes the model makes um and uh
and it will birth new product form
factors like artifacts and I think
that's what our team spends a lot of
time thinking about um and that's what
gets us like super excited
>> yeah anything on the near horizon to
tease otherwise um
>> just imagine just imagine um experiences
will get much faster
um experiences. The model will be able
to do more capable and complex tasks. Um
and expect to be grabbing coffee very
frequently while building software.
>> Pin, thanks for doing this. This was a
ton of fun. Um I feel like folks are
going to love Google Anti-gravity. Uh
Agy is is the coolest acronym ever. Um
and I'm excited for my AGY swag. So you
and the team owe me a bunch of stuff. Um
and yeah, thanks for thanks for spending
the time doing this.
>> This is awesome. Thanks a lot for having
me, Logan.
>> Yeah, of course. And thanks everyone for
watching release notes. We'll we'll see
you in the next episode.
>> [music]"
ku-N-eS1lgM,Gemini 3 for Developers,"Gemini 3 is our most intelligent model yet that helps you bring any idea to life. In this video, youâ€™ll learn how to create an API key and send your first request to Gemini 3 Pro. Youâ€™ll explore use cases like data analysis, image and video understanding, and combining multiple built-in tools like Google Search and Code execution.

Resources: 
Start building with Gemini 3 â†’ https://goo.gle/4r9LUNe 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Nikita Namjoshi
Products Mentioned: Gemini, Google AI, Google AI Studio",2025-11-19T20:03:14Z,"Google,developers,pr_pr: AI DevRel (fka Core ML);,Purpose: Learn;,Video Type:DevByte;,ct: ;,gds:N/A;",28,en,en,https://i.ytimg.com/vi/ku-N-eS1lgM/default.jpg,https://i.ytimg.com/vi/ku-N-eS1lgM/hqdefault.jpg,562,public,28932,791,46,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,ku-N-eS1lgM,"Gemini 3 is here and it is our most
intelligent model to date, performing
significantly better in key areas like
science and math with high performance
in multimodal reasoning, agentic and
coding tasks. This model is now
available for developers and I'm so
excited to show you how you can get
started building and creating. So let's
dive right in. Okay, first things first,
I'm jumping into Google AI Studio which
is our playground environment where you
can test out any new models and
features. And I'm going to type in the
chat box over here.
Analyze the sales data from
my fruit stand. Return the results to me
as a nicely formatted
HTML. And we'll go ahead and turn on
code execution. And we'll upload our
data. And then we can kick off this
request and send it to the model. So,
first thing you'll want to make sure is
that you've selected Gemini 3 Pro
preview over here in the model selector.
Gemini 3 Pro is the first model in the 3
series and we have released it in
preview. So, if you are watching this at
a future date, you might just see Gemini
3 Pro, but at least at launch, you're
going to see Gemini 3 Pro preview listed
here as the option to select. And then
what I've done here is I've uploaded
some data. We can take a look at this
data here. This is a bunch of sales
data, so transactions from my tropical
fruit stand. So there's information on
how much I sold and how much I made. And
I've asked the model to analyze this
data and put it into an HTML file for
me. So let's go ahead and check out this
generated file. We can open this up. And
here is our lovely HTML with um
information about what I sold, how much
money I made, how this changed over
time, um and my best performing items
all in one shot with Gemini. So, I'm
going to jump back over to Google AI
Studio now. And after you get a chance
to test out the model, you're probably
going to want to start writing some
code. So, we can do that with the Gemini
API using one of the SDKs. I'm going to
show you how to use the Python SDK, but
if you are a Go or JavaScript developer,
you can check out the docs for info on
how to use those as well. The first
thing you'll need to do is create an API
key. So, click over onto create API key
and you'll create a new API key here.
And you can give your key a name. So,
I'm just going to call this Nikita test
key 3. And you'll select a project or
create a new project. Um, if you have
never done this before, you will need to
set up billing in order to use Gemini 3
in the API. So, if you have not done
that for your project already, you will
see right over here a little uh button
you can click that says uh set up
billing and then you'll be able to walk
through those steps. Okay, let's jump
into our development environment. I'm
using Collab. So, if you click over here
on the secrets uh tab, you can set your
API key as a collab secret. So, you just
type in Gemini API key and then paste in
the value from AI Studio. I've already
gone ahead and done this with my API
key, but if you have a new one, um you
can go ahead and paste it right in here.
Once you've set your API key as a
secret, you can um extract it using this
user data module and set it as Gemini
API key right here. And then you are
ready to import the Genai SDK and set up
the client. So that's what I've done
right here. If you're not using Collab,
you can just set Gemini API key as a uh
environment variable. Just make sure you
don't hard code it right here in the
client because that's never a best
practice. Collabory comes with the
Google Genai SDK installed, so I didn't
need to install it. But if you are in a
different environment and you don't have
this installed already, you will need to
do a pip install Google Gen AI.
But now we're ready to start testing out
the model. So here I have a prompt and
it says classify the following five
items into either fruit or vegetable and
format your answer um as a simple
commaepparated list of pairs. Let's go
ahead and test this prompt and send our
first request to the model. response
equals client domodels
dot
generate content and then I'll pass in
model which is of course Gemini 3 Pro
preview
and then we'll say contents equals
prompt and we'll pass in our prompt. Um,
so we could go ahead and execute the
cell as is and we would get a response
from the model, but I'm going to add one
extra parameter here. And it looks like
Collab already knows what I'm going to
do. So let's autocomplete that. I'm
going to add a config for the thinking
config right here. And um, in the API,
we've introduced this new parameter,
which is the thinking level, which is
either set to high or low. And I've got
it set to low in this case because um
this is a relatively straightforward
prompt and we don't need a whole lot of
thinking and reasoning done in order to
answer this prompt. But for a more
complicated use case like the one we
tried out in AI Studio, you can set the
thinking level to high and that will
essentially control the amount of
thinking tokens and the depth of
reasoning. So let's go ahead and see
what the result is.
We'll say response.ext text and we can
see that avocado is a fruit and
apparently zucchini is a fruit which I
am just learning now for the very first
time. Okay, let's test out Gemini's
multimodal reasoning and understanding
capabilities. So the first thing I've
done here is I've grabbed some files
that I uploaded to the files API and
then I'm passing them to Gemini 3 Pro
Preview. So let's see what these
different files are. So, the first thing
I have here is this image of me as a kid
singing, and I knew that somewhere in
our home video archive. Um, there was an
actual clip from that day of me um
singing, but I don't know where it is.
And we have a lot of home video footage.
So, we can see here, this is like almost
an hour of footage here starting with my
brother's karate tournament. So I passed
both of the image and the video to
Gemini and asked the model to look at
the image and find when in the video the
scene occurs. And something to note here
is that uh I've also included the per
part media resolution. So first I pass
in the image data and I've set the media
resolution to high and then later I set
the media resolution to low when I'm
passing in the video and that is just to
save on some tokens um because video
data is very long and expensive. So,
let's check out the response from the
model. And it says, ""The image in the
video appears at the 3503 mark."" So,
let's see if Gemini got this right. I'm
going to open up the home video clip.
Um, we can listen for two seconds.
>> Me singing about butterflies. Okay. The
last thing I want to show you is how to
use Gemini 3 with any of our built-in
tools. So, here is an example that uses
Google search. So, let's execute this
cell. Um, I've prompted the model to
generate a color palette for the hex
codes for the aesthetic dark academia,
which is a trending popular style right
now, and then to return the output as
Python code that we can run. So,
essentially, because we've given the
model access to do this Google search,
it'll be able to figure out um what are
the common colors and hex codes um for
this particular aesthetic. So, let's see
what it returns.
Response.ext.
And now we have some Python code. I'm
going to copy that and create a new
cell. And let's run this cell. Let's get
rid of this Python formatting
and we'll execute it. And we can see
this lovely dark academia color palette.
Um I think it's pretty accurate. Uh but
if you are an expert in this aesthetic,
let me know. But I think this captures
the vibe. And then lastly, um you can
also use combinations of our built-in
tools. So here I have passed in the code
execution tool and the Google search
tool and my prompt is to calculate
concaid uh grade level of Edgar Allen
Po's story a telltale heart. This is
essentially a uh computation that looks
at the number of words and syllables in
those words in um sentences and tries to
determine what the reading level is.
This will take a little bit of time to
run because the model has to utilize the
Google search tool to extract the um
text from the short story telltale heart
and then write and execute the code for
this particular um calculation here.
This is a great example of when you
would want the model's thinking set to
high. And we can check out the response.
Let's see. Um and it says here that the
grade level is approximately 5.1. Um, so
if we wanted to also go and check out
the full response, we could see the
actual code that was executed, the
Google search results that happened. Um,
there's a whole bunch of information
that we can check out as well that came
back in the response. That's a wrap on
how to get started building and creating
with Gemini 3 for developers, but if
you're looking for some more
inspiration, be sure to check out AI
Studio and search through our app
gallery, or you can go into the
documentation to learn a little bit more
about some of the features I talked
about. As always, very excited to see
what you build. And if you do create
something cool, be sure to let us know."
mci0f2dy7G0,Gemini 3: Launch day reactions,"Join us for a special episode of Release Notes as we unpack Gemini 3, Googleâ€™s latest AI model with key team members. Learn how Gemini 3 empowers developers with enhanced multimodal understanding, agentic capabilities for complex tasks, and generative interfaces that transform prompts into interactive applications. We discuss real-world use cases, the iterative development process driven by user feedback, and the strategic balance between model performance and broad accessibility across various Google platforms.


Listen to this podcast: 
Apple Podcasts â†’ https://goo.gle/3Bm7QzQ 
Spotify â†’ https://goo.gle/3ZL3ADl 

Chapters:
00:00 - Introducing Gemini 3
03:08 - Gemini 3 everywhere
04:13 - The product-model partnership
08:20 - Balancing speed and quality
11:40 - Gemini 3 'wow' moments
27:47 - Generative interfaces and UI
31:44 - Gemini's agentic capabilities
33:55 - Proactive AI and future
34:55 - Managing compute demand
39:32 - The Gemini 3 family
41:45 - Conclusion


Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Tulsee Doshi, Josh Woodward, Logan Kilpatrick 
Products Mentioned:  Google AI, Gemini",2025-11-19T03:58:36Z,"Google,developers",28,en,en,https://i.ytimg.com/vi/mci0f2dy7G0/default.jpg,https://i.ytimg.com/vi/mci0f2dy7G0/hqdefault.jpg,2537,public,12381,357,38,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,mci0f2dy7G0,"Hey everyone, how's it going?
My name's Logan Kilpatrick, I'm
on the Google DeepMind team.
Welcome back to Release Notes.
Today we're joined by Tulsee
Doshi, who's the product lead
for actually more than just Gemini models.
All, all models now inside of, yeah, just
- Our Gen AI,
- Gen AI models inside of DeepMind
and Josh Woodward who leads
product for AI studio,
Gemini app, and Google Labs.
So the whole suite of DeepMinds
and actually more than
Deepminds product services.
Tulsee, Josh, we're sitting
here because of Gemini 3.0
and we're sort of entering
the Gemini 3 era.
Do you wanna give us the
sort of high level of the,
the exciting headlines
for this model launch?
- Yeah, we're super excited
about this model launch.
So we're releasing Gemini 3 preview.
We're releasing it in a
wide range of surfaces
and Josh can talk more
about what that looks like.
But I think the reason we're doing that is
because we're seeing this model
really be this step change
in terms of what you can do
with it and what you can build.
And so couple things that
we're really excited about.
I mean, one, the model is just state
of the art when you think
about reasoning and its depth
and its nuance, its ability
to kind of take simple prompts
and actually turn that into
extremely rich outputs.
So that's I think, one starting point.
It's multimodal understanding
is also incredible.
So I think one of the things
I'm really excited about is its
video understanding, its image
understanding, its ability
to actually go deeper between
the context of your text
and the images and then combine that
with its abilities in coding, for example,
to actually create just
really cool outcomes,
I think is awesome.
Right. That ties then to like,
it's an amazing vibe coding model.
It's also our best model yet
for agentic use cases and tools
and agentic coding.
So there's like a whole step change we're
bringing in that regard.
And then I think lastly, one
of the things we always look at
with Gemini is we're trying
to build well-rounded models.
So we wanna build models that
are actually really smart,
which this model is,
but also models that we think
are gonna be really good
for users in our products.
And so this model crosses
the 1500 ELO on Ella
Marina, which I think is,
it's 1501, which is awesome.
And that one matters. That
one matters, it matters.
And, but I think like we use
that as kind of a measure of
of well-roundedness of
like the, the usability,
the accessibility of
the model from like
a stylistic standpoint.
And I think that's something
we're also seeing in some
of our testing in the products as well.
And so I think the combination of all
of those things means this
model is gonna be really awesome
for this, this tagline we're
using, bringing anything
to life because you actually
can now take these different
modalities and formats
and use them to build really cool things.
- Yeah, I mean I think I
just add, there's one thing
for a model to be awesome
on the leaderboards,
which this one is, but when
you get your hands on it,
it's gonna be, you're just gonna love it.
You can feel it. Yeah, it's really good.
We're putting it in the
most services we've ever
done on day one.
So we're gonna basically right
crack of dawn in the morning,
get up, put it in the Gemini app,
it's gonna go into AI mode.
That's for all the sort
of consumer use cases.
Developers will be able to
get it at the same time AI
studio, it's going into Vertex.
There's a new sort
of coding product we're putting out called
Google Antigravity.
It'll be there too. So it's
just gonna be everywhere
and we can't wait for people to try it
out, give us feedback. Yeah,
- And I think actually like on the,
where it's gonna be everywhere,
one of the cool things
with Gemini 3 is this
time we've actually partnered
really closely with a lot
of customers externally.
- Yep.
- So I think this is one of those times
where we actually have a
lot of partners outside
of Google also who are gonna
be shipping the model tomorrow
and who have been working with
us to give us feedback and,
and help us make the model better.
And so that's also exciting.
It's not just that our products
are gonna be scaling this
model, it's hopefully broadly
we're gonna be scaling
this model, which is really cool.
- Yeah. I love the meet
the world, meet developers in,
in my world, but like meet
the world where they are
and like all these products
and like people don't just use
Google products so putting
the models out there
and getting in their
hands is super important.
There's an interesting thread
actually on this product story
and we were just talking to Koray
before this about sort
of like the partnership
between DeepMind and like
the PAs and all this stuff.
And I'm curious, Tulsee and,
and actually Josh as well
from a like a DeepMind
collaboration, like how much
as we sim ship across the
services, like you know,
the perspective of those
products matters more
and sort of influences
the models And then also
like is that like a net?
Is it like just harder?
Is it more constrained?
So like does that actually
help us in like, hey,
we need this model to be
really great for search
and that's pushing us in
this dimension which then,
you know, skews the model
capability in some positive way
that we don't or is it really
just like more juggling
of all the constraints?
- I think it's both things.
I mean I will say, Josh,
tell me I'm wrong. No, no.
I think this has been the
strongest partnership we've had
definitely in terms of
getting a model into
the hands of the product.
- Yeah, yeah.
- In terms of the feedback loops
that we've had with the product teams.
- Yeah.
- Because I think now, you know,
we have actually a user base
who is using our models regularly
that we can get feedback from,
that we can run live experiments
that we can actually,
you know, solicit feedback from.
And that is actually really huge, right?
Because then we can bring that
feedback back into the model.
And I think you're right, like
there are constraints, right?
So the, the kinds of things
that developers are looking
for may not be the same
things that are important
for the app, which are also
not necessarily the same things
that are important for AI mode.
And so it is a bit of a, a
tension game we're playing
between all of these moving pieces.
And I think that's hard from a model side,
but actually like as a PM
and as someone who's like trying
to think about this
holistically, it's awesome
because we're now getting
the feedback to be able
to actually have real
conversations about those
trade-offs, right?
To be able to say, okay, the
model is now gonna do this
really well, but at this
cost, are we okay with that?
And, and what are the trade-offs?
And I think that's been really good.
And I think this time
around we've had a bunch
of hard conversations actually,
- I don't know if we
can share any of them,
but like is there a specific,
like I was in a code,
oh I've got one, I'm like
- This.
Okay, so some of the
viewers may have seen,
we've been testing this model
kind of in the wild silently
and sometimes not so silent,
sometimes not so silently.
Yeah, I go to Canvas and
it's just there one morning
that was a mistake, but you,
you kind of in an early rev
of the model, what we were
really interested in kind
of trying to optimize some
aspects of the persona.
And I remember there was one
rev that we really liked on
that one dimension,
but it regressed on a whole
bunch of other dimensions.
And so that'd be an example
where we've got really positive feedback.
But like Tulsee said, we're
trying to make this a
well-rounded model.
And so if it's great personality
but it's like forgets how to do tool calls
for example, can't shut that.
Yeah. Yeah. So these are kind
of the kind of revs to revs
that we look at together.
And I would say, I think one
of the things we're
really excited for people
to try it out is because I
think we're now getting the
product model feedback
loops working really well.
Yeah. And so when people
are giving feedback in any
of these products, it's now
just like seamlessly starting
to like flow back in.
And that's, that's really exciting too.
'cause you can see the quality
gains happen when we do these
kind of blind tests side by side. Yeah.
- And it's kind of awesome
because even something like we
say tool use as like a broad phrase,
but even the way that tools are
being used in the Gemini app
is a very different construct than like
how a developer might use tools in a,
in a very different context or product
or like how Antigravity
expects tool use to work.
And so I think now
that we actually have these
different product surfaces
that we're getting feedback
from, we're actually able
to see those differences
in those trade-offs.
And I think that also allows us
to actually have a much more
holistic understanding of
what do we mean when we say tool use
or what do we mean when
we say code performance
or what do we mean when we say, you know,
creative writing abilities
or something like that.
- Yeah. I think the
product scaffolding sort
of grounds a bunch
of those capability conversations in a way
that would is like really you
have to like proxy with some
of the academic benchmarks Yeah.
That are oftentimes like
actually not representative of
how users actually use the
models in, in certain cases.
Which is, which is tough.
But, and and you both alluded
to this, I'm curious when,
and this was sort of, you
know, I get all the tweets
of why are we not shipping
Gemini 3 earlier,
which I'm gonna start just
sending them to you two.
Yeah, please.
And sort of,
and like I think it gets at
a broader conversation
of like when obviously Gemini
3 out the door now at
the time of this
conversation being released.
But like when was the moment
where we knew like, hey,
this is the model we
want to get out the door
and obviously we've been
doing tons of this iteration,
but yeah, is there a
way we can ship faster?
Is there, is there, why not ship earlier?
Like in the past we've done
all these like crazy experimental
checkpoints, which we've gotten tons
of feedback both positive and
negative in a lot of cases.
So I'm curious what the, the balance,
- The calculus was there.
Yeah, yeah. I mean we do wanna ship fast
and I think that's actually
super important to keep alive.
Like I think this whole idea
of relentless shipping,
put things in the hands of
users as soon as possible,
get feedback as soon as possible.
I think that we want that to
continue to be the theme. Yeah.
And I think we want that
to be the theme across all
of our models, not even just
like Gemini Pro for example.
I think in terms of this
model in particular, one
of the things we really wanted
to focus on was it's kind of
real world usability.
So real world usability
both in our products
but also with developers.
And that means actually
getting a lot of feedback
and actually iterating on that feedback.
And so we spent, you know, an a period
of time putting the product,
putting the model in the product.
To Josh's point about the
persona discussion we had,
we put the model in the hands of customers
and then we basically used
that to also help us calibrate,
okay, where is the model
meeting our expectations?
Where like it matches what
we are seeing internally?
Where is the model not
necessarily meeting those
expectations and how
might we wanna adapt it?
And so we had kind of quality goals
and bars for ourselves that we wanted
to meet on our benchmarks,
but also on like how it
would work in the products,
what kinds of, you know,
experiences we wanted
to create with the model.
And so it was all kind
of working towards that.
I still think the goal
is like ship it as soon
as possible within the constraints of
that realm of things.
But it's about like making sure
that it hits those goals.
And I think now from the
previous models we've set,
we're actually setting more
complex goals for ourselves.
Yeah. So every time we release one
of these models we're
we're setting the bar
higher for ourselves.
That also means that meeting the
that bar is harder I think. Yeah,
- Yeah.
Yeah. I mean the other thing I would add,
I feel like the speed is, is great.
'cause when a model comes,
we measure it in like days
and hours until it's actually ready.
So I guess we could get to minutes.
Maybe that's the goal,
but I mean I think some
of the turnaround on this is just amazing
and the teams working across
either the developer side
or the consumer app
side, the modeling side,
obviously it really is,
you're seeing I think out
in the world the momentum
of like the compounding.
And so I think it'll get faster
but at this point we're
kind of like days, hours
and maybe we go to hours minutes.
That's the next, the next jump
- For, for both of you,
what was that moment?
And Josh, maybe you start with this one
that like you tried one of
the Gemini 3 checkpoints
and sort of, you know, the the use case
or the demo that clicked.
Was there any of them that
like you sort of saw this
and you're like, this
is clearly a step change
and I'm not, I'm happy
to give mine as well.
- Yeah, yeah. We should go round.
I mean I had a few of these. Yeah.
I think this model's really
good at vibe coding.
It's very good at web devs
and you can just describe
something that's in your head,
hit a button and it's there.
And that is kind of a crazy
sort of compression of tie
and skill and expertise.
So that was one I'm really
excited about the multimodal
capabilities too.
We're doing some stuff, we're kind
of both on the Gemini side
and the Google Lab side.
We're really caught up in this idea of
how you can transform content
and this model's very good at that.
Better than I've ever
seen. So those would be two
off the top of my head.
- What are some examples of
the transforming content piece?
Like is this
- Yeah.
I mean say you're a student,
you've got a whole bunch
of like video lectures,
handwritten notes, any of
that you just loaded in, hit
a button, it goes anything
around like I'm very
excited about also kind
of visual formats.
We're starting to play
around a lot with that.
There's a experimental feature
we're putting in the Gemini
app where it'll literally,
you can sort of describe something.
It'll make an interactive website
for you on the fly. That's crazy. And
- So, and that's not Canvas though, that's
- It's built, yeah, it's
built its own little thing.
Got it. And I think to me
that's a whole new
frontier with these models.
We're calling it generative
interfaces on the team,
but this idea that just
pixels are streamed in
as they're ready based on your prompt.
And I think that's a whole leap.
We've never been able to do that before.
So that's now possible. Still
a little slow right now.
That's why we have it as a labs feature.
But I hope people play
around with it a lot
because we're doing experiments
where it's like give like the
history of Van Gogh's life
by period with example artwork
and you'll just build a whole timeline
and just let you go through it.
and it's all just generated, pretty
amazing. That is, that is crazy. Tulsee?
- Three examples from my side.
One is I do think the vibe
coding piece really does hit
and I think it does because
the visuals are so rich
and the interactivity is so rich.
And so I was like, I
was using it literally
to create like a fun game
to play with my niece
with like a single prompt.
What you can get the like her ability
to actually engage with it.
I think that kind of fidelity of,
of the, of the experience we
haven't been able to see before
and that I think is really hits home.
- Were you using AI Studio?
- I'm using AI Studio of course.
Fear thoughts. The second one though
that I think is really
interesting is actually one
of the things we don't talk as much about
with this model is it's
multilinguality is also really good.
So its ability to write in like Hindi
or actually in Gujarati, which is like not
as common a language
but is like from the western
part of India, which is
what my family speaks that
is also really strong.
And I think that was
another wow moment for me,
which is like, oh this
model is really good.
- Yeah.
- And it's actually able to do something
that I personally struggle
with, you know, in terms
of daily life that I
would actually really love
to see kind of play out.
And then the third is,
my husband is a very
avid pickleball player,
but the like video
understanding of the model
and its ability to kind
of break down a video
and like break down a set of moves
and actually like give
you critical feedback.
I think like all three
of these examples are basically
combinations of the model
being able to understand
something that maybe you
by yourself either struggle
to do or it's harder to do
or just requires a lot
more focus and attention
and the model is able to pull
out some of those nuances
and actually like help you
with them in a much
like richer kind of way.
Yeah. And I, yeah, I think
it's just been awesome.
Also, I will say like the vibes
of the model, even in terms
of like the, the style in
which it responds,
it feels different than other models.
It feels smart.
- Yeah.
- And I think that also stands out when
you, when you engage with it.
- I love it.
I think the
one other thing I would add
to you said it is like
to me we're getting in
a point in Gemini 3
where it's like the combination of a lot
of these things is really starting to show
and differentiate.
So like when you go back Gemini
1, long context Gemini 2
and more modalities, some
of the coding started
to come in 2.5, even more coding.
Now we're kind of in like
agentic tools, it's all starting
to kind of almost like ladder
and sort of build on itself.
And I think that's what
we're starting to feel,
at least I'm feeling on the team.
Yeah. Is like there aren't
these little isolated model
features anymore.
It's actually like the
combination that starts
to get really interesting
and I think we saw this even
with Nano Banana a few months
ago where you kind of take something
as like put it together.
So to me this is like
Gemini 3 gives us a huge
foundation of sort of things
we can combine in new ways
and that's where you get like
multimodal and vibe coding.
Suddenly you've got some really
interesting interactive thing.
- Yeah.
- That just before
wasn't it really possible
or that easy to do
- Or there's this cool demo
someone on the team built
and I think we're actually
using it in the materials
where they took a picture of
a handwritten recipe in Korean
and then turned it into like
a full vibe coded like family
recipe app in English
with the measurements
where you could adapt them
for like different, you could switch.
You could switch. Yeah. And like again
that's a great example
of like the combination
of like it has strong
multilingual capabilities,
it has strong multimodal understanding.
So you can literally take a picture
of a handwritten recipe in Korean,
you can then get a fully fledged
like interactive app on the
other side that you can actually
use in your day-to-day life.
Like that combination of
things is really cool.
- Yeah. I feel like this,
this gives so much credence
to the bring anything to
life narrative for Gemini.
I'm like maybe that's a
Gemini long-term slogan.
Like do we have a Gemini model slogan?
Like I know there's a
Gemini app like the three Ps
and yeah all the research
to reality stuff.
- Talking to the first
person
- do we have one
because maybe we should adopt
right now, 'bring anything
to life' as the new Gemini,
- The new, the new we are.
I mean for Gemini 3 I think that is
for all practical purposes the slogan.
- And I think what's cool is
you talk about that example
with the recipes and handwritten
and Korean and everything.
In some ways it's mind blowing
but in other ways it's like
yeah this is how it should work.
- Yeah,
- Yeah. Think about kind of the boundaries
and sort of human computer interaction.
I think Gemini 3 like
blurs those even more in a
way we've never done before.
Yeah. Which is cool. Yeah.
- What was yours?
- Oh yeah, that's a good question.
My, well mine was Josh.
We were in Arizona
and I spent like all day
making demos with 2.5 Pro
and I was like, gosh, 2.5
pro is such a good model.
And then I was like, wait,
I should probably try these
with the new checkpoints.
And then it was just like,
I was like okay, this thing's incredible.
Like especially like
it was like very front
and center in my mind.
Like this was the limitation of 2.5.
There was these cases where I, which was,
and then I could just
like see automatically
and then I, I had a
interaction where I was DMing
with someone and they were like, hey, try
to make this random game.
And there was like a RPG
sort of like fishing game
and it just like one shot at it.
And then I went and you
know, Demis worked on some
of the original sim rollercoaster
type, no not rollercoaster
but like Sim City 1984
or something like that, sort
of tried to remake a game like
that and it just worked using
one of the earlier checkpoints
and then again with, with some of the,
the most recent checkpoint.
And it's just, it's, it blows my mind.
I think the video game
example feels so pertinent
because there's so many people
who like learned if it
feels top of mind for me
'cause there's so many people
who tried to learn computer
science with the end goal
of like building games.
'cause it's just fun interactive thing
and like building games actually sucks
and it's not fun in a lot of cases.
And I feel like Gemini
3 has this power
to like help people bring the
thing that they want to life.
- You know what's actually
awesome is we use Gemini 3.
So YouTube has this
notion of playable. Yeah,
- Yeah.
- Right. Where you can go
to YouTube and play a game
and we actually used Gemini
3, well by we, I mean D Shin
who is awesome and does
some incredible things.
He actually used Gemini 3
to build a bunch of YouTube playable.
That we actually are,
are going live on Tuesday
with the model and they're awesome
and they're like so fun to play.
And I think like just the idea
that you can create this
like vast set of creators
who can actually build these types
of interactive experiences
is really awesome.
But yeah, I think it was after
your Arizona trip where you,
you pinged me and were
like, we gotta ship this,
- We gotta ship this.
- It's awesome.
- But it felt, it felt like it.
I think if you spend time
with the models all the time,
like it's just so clear that 3.0 is,
or 3 is is just a step change.
So it's, it's exciting.
Did the SVG art people
influence our decision on
sub-checkpoints to use?
SVG fan animation.
- Well actually it's really
funny because we did,
after seeing some of that
feedback come through,
we legitimately like maybe a
week or two ago, like sat down
and had a bunch of people
try across our different
checkpoints, SVG art.
Just because we were like, is it really
that different checkpoint to checkpoint?
Yeah. Like are we really
seeing one checkpoint be like
so much better at this than
another based on kind of the,
the Twitter feedback?
- Yeah. Is there a good
benchmark for SVG art? No,
- Not really.
No.
but I think the other thing
is like it is a very narrow
sliver of how the model operates.
Right. And I think what's
interesting is also one,
whether something is cool
from an SVG art perspective is
like very much up to some
interpretation, right?
Two, I think what we've found is
that some are really good
at certain types of things
and then others are like other
checkpoints are really good
at other things, but
it's not like universal.
Like all SVG art one is really good
and then the other one
was really not good.
Yeah. Kind of thing. So it was
very like mixed feedback in
terms of what we were also
getting from, from people.
- I had to go do a bunch of research
to learn more about SVG art
because people were
showing some of the demos
and I was like, I had no
idea that SVGs could move.
I was like, they're like
showing like fully animated
sequences and I was like,
I thought SVGs were just static images
but they're like the programmatic
representation of them.
Which is also the, the thing
that is most surprising
to me is like I get that it's like a,
I assume it's like the
underlying code capability that
as it improves, it sort of
pushes sort of SVG ability.
But then you look at what
is actually being outputted
and it's like the SVG
representation is actually just a
bunch of like random numbers
and letters, which I'm
like is crazy that that
that the model is actually
able to like pick up and learn.
Like I get, I can kind
of intuitively understand
how the model learns to code
because it's English basically
and a representation of this
structured thinking. But
- I mean this is kind of
too in some ways, right?
It's just more, I, I think like in
it also reflects the model's
ability to like reason about
how something will
manifest in two dimensions.
- Yeah.
- Right. Because it's able
to actually take the concept, right?
So like, oh I wanna generate a span
and I want it to look
realistic and things like that.
And then actually translate that into
those numbers and letters, right?
That actually will reflect the colors
and the points in like
an X and Y axis.
- Buttons to turn it up and down.
Right. They they get fully
interactive. It's cool.
It is cool.
- Are there any other
examples of like things
that we saw from early our internal tests,
our external testing
that like sort of we all
of a sudden we're like, hey we
actually maybe do wanna track
that because that would be an
interesting use case that I,
I think the SVG art was one of those
and I'm trying like the Voxel stuff has
been around for a while.
- The Voxel stuff. Yeah. That's
- Cool.
- I'm trying to think. We've
been tracking a lot on the app
side lot
and there's not an official
benchmark for this either,
but like emotion emoji usage,
how concise our model is.
Yeah. So sometimes we've been
a long-winded Gemini model.
You'll notice with 3
that we're like much tighter.
- Yeah.
- Tends to be, it
picks up on the subtleties sort
of the way it writes.
So there's a whole set of
kind of persona style kind
of house style things that
we've definitely been monitoring
with the various checkpoints that
that viewers have probably
been seeing out there.
So that's definitely something
we've been looking at. Yeah.
- Yeah. It's also been
interesting because persona
and like style is so subjective
and so that's one where I
think feedback actually really
does make a huge difference
because my perspective on what good
persona is is probably very
different than both of yours
in terms of like what I
wanna interact with, right?
And so I think that's one
that like really stands out.
But that one we were also tracking even
before like social feedback, right?
It's, it's one that we
were kind of really trying
to monitor ourselves
and kind of keep a pulse on
games is something we knew was
exciting to your point.
Like we knew that like games is a rich
thing that you can build.
But I think when you put these models out
and you start to see
what people do, you start
to really see the magic of
what people can create,
in the range of what people can create.
And then that I think
inspires a set of like demos
but it also inspires a
set of like just people
to go start exploring
and building something.
And I think that also is like a really
nice feedback loop too.
- Yeah. I have a meta question
which is I think the last
time all three of us were
sitting down for this type of,
we need to sit down more
and do these more often.
But was IO a few like, you know
- Yeah, yeah.
- six months ago, which is crazy.
And to see like it is very,
my like meta step back take
is like, it's just crazy
how the model capabilities
continue to improve
and it just like dwarfs
like, it just makes like two,
2.5 when 2.5 launched like,
you know, it had its challenges
and hopefully we addressed
a bunch of the things
that people had feedback on
and sort of built on on the things
that people really liked about 2.5
but 3.0 just makes it look so it's like,
it makes it look like a goofy toy
example and it's just kind of weird.
And I'm curious like how
both of you think about that
as you take a step back
but also like, you know,
when we make the next,
like it is just crazy
to me to project forward.
Like we'll make another step change
that will make this model
now that we're sitting here
so impressed by
and hopefully everyone else
is impressed by feel like kind
of a goofy toy in, in you know,
six months or something like that.
- Yeah, no pressure.
- No pressure.
It'd be awesome. No pressure.
- I mean I do, I think about this a lot.
You know, people talk about AI time
and how like six months
of normal time translated
to AI time thing.
But it is crazy when we were out like
outside shoreline amphitheater
chatting like this.
- Yeah.
- And we were, it was a very good model.
It still is a very good model.
It's like, you know, incredibly
2.5 pro was like incredible.
And here we are, it's like every week
we're shipping, you know?
So in some ways
that's why it probably does
feel like forever ago. Yeah.
- I think it's also interesting
'cause like we were talking
about this too, is it puts actually more,
pressure is not the right word 'cause
that like sounds negative I think,
but it, it puts more
intentionality on like the model,
product story together.
Yeah. Because like as
you have these models
that can do cooler and
cooler and cooler things.
Yeah. You can talk about
that through the benchmarks
and you know, we obviously
want the developer community
to explore things and,
and they will come up with
amazing use cases as always.
But it also puts I think a
lot of pressure on, on us
to come up with like what, how
can we really take advantage
of the strengths of this model
to really build amazing things for users.
And it's interesting
coming from like, you know,
you typically talk about
you wanna start with
where the user is and the user's needs
and then work backwards
to build a product.
You obviously still wanna do that.
You still wanna focus on where
users can get a lot of value,
but you also wanna also start
from what the model can do
and what kind of unique
capabilities it has
and then what can that empower for users.
Yeah. Both ways. Both ways. Yeah. Yeah.
And I think like that was
actually an easier conversation
to have when the models
were a little less capable
because the range of what
they could do was smaller
and so they kind of fit
into your like, okay, yeah,
these are like clearly
things we wanna build.
- Yeah.
- And now I think it's also pushing
for more creativity of like, okay, now
that the model has this range,
what can you, what can you build?
- Yeah, Josh, that's a great, we
talk about vibe coding stuff.
Yeah. But like there's a
bunch of new, it's not just,
you know, Gemini 3
sticking it into the existing
- Yeah.
- Gemini app. Like there's
a bunch of new products up.
So maybe we actually talk about that
as this, as this through line.
- Yeah. I mean there's
a few things coming.
I'm, I'm excited about one
across Gemini and search AI mode.
There's gonna be a whole wave of kind
of generative interfaces, generative UI.
- And just to clarify, how
is this different for folks
who have like seen vibe coding stuff?
Is that similar? So like
the model is generating code
and then there's like,
so like function calling
behind the scenes or can you
give like a little bit more
detail of like the user
who hasn't seen this
before, how does it work?
What, what should they expect?
- Yeah, so the, the shift
we're starting to see,
and this is really the
frontier, the beginning
of it is in the past some
engineer here in the building
would've coded the UI.
It would've been one way for
all users until it got changed.
And what we're seeing now
with Gemini 3 is it
can actually make a lot
of those design and
implementation choices on its own.
It's a big shift. So
you may do a query like,
hey I'm planning an upcoming trip,
give a three day itinerary.
You know, up until Gemini
3 we would've responded
with a wall of text with some pictures.
Now the model is gonna respond
obviously more concise like
we talked about, but it's
also gonna lay out the page.
And so at a practical
level sometimes we do that
where it's actually
saying, here's a tool call,
give me this tool that's gonna lay out a,
I don't know, carousel.
I can scroll through. Other times
what it's gonna do is actually
almost act like a design
agent and be like, I'm gonna
go through a design process
and think about what's the best way
to present a three day trip to Rome.
And so it may lay out a
table, it may give you kind
of something almost looks
like a magazine style layout.
- Yeah.
- And all of this we're kind
of experimenting right now.
Like how much can we
sort of trust the model
to lay this stuff out.
So it's super cool because
you're giving the model widgets a
style sheet, different tools it can use
and you're kind of saying go wild model.
So, and so I think for a lot of people
what they're gonna feel is,
oh this is a much more visual
kind of immersive, interactive
but also kind of customized
to me sort of view.
And I think that's what's
exciting is like it's
just the beginning of that.
And so I think if we look
throughout the Gemini 3 series
as well as 3.5
and beyond, this idea of a
model's being able to kind
of compose more and more things
feels like an interesting
theme to kind of explore.
- Yeah. It's also like, you
know, Josh mentioned widgets.
Like I have this like
particularly fun example from
for myself at least on AI
mode, which is, you know,
if you ask in AI mode
with Gemini 3 about
bubble sort, right?
Which is like just a very
sort of standard notion of
how you can sort, you know,
elements in a, in a series,
it actually creates this
like interactive widget
that like you can play
with and that's awesome.
See like how things actually get ordered
and how bubble sort works.
- The real question is why
were you Googling how to
- Yeah.
How to do bubble sort. No question.
- You're preparing for
like a coding interview
- To somewhere with like shows up on this
No, but but I think the reason it's fun is
because I like have this like
it's, you know, credit to,
to Madhvi who is, you
know, one of our coworkers
who was showing me this example.
- But it's like, it's
kind of a blast from the past.
'cause if you remember like
when you were learning about
these things, you're learning
about them from like static
like textbooks.
- Yeah.
- Right. And you're trying
to like take the written
notation and like map it
and be like, okay, like try to
visualize, try to visualize it.
Like let me try to write it out like okay
now it makes sense.
Versus being able to actually see it.
- Yeah.
- And play with it
and reflect on that in the
context of how you search.
Like it really feels like we're bringing
the Google mission to life.
You know, the like make information
what is it universally
accessible and useful.
- Yeah.
- Like it's that kind
- Gemini 3 brings
the Google mission to life.
Not only, not only, not only,
but even the Google
mission is coming to life.
That's awesome. Josh.
Other things Gen UI?
In in AI overview or in AI mode
and the Gemini app, other things in Gemini
- We're also trying to explore
some new features around
how Gemini can act as an agent for you.
So you've seen probably
over the last month
or so we've put out a new API
for computer use, which is really cool.
Yeah. This Gemini 3
model takes it kind
of one level even beyond
how we think about multi-step actions.
Being able to kind of take
sort of different tool calls
and just do stuff for you.
So the Gemini app's
gonna have an agent kind
of experimental feature.
It's interesting, you can go in
and say things like create to-do lists out
of everything in my inbox if you're
behind like me on my inbox
and it'll sort of give
you this bird's eye view
and be like, let you add stuff
directly to Google Calendar.
- What? So yeah. What's
been your personal feeling?
Like I, I have a very high
bar for this inbox use case
and what's your sense is
like it's, it's working
and it's hitting it outta the park
or like what's the, where,
where should my expectation be
for how good it is at doing some of these
- Yeah.
Box tasks. I think you,
you should give it a try.
See what you think. I think
that one is actually
approaching, like it's starting
to get pretty useful for tasks.
I would say the other one
we're seeing a lot is kind
of helping people do research across a lot
of disparate things.
So you could do this kind
of through a deep research
and get a report or you can try this agent
and it'll actually start
trying to do things for you.
And if it's not
sure, it'll actually pause
and say, Hey Logan, before
I send this message or
before I hit click this,
click this buy button.
Do you want me to do this?
So I think that'll roll out
to all of our ultra plan
members of on launch day,
which we're excited and we'll kind
of take it from there as we go.
It builds on a lot of the
stuff we've worked on together.
Going all the way back to Project
Mariner almost a year ago.
That was some of our
first forays into this
and there's still a ways to
go but we're really excited.
This is one where we're really
excited to see the feedback.
- Is that how the inbox
one is working as well?
It's like actually like you,
it's like actuating a browser
and it's do or is that like
using a bunch of like tool use
to send requests to like the
Gmail API or whatever? It's
- More that.
Yeah. So in this case
if you go into Gemini
and connect Gmail, which
a lot of people do,
- Yeah.
- It'll start doing kind of tool use
and API calls in the background
and then Gemini 3
kind of oversees that
and kind of orchestrates it for you.
So there'll be a lot there.
I think that's like exciting
to keep exploring as we kind
of  push forward ahead too.
- Yeah. I love that.
Any proactivity stuff?
I know just we, we talked about
personal and powerful Yeah.
Implicitly yeah. So far.
But is there anything like on
the horizon from a proactivity
standpoint, which is my,
this is my number one feature of us.
If I can get the Gemini app to
just look at my email for me
and and look at
and see all this tasks I need to be doing,
that would be my ideal case
because I don't want, it's just
too stressful to look at all
- Overwhelming though.
- I'm like I already have chats and text
- Shared, shared pain here
I guess let's say watch this space.
- Okay.
- Yeah. We're very
interested in this right now.
We're starting to see a
lot of people use kind
of scheduled actions in
Gemini as kind of a
like a proactive way to engage.
But we think there's a lot to do here
and I think some of the things
we've talked about combining
different modalities,
letting people hook up
different Google apps, kind
of bring some of that together
to orchestrate, stay tuned.
- I'm excited. I'll, I
will be staying tuned.
I love when people say watch this
space. 'cause I'm like, where,
- Where do I watch this space?
Gemini.google.com
Gemini.google.com. Perfect.
It feels like one of the big constraints
of like actually bringing the model
and letting the world experience
how it can bring anything
to life is like we need
crazy amounts of compute
because the demand is so,
it's not even like we
don't have lots of compute,
it's just the demand curve is
Is like this, you know,
can't build physical compute
that fast in six months.
Yeah. So I'm curious
like for the Gemini app
and maybe more broadly
across other products in,
in your scope, like what,
how are you thinking about
the balance from a like also
from a user perspective of like
where we're allocating compute
because we think there's
value for different use cases.
- Yeah, it's super hard.
I would say like we could share some
of our stories like the
Friday before launch.
I mean I don't know
what's harder solving AGI
or solving the puzzle of
compute trying to like,
but I think like what makes it hard
and you hit on this Logan is
like there's just demand everywhere.
And so that's one thing.
Well I think we're, we are making products
now that people want.
- Yeah.
- And they are coming
back to them and using them.
I mean even just in the Gemini app,
our daily requests has
tripled in the last quarter.
- That's correct.
- And so,
and I know we've seen
similar stuff on AI studio
and developers are just like,
so when you're straining
that many tokens you really
have to try to think through it.
I mean one of the things we spent a lot
of time up here on Friday was like trying
to think about okay where
are the places where a lot
of people can experience kind
of the great stuff in Gemini 3
So there's gonna be some consumer apps,
there's gonna be some developer stuff,
there's gonna be some customers.
And so we try to think about that.
Then we try to get very creative,
like there's a great kind
of tool that came outta Google
Labs called Flow helps you
make sort of videos.
And that was one we were like,
oh there's just not enough.
And we're like No but there has to be,
'cause this product's like
people love this product
and so we were able to find
some crazy deal over the weekend
where they're gonna
convert some of their chips
to a different kind of TPU to
get if another TPU free do.
So you know, you go to
all these ways to try
to like make it work and
we'll see it's launch day.
So we'll see how well
our estimates played out.
But we have a crazy Google
sheet where there's all
- These estimates or like
moving things around.
- Yeah,
- I think it's also interesting
'cause one of the conversations
we tried to have as a part
of this kind of planning
exercise is like, okay,
what is a P0 experience?
We really wanna ship with the model.
What is P1, what is P2?
And what's hard is it really
does feel like you're comparing
apples and oranges
because part of the goal is
to create like a wide range
of experiences, right?
Like NotebookLM is a very different
and very compelling product
than the Gemini app,
which is also a very
different product than vibe
coding on AI studio.
And so, you know, when
you're now sitting down
and looking at these
products and saying, okay,
where should we be bringing
Gemini three to life?
You are talking about
very different versions
of the products and you actually
like Gemini 3 will
manifest in very different
ways in these products.
And so actually if you really
wanna showcase the breadth of
what Gemini 3 can do,
- Yeah
- You actually, you wanna ship
across that breath, right?
And so then you're like, okay well
how do I think about
how do I think about how the math,
the math works out
across all these things.
- And the real problem is we
keep making great models across
all of these dimensions
and it's like I, I do think
there's like something really
interesting where it's, you know,
we had like Veo was crushing it
and then it was like where, you know,
we couldn't get enough compute for Veo
and you know, we made it happen
and then Nano Banana happened
and it was the same thing as
like we couldn't get enough
compute, there's so much
and then Gemini 3 is
happening, it's gonna be the same story.
And the interesting thing is
like,
- and there's only more coming
- And there's only more coming
and it sets this new floor.
And I'm like yeah it's crazy.
It's crazy to think about how much
demand there is for this stuff.
- Yeah. I mean this is where
all the efficiencies matter
- So much.
- There's like an unsung cast of heroes
that are making the models, you know,
2, 3, 4 times sort of more efficient.
I think the other thing you
kind of have to just step back
and realize probably
the most consistent way
to get good models is to sign up
for one of our subscription plans.
I go, this was not
a commercial
- it's not a commercial
We do give the highest rate limits
and we really do try to kind
of people that are willing to
to pay are gonna get that too.
- And if you're a student,
- that's the other part
they
actually launch day if you're a
university student in the US
- Yeah.
- Go sign up Google AI
Pro for free for a year.
That's awesome. That's $20
a month times 12 free.
- It's totally worth it by the way.
Not just for this model but
because there's just a lot coming for
- Yeah, that's right.
You'll get this model,
you'll not only get the
highest rate limits kind
of in Gemini app, you'll
get access to NotebookLM
and Flow kind of all the
good stuff coming, a lot
of our developer products too.
So it's exciting,
- It's crazy.
It is a good deal. Storage,
I mean now this is just a,
- Yeah,
- Sponsored by Google AI Pro subscription.
That's awesome. What any,
any sort of, you know,
obviously we're starting with,
which is interesting to sort
of, and I don't know if
there's a story here Tulsee
that we can tell but last
December we shipped Flash.
Flash, yeah. Gemini 2.0
Flash and it was awesome
and it was a frontier sort
of state-of-the-art model.
Now we're shipping Gemini 3.0 Pro
or 3 Pro depending
who you ask
- Gemini 3 Pro,
Gemini 3 Pro officially do like
what obviously people love
like Flash is actually
what I think made Gemini
popular in some sense.
Like it was our workhorse model.
We launched 1.5 flash back at
IO a year plus and a year
and a half ago, which I
think put Gemini in the map
and has sort of pushed
the Pareto frontier.
When are we working on
Gemini 3 Pro Gemini 3
Flash and all the other models?
Like do you have a sense of like when,
when we get smaller models and
- Yeah this is my turn to
say watch this space, but
- including the app.
But actually I think,
so we definitely want
to build out the Gemini 3 family.
So I think Pro is just
the beginning of this
and like we're already I
think very excited about the
direction we're going with Flash
and where we're gonna be with the rest
of the Gemini 3 models.
I think the reason for sort
of shipping them in sequence instead
of like bundling them all together
and trying to ship them
all as one package is one,
when you ship one of these models you get
to learn a lot about how
people are using them.
And so you get to
learn a lot about like, okay,
where is 3 Pro really resonating?
Where did we think it was
gonna resonate versus where,
where are people resonating with it?
Where are we seeing people
say, hey actually like this is
too expensive for my use case
or it's too slow for my use case
or I might actually need
something different.
Which then influences what
you do for Flash, right? Yeah.
Because Flash is meant
to be the workhorse model
and we wanna make sure we kind of account
for those pieces when we're building that.
And so I think actually
shipping them in sequence kind
of allows us to like learn from
one and build to the other.
I think it's also part
of like the relentless
shipping exercise, right?
If you wanna ship quickly,
we wanna actually like
put these models out there
and see what people do
with them and get feedback.
So yeah. But there, there will be,
there will be some exciting stuff coming.
- Thank you both, this was
an awesome conversation.
Thank you both for the hard
work of pushing to get the model
and the model into all the products.
I feel like folks are
going to enjoy the moment.
So hopefully we'll be sitting
here for IO maybe sooner next,
next year and launching other cool
models. So thank you both.
- Thank you. Thank you. Enjoy it.
- Yeah,
- That's gonna be great
- And thanks everyone for
watching Release Notes.
We'll see you in the next episode."
Pb6XHGi542A,Vibe coding with Gemini 3 in AI Studio,"Gemini 3 is our most intelligent model yet that helps you bring any idea to life. It excels at coding and masters both agentic workflows and complex zero-shot tasks. In this video, youâ€™ll learn how to build and vibe code apps with Gemini 3 Pro in Google AI Studio. Youâ€™ll build three different apps: a landing page, an interactive dashboard, and a video game.

Resources: 
Start building with Gemini 3 â†’ https://goo.gle/47Qa5cc 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Patrick Lober 
Products Mentioned: Gemini, Google AI, Google AI Studio",2025-11-18T20:00:53Z,"Google,developers,pr_pr: AI DevRel (fka Core ML);,Purpose: Learn;,Video Type:DevByte;,ct: ;,gds:N/A;",28,en,en,https://i.ytimg.com/vi/Pb6XHGi542A/default.jpg,https://i.ytimg.com/vi/Pb6XHGi542A/hqdefault.jpg,680,public,31718,794,47,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,Pb6XHGi542A,"Hi everyone, Gemini 3 Pro is finally
here and I'm super excited to show you
how you can build apps with it and how
you can vibe code different apps in
Google AI Studio. The model shows a new
level of reasoning and strong real world
agentic coding capabilities. So it's
perfect for bringing your ideas to life
and in this video I want to show you how
you can do that. So we're building some
apps in AI Studio and I want to give you
a first impression of what's possible
with the model. So, we're going to build
three different apps. The first one is a
landing page for an AI course platform.
So, here we're going to use a modern UI
with some fun colors and some nice
animations and it actually has both
light and dark mode. Then the second one
is an interactive dashboard for a SAS
business. So here we're turning sales
data into this interactive dashboard
where we can get some insights and play
around with the data. And it actually
also has a built-in chatbot feature so
we can ask Gemini questions about our
business. And then the last one is a fun
one. So we're going to vibe code a video
game. In this case, it's kind of a a 3D
space shooter clone with a more playful
design. So here we can shoot the
different objects and then it has uh
object and collision detection. So now I
crashed. Um but yeah, this is what we're
going to build. You don't need any
coding skills to follow along. The final
apps might not look exactly like this,
but uh pretty similar. So hopefully you
have some fun. So let's dive into this.
To start building apps in AI Studio, you
can go to ai.studio/builds.
And then you should land on this page
where you have the VIP coding experience
inside of AI studio. Here you can
describe your idea. And first of all
make sure to now select the new model
here. Gemini 3 Pro preview right now.
And then let me copy and paste the first
prompt in here. So we want to make a
landing page for an AI course platform
called the hidden layer. And then we
describe a little bit the style that we
want. So, we want neo brutalism style
with creative and fun design and smooth
scroll animations and googly colors at
both light and dark mode. And down here,
optionally, you could add different AI
capabilities to your app. For example,
we could add nano banana for image
generation or VO for video generation or
some of the tools like Google search or
Google maps and some more. But for now,
we keep it simple. So, let's click on
build and then Gemini 3 Pro will build
the app for you. So, it starts by
thinking about this and breaking this
down step by step. So, you can follow
the thinking steps here if you're
interested in that. Or in the meantime,
you can have a look at the loading
screen here in the middle. Here you get
some feature ideas for example or some
tips about the whole experience here and
some of the features you can use inside
of AI Studio. For example, we could
generate images with a prompt. We could
add a testimonial section. And if you're
interested in that, we could add this to
the chat and then it will add it to the
uh next prompt. But for now, let's just
wait until the app is finished. And now
Gemini 3 finished building the app. So
it ran for 86 seconds and created this
in its first try. So let's have a look.
Unlock your hidden potential. Start
learning or view the curriculum. Uh some
of the latest drops here. Why learn with
a hidden layer. Here we can leave our
email. Uh let's test the dark mode. Um
yeah. So I think it looks nice and this
is a oneshotted landing page of an AI
course platform. Now of course you can
go in and iterate. So you can describe
uh new features or changes that you
want. I also want to show you a very
cool feature. So you can use the
annotation mode and here for example
point to different um parts in your app
that you want to change. For example,
let's mark this button here and say
change the button color to green and
then accept and then let's add this to
the chat and send the prompt. So now
what's happening is that here we are
saying the prompt apply the edits shown
in the screenshots and then it's taking
a screenshots with your annotations and
Gemini has really good visual
understanding so it knows how to
transform the annotations into the code
changes. So let's wait to see if it
comes up with the correct fix. And now
it made the correct change. So now we
have a green button and then of course
you can go in and add many more features
and build this out the way you like. So
yeah, this was the first example. Let's
go to the next one. For the second
example, I want to turn SAS data into an
interactive dashboard. So here I have
this CSV file with some sales data. So
we have date, customer ID, customer
name, plan, MRR, region, churn status,
and optional churn date. and let's turn
this into a dashboard. So let's again
describe the idea that we have analyze
my SAS sales data and build a dashboard
that nicely displays the insights. Make
the graphs interactive and use a minimal
modern UI. I'm interested in total MR MR
growth over time, churn rate, average
revenue per customer, and tier
distribution. And I also want to be able
to ask Gemini questions about the data.
So for this you can click on this chip
here and add the a AI powered chatbot
feature to the app. And then here this
is optional but I want to show you how
you can turn a screenshot into a design.
So the UI should be hyper clean and
minimalist dark mode take inspiration
from the attach screenshot. And here I
took a small screenshot from the AI
studio landing page. So I kind of hope
for these for the stark mode and the
yellow and blue colors. So let's add uh
the CSV file and the screenshot to the
prompt. So we can upload a text file
here. I select the CSV and I also want
to attach the screenshot and then let's
click on build. And now Gemini 3 Pro
again starts to think about this. So
let's wait until this is finished. All
right. And it finished building the app.
So this time it ran for 144 seconds and
we have our dashboard. So let's take a
look. We have the total MRR, the active
customers, the turn rate, the average
revenue per user, the MRR growth, and
then the plan distribution, and also a
regional breakdown. And we also have our
chatbot feature here where we can ask uh
questions. And then behind the scenes,
it's using the Gemini API to answer the
questions here. And if we compare this
with the screenshots, so it definitely
took inspiration from the gray and the
yellow. Uh for my taste, it's using the
yellow a little bit too much. But then
again, we can go in here. So let's say
we also want the blue and it should
still have the screenshot in the
context. So let's again use the
annotation mode. And this time let's
draw an arrow and then let's uh point to
here. And then let's add a text. And
then use the blue from the initial
screenshot for this graph. And then
accept. And then let's add this to the
chat. And then again, let's send the
prompt and see if it can make the fix.
All right. And after a little bit of
thinking, it refreshed the app and we
now have a blue graph. This is what we
wanted here. And I think this is a nice
example of how you can turn data into an
interactive dashboard with Gemini 3 Pro.
And yeah, let's have a look at the last
example. For the final app, let's have
some fun and vibe code a video game
together. And I also want to increase
the difficulty level here. So first,
let's copy paste the prompt here and go
over this together. So create a polished
low poly toybox style 3D spaceship web
game structured as a multifile react
application. The game should feature a
bright and playful aesthetic with the
following specifications. Again, you can
just use three four lines to describe
your app, but here I want to use a much
more complex prompt and show you that
Gemini 3 Pro is able to follow very
complex instructions and then turn this
into the game that we want. So, let's go
over this quickly. I'm describing the
text tech a little bit. I want React
3JS, React 3 Fiber, and some more helper
libraries. Then, I'm describing the
style, so the visuals and the
atmosphere, a bright, vibrant
environment that resembles a digital toy
set, and then the color palette that I
want. Then, some info about the
materials and lighting and
post-processing and the environment. And
then here I described the gameplay
mechanics. So we want as the perspective
we want a fixed camera behind the ship
and then the player moves on the X and Y
coordinates and the enemies sparn as
negative set depth and then move towards
us. Then I want to use the space bar to
shoot projectiles and also I want
collision detection and explosion
effects. And then for the controls, I
want for example the arrow keys um to
control the spaceship and also I want
this to be mobile already. And then some
info about the code structure. So I want
this in multiple files and also ensure
that the canvas is responsive. So let's
click on build and see if Gemini 3 Pro
can handle this. And it finished
building our game. So let's test this.
We have the toy box starfighter and can
start the mission. So, let's click on
this. And now, let's see if I can uh
control this. So, yeah, I can shoot with
space and navigate. And it's destroying
the objects. So, let me hit the next
object. And now it's game over. And I
can see the final score. So, yeah, it
was able to build this on the first try,
which is super impressive to me. One
thing I do want to call out though is
that while doing the generation here, it
ran into some errors. So you can see two
errors running the code and then it was
able to apply an autofix. So this is a
feature that's possible inside the AI
studio environment where it can correct
itself and analyze the error messages.
But still it was able to generate this
without me having to add any prompts
myself. All right, hopefully this was
fun and gave you a first impression of
what's possible with Gemini 3 Pro and
how you can vibe code and build apps and
video games in AI Studio with it. So try
it out. Let us know what you think and
have some fun. Happy building."
3F9nDL0jGW0,What is a Service Level Objective (SLO)?,"A SLO is a structured way of understanding your systemâ€™s performance and communicating this to your users. Moreover, see how an SLO takes the guess work out of your priorities, makes shared goals crystal clear for your team, and more! 

Resources: 
Learn more â†’ https://goo.gle/4o2A6tx 

Subscribe to Google for Developers â†’ https://goo.gle/developers",2025-11-18T05:00:11Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Series: AASRE;,type:G4D SV: Educational ;,gds:Yes;,ct: AIG;,campaign: BBDS;",28,en,en,https://i.ytimg.com/vi/3F9nDL0jGW0/default.jpg,https://i.ytimg.com/vi/3F9nDL0jGW0/hqdefault.jpg,58,public,5335,95,5,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,3F9nDL0jGW0,"A service level objective or SLO is a
structured way of understanding your
system performance and communicating
this to your users. It's a great
engineering practice. You've already
designed a robust system and you've
implemented observability and you have
mature automation in place. Well, now
you can set targets for reliability. A
basic SLO might say the system that
shares cat photos will be available
99.9% of the time. And you can refine it
to say a user will get the cat photo
within 250 milliseconds after clicking
on it at the 99th percentile. That's the
real magic of an SLO. Takes the
guesswork out of your priorities, giving
your whole team a clear, shared goal for
when it's okay to build new things and
when it's time to focus on reliability.
Read lots more about SLOs's and how to
use them effectively at
s.google/resources.
Follow Google for developers to learn
more and ask your questions in the
comments."
GHPjBkwEKh4,Why does this throw a null pointer exception?  Go!,"Time for a Java debugging challenge! This code attempts to switch on an Integer wrapper type, but crashes with a NullPointerException. Can you identify the minimal fix? Share your solution in the comments!

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speakers:Dio Gado",2025-11-17T14:00:21Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Campaign: G4D: Puzzles;,Video Type:G4D SV: Educational ;,ct: AIG;;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/GHPjBkwEKh4/default.jpg,https://i.ytimg.com/vi/GHPjBkwEKh4/hqdefault.jpg,17,public,5492,61,11,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,GHPjBkwEKh4,"Snow's falling, logs are crackling, but
this RA statement still crashes. We're
switching on integer, case are bundled
up, and yet null pointer exception.
Here's the question. Why does this throw
a null pointer exception? What is the
minimal edi? Drop your solutions in the
comments."
QAXe1nfo8Jk,It was â€“ indeed â€“ not their first day.,"That ""first day"" state of mind ðŸ™ƒ

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Crystal Endless, Justine Lai",2025-11-14T20:00:25Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Influence;,Campaign: ;,type:G4D SV: Comedic skits;,ct: AIG;;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/QAXe1nfo8Jk/default.jpg,https://i.ytimg.com/vi/QAXe1nfo8Jk/hqdefault.jpg,12,public,31197,316,18,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,QAXe1nfo8Jk,"Hey, I think you forgot to pull the
latest changes before you pushed your
code. Oh, I'm sorry. It's my first day.
>> You're [music] going to love me. You're
going to love me. You're going to love
me."
RQ_WppuZxWk,5 ways Genkit makes building AI apps faster âš¡,"Genkit is an open source framework that helps you build, deploy, and monitor production-ready AI-powered apps. Learn 5 ways this tool can streamline your projects. 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: M.E Francis 
Products Mentioned: Google AI",2025-11-13T20:00:21Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Influence;,Campaign: STV; type:G4D SV: Educational ;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/RQ_WppuZxWk/default.jpg,https://i.ytimg.com/vi/RQ_WppuZxWk/hqdefault.jpg,57,public,7767,128,14,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,RQ_WppuZxWk,"Here are five ways that Genkit makes
building and shipping AI apps faster.
One, Genkit supports multiple models,
not just Google's, [music] and you can
add more. It runs on any cloud platform
and integrates with observability
[music] systems through open telemetry.
Two, you can test with the dev UI and
skip building custom UIs or manual
tooling. Genkit's developer UI lets you
call and tinker with your AI flows,
saving [music] serious dev time. Three,
Genkit gives you full observability.
Genkit's dev UI gives you a trace of
what happened when your AI code
executed, inputs, outputs, and every
step in between. Four, it has tools for
LLMs. Define functions [music] in your
code, register them as tools, and let
your LLM call them as part of a flow.
And finally, number five, Genkit [music]
has built-in eval. Turn your manual
tests into data sets, run evaluations,
and review results so you can measure
and improve your flows [music]
continuously. All of this in one
streamlined developer experience. Try
Genkit and start shipping AI ops"
hQ6nqDP905k,Introduction to Metrax: Evaluation metrics for JAX,"Metrax is a JAX-native, high-performance, open-source evaluation metrics library developed by Google. Metrax offers standard evaluation metric implementations using JAX, allowing you to focus on your model and training. 

Resources: 

Documentation â†’ https://goo.gle/3LFkZIR 
GitHub repo â†’ https://goo.gle/3WMu05o 
Example notebook â†’ https://goo.gle/3JYO0yN 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Yufeng Guo 
Products Mentioned: Google AI",2025-11-13T17:01:15Z,"Google,developers,pr_pr: AI DevRel (fka Core ML);,Purpose: Learn;,Video Type:DevByte;,ct: ;,gds:N/A;",28,en,en,https://i.ytimg.com/vi/hQ6nqDP905k/default.jpg,https://i.ytimg.com/vi/hQ6nqDP905k/hqdefault.jpg,320,public,3054,107,9,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,hQ6nqDP905k,"While evaluating metrics might seem
straightforward enough at first glance,
it becomes considerably more complex
when dealing with largecale training and
evaluation across distributed compute
environments, which is pretty common
these days in training large-scale AI
models. My name is Euango and in this
video we're going to introduce the
Metrax library made for doing evaluation
scoring of your machine learning models.
Metricx is a Jax native high performance
and open-source evaluation metrics
library developed by Google. The
ultimate goal is to allow developers to
focus on the model evaluation results
rather than expending effort on
re-implementing and verifying various
metrics definitions themselves. One of
METRAX's core strengths lies in its
foundation as a Jax native library,
which allows it to leverage key JAX
features like VMAP and JIT or just in
time compilation.
Many of Matrix's metrics are jitterable,
meaning that they can be seamlessly
integrated with the Jacks.jet function,
significantly improving performance and
accelerating your evaluation workflows.
While not every metric can be JIT
compiled, the library is designed with
best practices, ensuring that all
metrics are still well written and
optimized where possible. Metrax offers
a comprehensive suite of predefined
metrics commonly used across various
types of machine learning models,
including classification, regression,
recommendation, vision, audio, and
language models. The library will
incorporate even more metrics in the
future as the community's needs evolve.
I'll briefly highlight some of the
specific metric categories and
capabilities that are particularly
noteworthy.
First up, let's check out the ranking
metrics at K. This one is a personal
favorite. Metrax has the ability to
compute at K metrics for multiple values
of K in parallel. This feature allows
for a much more comprehensive and
efficient evaluation of your ranking
model's performance. For instance, you
can compute things like precision at K
or recall at K for several different K
values like K equals 1, K equals 8, K=
20 all together in a single forward
pass, saving considerable time and
computational resources compared to
running separate computations for each
value of K.
Next, it's worth mentioning that Metrax
has a number of standard natural
language processing metrics, too, such
as perplexity, blue score, and rouge.
The library also includes word error
rate, which is really useful for speech
recognition or text generation by
measuring the edit distance between
text. For computer vision applications,
METRAX provides metrics like
intersection over union for semantic
segmentation and both peak signal to
noise ratio and structural similarity
index measure for image quality
assessments.
Of course, you'll still find those
classic metrics like accuracy,
precision, recall, F1 score, and more.
On a practical level, Metrax is designed
for easy integration into your existing
Jax workflows and provides a consistent
functional API. This API typically
involves three main method calls from
model output used to create the metric
class from the inputs. Merge to update
results each time there are more model
outputs and compute to get the final
result. This makes it straightforward to
incorporate Metrax metrics directly into
your training or evaluation loops,
especially for jitterable metrics.
Metrax is already being used by several
Google core products. Teams within
Google search, YouTube, and Google's own
post training library, Tunix, are
leveraging Metrax for their evaluation
needs. So you can use Metrax knowing
that it has reliability and performance
in demanding real world applications.
Metrax has taken on a communitydriven
development approach and is developed as
a GitHub first repository. This means
that the project is open and actively
welcomes community contributions. In
fact, some of the metrics available in
Metrax today were added by external
community contributors. So, if you have
ideas for new metrics or improvements, I
encourage you to submit a pull request
and collaborate with the development
team. Metrax is released under the
Apache 2.0 license. I'll link the
documentation and GitHub repo in the
description down below. The
documentation provides really detailed
explanations of all the metrics and
their definitions. So, I hope you find
it to be a valuable tool in your machine
learning endeavors. Do you have a
favorite machine learning metric? Share
your thoughts in the comments below. And
if this video was helpful, let me know
by clicking the like button. That's all
for now. I'll catch you in the next one."
iezyXHUniVI,How are AI agents useful for reliability?,"Agents are autonomous helpers powered by AI that take actions. Specifically, they can help you move from reactive to proactive levels of maturity, digest a plethora of alerts from operations, and more! See how AI agents can help you with SRE. 

Watch more Ask A Site Reliability Engineer â†’ 
Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Nastassia Mitskevich 
Products Mentioned: Google AI",2025-11-12T05:01:03Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Campaign: BBDS;,Series: AASRE;,type:G4D SV: Educational ;,ct: AIG;,gds: yes;",28,en,en,https://i.ytimg.com/vi/iezyXHUniVI/default.jpg,https://i.ytimg.com/vi/iezyXHUniVI/hqdefault.jpg,82,public,5431,152,9,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,iezyXHUniVI,"How are AI agents useful for reliability
engineering? Agents are autonomous
helpers powered by AI that take actions.
[music] Yes, even capable of changing
things in production based on their
knowledge of your systems. This
knowledge comes from documentation,
configuration such as Terraform and from
the systems code. Agents can also
discern more runtime [music] information
from logs and system messages. When used
with thought and care, AI agents can
help you move from a reactive
firefighting level of maturity to a
proactive and preventative level. At
this level, agents adjust available
capacity in anticipation of changes in
demand, raise alerts or [music] make
changes based on error budgets. And
significantly, AI agents can digest the
massive amount of alerts, system logs,
traces, [music] and other automatically
generated information about your
operations. This means that the AI
behind the agent becomes the first
responder [music] and escalates to you
the human operator only when it can't
determine an appropriate path of action.
Of course, you need to develop, run, and
verify these agents. The best work is
[music] done with AI and people working
together. AI agents have tremendous
power. Assess your [music] risks
carefully before giving any AI tool your
production credentials or providing any
AI [music] access to your production
systems. To learn more about site
reliability engineering, follow Google
for desk and comment here [music] with
your questions."
qf_HIvdmNcw,What's the one tool you couldn't work without? Share your go-to. ðŸ‘‡,"What tools help boost your productivity? Let's hear what these devs have to say. 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Event: Google IO",2025-11-11T20:00:53Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Influence;,Video Type:G4D SV: I/O ;,ct: AIG;;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/qf_HIvdmNcw/default.jpg,https://i.ytimg.com/vi/qf_HIvdmNcw/hqdefault.jpg,43,public,5587,107,4,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,qf_HIvdmNcw,"One thing that I like is uh Google's
deep research. There's a lot of noise
out there on internet. But I feel deep
research helps [music] me understand
where the source is coming from. That
helps me really validate if it's a right
thing that I'm actually [music] going
through or is it something like is it a
fabricator or madeup data. So a secret
weapon for me would be to use Google
sheet for like literally [music]
everything. If I'm managing like a
manager or if I'm doing something which
is on my [music] own personal list, I
would be just using Google sheet. So I
have a lot of shortcuts in VS code which
makes me feel like lot of efforts are
like decreasing. If you do control D and
you can select a lot of elements and
then you can delete all and you can
select all you can [music] write all
together. So there those are the few
shortcuts which I keep it secret all you
know kind of my secret [music] tip."
GH1Ns0Wenec,Which DevFest dev are you?,"DevFest so you can manifest your personality!

Resources:

RSVP for DevFest here â†’ https://goo.gle/43wInOK 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Event: DevFest 

Speaker: Crystal Endless",2025-11-11T17:00:24Z,"Google,developers,pr_pr: Google Developer Groups;,Purpose: Learn;,Campaign: ;,Video Type:Trailer/Promo;,ct: AIG; ;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/GH1Ns0Wenec/default.jpg,https://i.ytimg.com/vi/GH1Ns0Wenec/hqdefault.jpg,23,public,4946,73,5,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,GH1Ns0Wenec,"I'm letting my network know that I'm
going to be at Defest so I can meet up
with people there. I'm just going to
show up and have fun. I have a QR code
that leads to my personal [music]
website and my socials. I'll exchange
contact info if people ask. I read all
the session descriptions and I'm
preparing questions to ask the speakers.
I'll just figure out what sessions are
interesting [music] when I get there.
I'm challenging myself to connect with
five new devs who live in my city. I
hope I meet some cool"
U4r5v837JaA,What do you think this Python code will print? Go!,"Here's a Python closure puzzle that catches even experienced developers! This code creates lambda functions in a loop, but the output might surprise you. The issue involves how Python handles variable binding in closures - a subtle behavior that can lead to unexpected bugs. What do you think gets printed when we call these functions? Understanding closure semantics is crucial for functional programming patterns.

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speakers: Juan Vasquez",2025-11-10T17:01:01Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Campaign: G4D: Puzzles;,Video Type:G4D SV: Educational ;,ct: AIG;;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/U4r5v837JaA/default.jpg,https://i.ytimg.com/vi/U4r5v837JaA/hqdefault.jpg,18,public,5968,68,10,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,U4r5v837JaA,"Here's a little Python brain teaser.
We're creating a few lambda functions
inside a loop. Nothing fancy, but the
output might be a little bit surprising.
Check it out. What do you think this
will print? Drop your [music] answer in
the comments. This one can catch people
who have been writing Python for years."
r3-x0GtOmmc,Code Execution with Gemini | Intro to Tools,"Tools extend the abilities of large language models by connecting them to the outside world. Learn how to use the Code Execution tool with Gemini, which enables the model to generate and run Python code, for problems that involve math, data analysis, and code-based reasoning. This is Part 2 of our series on using tools with the Gemini API.

Subscribe to Google for Developers â†’ https://goo.gle/developers

Products Mentioned: Gemini, Google AI Studio,  Gemini API",2025-11-08T00:58:43Z,"Google,developers,pr_pr: AI DevRel (fka Core ML);,Purpose: Learn;,Video Type:DevByte;,ct: ;,gds:N/A;",28,en,en,https://i.ytimg.com/vi/r3-x0GtOmmc/default.jpg,https://i.ytimg.com/vi/r3-x0GtOmmc/hqdefault.jpg,289,public,10195,253,12,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,r3-x0GtOmmc,"Large language models have come a long
way when it comes to solving math
problems, but fundamentally they still
work by predicting the most probable
next word in a sequence based on
patterns learned during training. In
other words, they aren't calcs and they
aren't the most efficient or accurate at
numerical computation.
But by connecting large language models
to tools, we can extend their abilities.
So, let's check out the code execution
tool, which makes Gemini more effective
at problems that involve math, data
analysis, computation, and code-based
reasoning. Okay, I'm going to jump into
Google AI Studio, and I'll make sure I
have Gemini Flash latest selected as my
model, and we'll type
calculate the sum of the first 60
prime numbers. And I did check this
beforehand. The answer is 7,699,
but let's see what Gemini says here. Oh
boy. All right, we got 8,368.
So, I've run this query a bunch of
times. Um, sometimes it works, sometimes
it doesn't. Um, the model has a whole
lot of hard work that it's put into here
to try and solve the problem. But again,
to come to the answer, it's really just
using this next token mechanism um
predicting the most likely next token.
And that's not the best way to solve a
math problem like this. So, we can use
the code execution tool to give Gemini a
little bit of help here and improve the
ability to come up with a correct
answer. So, what I'm going to do is I'm
going to copy this prompt and let's
restart. I'll type it in again here, but
this time I'll make sure I have code
execution selected in the tool toggle
over here and we'll run the prompt. And
this time, you can see we actually did
get the correct answer, 7,699.
But instead of that kind of like long
string of of text that the model had
produced earlier trying to come up with
an answer, it's returned Python code. So
while LLMs maybe aren't the best at
doing actual mathematical computation,
we know they're really really good at
language and they're really good at
generating code. So we can generate the
code to solve the problem, which in this
case is a function that checks if a
number is a prime number or not and then
adds up um the first 60 prime numbers.
And this Python code is executed um on
the API back end and it's returned and
the result is returned to you right here
which is 7,699.
So not only does this make the model
better at doing math, but it also gives
us this nice chunk of code here where we
can always refer to and see, okay, where
did this answer actually come from?
Okay, so let's check out a more
practical example of how you might use
code execution um in the real world. So,
here's a little app that I made with uh
Vibe Coding in AI Studio and uh it's my
CSV Genius app. So, we're going to
upload a file here and we'll say what
fruit did I sell the most of
parenthesis quantity. Okay, so let's see
what this is. So, while the model's
thinking, here is the uh CSV file that I
uploaded. It is data from my tropical
fruit stand that I wish I indeed owned.
Um so there's a transaction ID, the
date, the product sold, um quantity,
price, etc. So the idea here is can we
ask questions about this data um things
that you would sort of normally as a
data scientist maybe try and solve in
pandas. Um but this makes it a little
bit easier if you don't know how to use
pandas and you don't know how to um do
this kind of data analysis. We'll let
the model do it for us. So, we'll jump
back into the app here, and you can see
it says that I sold the most of mangoes,
and that was 63 units. So, that's all
fine and great. Uh, but now I'm
wondering, well, how did it actually
come up with that answer? Um, so I have
this little generated code toggle right
here that I've added in. And we can see
the model's loaded in the data. And then
once it's done that, here is the little
code snippet that Gemini wrote that was
then executed. So, it loaded in my data
as a data frame. It filtered for fresh
fruit and then um eventually prints out
the fruit that I sold the most of. So
now we know where this answer came from.
It was this code that was generated by
Gemini that was executed on the back end
and that answer was returned to um
returned to the user and that's where
Mango came from with 63 units.
If we toggle over to the code section,
you can see here in the code uh during
the AI chats create method um we have
Gemini 2.5 Flash selected as our model.
And then we also have the code execution
tool uh being passed here. And so that
is how the model is able to call out to
that tool to run the Python code that it
generates. And that is all for code
execution. But if you build something
cool with this tool, definitely let me
know."
u5sQDKot0qE,Building AI Agents with ADK Go,"Introducing the  Agent Development Kit (ADK) Go! This new toolkit is officially here, designed for Go developers ready to build powerful generative AI applications.

Get started with our hands-on tutorial, where we'll guide you through building your very first intelligent Go agent. You'll learn the core concepts and see how the ADK Go streamlines the development of sophisticated AI solutions natively in Go.

Resources:
Google for Developers â†’ https://goo.gle/developers
ADK documentation â†’ https://google.github.io/adk-docs
Sample agents â†’ https://github.com/google/adk-samples
ADK Go source code â†’ https://github.com/google/adk-go

Chapters: 
0:00 - Introducing the Go ADK
0:53 - Build your first Go agent
1:49 - Run your agent
2:23 - Understanding Tools
3:28 - Run your agent with tools
5:00 - Multi-agent patterns in Go
7:58 - State, Sessions, and Events 

Subscribe to Google for Developers â†’ https://goo.gle/developers 


Speaker: Ivan Cheung 
Products Mentioned: Agent Development Kit (ADK)",2025-11-07T17:13:59Z,"Google,developers,pr_pr: Google Cloud Tech;,Purpose: Learn;,Video Type:DevByte;,ct: AIG;,gds:N/A;",28,en,en,https://i.ytimg.com/vi/u5sQDKot0qE/default.jpg,https://i.ytimg.com/vi/u5sQDKot0qE/hqdefault.jpg,602,public,11012,428,26,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,u5sQDKot0qE,"Hi, I'm Ivan, an engineer working on the
agent development kit. I'm excited to
announce Go ADK. Go ADK is a handcrafted
AI agent library idiomatic for Go. In
this video, we're going to learn all
about what it is, how it works, and how
you can get started.
Building AI agents from scratch, dealing
directly with LLM APIs or SDKs can
quickly become complex. You're managing
state, memory, tool calls, control flow,
and more. often reinventing the wheel
for common patterns. That's precisely
what the Asian Development Kit or ADK is
designed to address. It's a flexible and
modular open source framework
specifically built to simplify the
development, deployment, and evaluation
of AI agents. Let's get started with Go
ADK. Our goal is to run your first Go
agent in about 5 minutes. Then we'll
dive deeper into some of the more
advanced concepts. The first step, as
with any library, is including ADK in
your project. Since we're using Go, it's
as simple as running this go get
command. Now before we look at the code,
let's discuss the notion of agent in
ADK. There are three main categories of
agents. LLM based reasoning agents. They
decide which tools to call or which
agents to transfer to. Then there are
workflow agents for more prescriptive
and deterministic logic flows. Finally,
you can also build custom agents where
you can write arbitrary logic like
combining both the LLM and workflow
agents. Let's create an assistant for
teaching science to students. The agent
is defined by a name, a description, an
LLM model object, and instruction
prompt. To run the agent, we'll use an
in-memory session service. Create a
session to hold the conversation and a
runner to run the agent. Once the agent
runs, it returns an iterator of events,
which we can print out in parts.
Okay, let's put this in a main function
and run it just like any other Go
program with go run. First, I'm going to
say hello.
Great. Now, let's give it a science
question.
Nice. That looks good to me. We also
have a web UI interface. After some
initial setup, you can open up the web
UI and choose your science agent from
the menu. I'm going to say hello again.
Why is the sky blue? In this event tab,
you can see the events that are flowing
through.
Cool. Let's move on. Now, what if we
want our agent to talk about recent
events that aren't in the LLM's training
data? To do that, we have to give it
access to Google search via tools. Think
about it. A large language model is
incredibly powerful with text or
multimedia elements, but it can't
natively browse the internet, query your
database, send an email, or call an
external API. Tools can augment your
agents with these capabilities. ADK
provides a structured mechanism for your
agent to use these tools. The process
generally looks like this. Your agent
receives a user request. The LLM
analyzes it, determines if an external
action is needed, and decides which
register tool to use along with the
arguments for that tool. The ADK
framework then intercepts this function
call from the LLM, executes the
corresponding code or action you've
defined for that tool, and finally, the
tool's result is sent back to the agent,
usually to the LLM again, so it can
process the result and formulate the
final response to the user. ADK
abstracts this away, providing a clean
interface for defining tools and
handling that entire execution loop for
you. Go ADK provides three types of
tools for your agents to use. Built-in
tools like Google search, third party
tools like the ones provided by MCP
servers or function tools. Let's talk
about function tools. A function tool
essentially wraps arbitrary go code and
makes it available to the agent. If
you're building a customer service agent
to let customers ask about the status of
their order, you can add a function tool
in the tools method of your agent
definition. Now let's see how that
function is defined. First, you define
strrus to represent the input and return
values. Then a function with your custom
go logic. In this case, we just return
the complete status for all orders. We
then pass this function to the function
tool function which transforms it into a
tool object for us. We provide a
suitable description that the LLM will
use to determine when it should call the
tool. Let's chat about prompt quality.
It's critical to give precise and
explicit instructions to define the
agents purpose, its persona, its goals,
and to provide guidance in how it should
interact with the user and tools. The
tool should also be described with great
detail so that the LLM clearly
understands its purpose and its
arguments. With these basic steps,
adding the dependency, defining tools,
creating and configuring your agent, and
calling the runner's run method, you can
quickly get a functional agent up and
running. It'll be capable of
understanding natural language requests
and performing actions through the tools
you've defined. This forms the essential
foundation for building more capable and
complex agents in Go. Next, let's get
into how we can implement more complex
concepts. So far, we've only used the
single agent, but we can have multiple
agents collaborating together in a
multi- aent scenario. Go ADK supports
interoperability for tools via MCP and
for agents via the agentto aagent
protocol. We'll talk more about these in
future videos. For today, let's explore
how to build multi-agent architectures
using ADK's native functionality. A
powerful pattern ADK enables is allowing
one agent to use another agent as a
tool. This agent tool concept lets you
build hierarchical or collaborative
systems where agents delegate tasks to
others. Let's say your main agent needs
a tool to create summaries of long text.
You can define a summarizer agent like
this. Then use the agent tool new
factory function to wrap it into a tool.
In your root agent, pass in the summary
tool on creation. The main agent can
then call the summarize agent to do a
one-off summarization. For more
prescriptive agent systems, ADK's
workflow agents can orchestrate complex
flows involving multiple agents. The
sequential agent is the first of the
workflow agents. It's simply a sequence
of agents called one after another with
the outputs of each agent being passed
on to the next one. The parallel agent
allows concurrent execution of each sub
aent. For example, if you're building a
research system, you might run multiple
agents to find news for a given topic.
The results can then be collected and
summarized by a downstream agent.
Similar to a sequential agent, the loop
agent calls a chain of agents in
sequence. However, it may run this chain
multiple times until an exit condition
is met. For example, you might have a
storyw writing system that takes an
initial draft and iteratively adds
details only exiting when a critique
agent is satisfied with the final
output. Now, this brings up an important
question when designing agentic systems.
How do you decide whether to use a
single agent, multiple agents, or a
workflow? A single agent is suitable
when the task requires reasoning and
dynamic tool use based on input, but the
overall process isn't easily broken down
into very distinct sub problems or
domains. Similarly, a single agent is
fine if there aren't too many tools at
its disposal. So, it'll be easy for it
to figure out which one to use. For
example, our science teacher agent from
earlier was fine as a single agent
system. Multiple agents enabled by ADK
are ideal for complex problems that
naturally decompose into smaller, more
manageable subtask or require different
areas of expertise. For an e-commerce
order processing system, you might use
several agents, one to verify inventory
levels, another to process the payment,
and a third agent to coordinate shipping
logistics. This approach offers
modularity and can make the system
easier to develop and maintain, although
there might be some overhead in
communication or latency. Choosing the
right approach depends on a problem
scope, required flexibility, and need
for distinct areas of intelligence or
capability. A key aspect for handling
complex ongoing interactions is context
and knowledge management. ADK provides
comprehensive tools for managing this.
The core concept here is the session.
Each session represents a single ongoing
conversation thread with a user.
Sessions have key identifiers like a
unique ID, an application name, and a
user ID, helping you manage multiple
conversations for different users and
applications. Sessions also store the
history, a shared state map, and tell
you when the last event happened. The
session maintains a history of all the
interactions, messages, tool calls,
results, and more. These are stored as a
sequence of events. Let's take a look at
the go definitions per session and
state. The state is your agent's
temporary scratch pad, a place to store
data specific to this particular
conversation. Let's see how we can
specify the initial state. By revisiting
our science teacher example, we improve
the agent by making the instruction
generic such that the teacher's topic
can change depending on a state
variable. Notice the curly brace
placeholders accepting different
subjects and audiences. When we
initialize the session, we can specify
the initial topic and audience. In our
documentation and samples, you'll see
examples of how to update state via
callbacks, tools, and a special output
key mechanism. Speaking of persistence,
for knowledge that needs to survive
beyond a single conversation, you need
long-term memory. ADK addresses this
with the memory service, which we'll
cover in a future video. And that brings
us to the end of our introduction to ADK
Go. We've covered key concepts like
session, state, and events. We've also
seen some examples of agents using tools
and even calling other agents by
combining workflow orchestration,
multi-agent collaboration, and robust
state and memory management. ADK
provides building blocks to create
sophisticated AI agents for your most
challenging problems. Start building
today and check out ADK documentation
and samples available in Python, Java,
and of course, Go. See you in the next
one.
[music]"
QagTKYVfF_k,Differences between AI automation and augmentation? ðŸ¤”,"Automation is when AI completes a task with minimal user input and handles routing tasks. Augmentation amplifies the userâ€™s own skills, maximizing their creativity.  See how you can strike a balance between these powerful approaches to unlock new possibilities and enhance human potential.

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Jonathan Caton 
Products Mentioned: Google AI",2025-11-07T05:00:29Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Campaign: BBDS;,type:G4D SV: Educational ;,ct: AIG; ;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/QagTKYVfF_k/default.jpg,https://i.ytimg.com/vi/QagTKYVfF_k/hqdefault.jpg,73,public,5757,208,4,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,QagTKYVfF_k,"AI can become a genuinely helpful tool
in your product if you can balance
automation and augmentation. Automation
is when AI completes an action with
minimal user input. It's more of a tool
that works in the background [music] to
handle tasks for the user. It's ideal
for tasks that are repetitive and need
little oversight or when the task is so
challenging is better suited for
machines. Whichever the case, it should
be easy for users to recover from any
mistakes. Augmentation is when AI acts
as a partner to amplify a user's own
skills. It's a collaborator that
enhances the user's ability when and
where they need it. It helps users
explore and understand the impact of
their choices. Augmentation works best
for high stakes, complex, or
unstructured [music] tasks that require
personal responsibility so people can
remain in control. Remember, the goal
isn't total autopilot. It's about
building the perfect co-pilot. Great AI
products will often blend both
automation to handle the routine and
augmentation to help people maximize
their creativity. Finding this perfect
balance starts with [music] learning
what people are trying to do and what
type of AI assistance will be most
helpful to them. Explore the people plus
AI guidebook for frameworks [music] and
best practices around building with AI."
lytlt8gBAYo,THINK FAST: register for Devfest!,"Where did the box go? Never you mind. Just ask us about DevFest.

Resources:
RSVP for DevFest here â†’ https://goo.gle/43wInOK 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Event: DevFest 

Speaker: Crystal Endless",2025-11-06T20:00:47Z,"Google,developers,pr_pr: Google Developer Groups;,Purpose: Learn;,Campaign: ;,Video Type:Trailer/Promo;,ct: AIG; ;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/lytlt8gBAYo/default.jpg,https://i.ytimg.com/vi/lytlt8gBAYo/hqdefault.jpg,9,public,22338,154,13,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,lytlt8gBAYo,"Are you wondering why the fox didn't hit
me? Well, I was wondering why you have
an RCVP to your local Dubfest."
yx9zHnBK85o,Just in ðŸ“°: Google Play Console updates -Gemini localization & deep link validation,"Uncover powerful new tools and features in Google Play Console designed for app developers. See how you can leverage Gemini for effortless localization, ensure reliable deep linking, and more!  Explore how these updates can accelerate your app's success and market reach.

Resources: 
 Learn more â†’ https://goo.gle/49711R4

Subscribe to Google for Developers â†’ https://goo.gle/developers 

#GoogleDeveloperNews

Speaker: Andrew Brogdon
Products Mentioned:  Gemini",2025-11-06T17:29:09Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Influence;,Campaign: ;,Video Type:G4D SV: Educational ;,ct: AIG; ;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/yx9zHnBK85o/default.jpg,https://i.ytimg.com/vi/yx9zHnBK85o/hqdefault.jpg,58,public,5118,111,6,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,yx9zHnBK85o,"Ready to supercharge your app's growth?
We're excited to share the latest
updates from Google Play. First, we've
added new overview pages in Play
Console. Now you can instantly see how
your app is performing across all your
key goals. Even better, we give you
recommended actions so you know exactly
where to focus [music] to boost your
business. Next, we're making
localization simpler than ever. We've
integrated the power of Gemini right
into Play Console to give you
highquality translations for your app
[music] strings at no cost. New app
bundles get translated automatically,
speeding up your time to market. And
yes, you are still in full control to
preview, edit, or disable them. Finally,
[music]
say goodbye to broken links. We've
launched a streamlined deep link
validation experience. [music] You can
now instantly test your deep links using
a built-in emulator and see the exact
user experience on the spot. Well,
that's it for this week's bite. Check
out the blog post linked here for all
the details and access these new
features and play console"
ukW6n0UCp5g,Supercharge your agents with Agent2Agent (A2A) Protocol! âš¡,"AI agents can write code, fetch data, and handle multi-step tasks, but often work in isolation. See how the Agent2Agent (A2A) Protocol can help your agents communicate, coordinate, and complete tasks across different systems. 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: M.E Francis 
Products Mentioned: Cloud",2025-11-06T05:01:10Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Influence;,Campaign: STV; type:G4D SV: Comedic skits;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/ukW6n0UCp5g/default.jpg,https://i.ytimg.com/vi/ukW6n0UCp5g/hqdefault.jpg,60,public,6673,154,7,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,ukW6n0UCp5g,"Have you tried using AI agents yet? They
[music] can write code, fetch data, and
handle multi-step tasks. But so far,
most AI agents work in isolation and
can't collaborate or share tasks with
[music] each other. That's where the
agentto agent protocol or ATA comes in.
It's an open standard that gives agents
[music] a common way to communicate,
even if they're built by different teams
or using different frameworks. Under the
hood, it uses proven technologies, HTTP,
[music]
JSON RPC, and server sent events. Agents
use these to share capabilities,
exchange messages, and coordinate
[music] to complete tasks. It's designed
for real world production. Secure by
default, supports longrunning jobs, and
works across text, audio, or video. And
the best thing, because AA is an open
standard and not tied [music] to any one
model or provider, it's easy for agents
to collaborate across different systems.
And it complements the model context
protocol, [music] which a lot of devs
are using already. If you're building
agents today, AA is worth checking out.
[music] It can help your agents
interoperate and break up complex tasks
across specialists.
>> [music]"
NWNNDvehBIU,How to get started on Opal,"Megan Li  - Senior PM @ Google Labs - walks through a tutorial of Opal's visual editor. Learn how to create a new AI workflow or mini-app using natural language and a visual app builder. 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Megan Li 
Products Mentioned: Gemini",2025-11-05T20:35:20Z,"Google,developers,pr_pr: AI DevRel (fka Core ML);,Purpose: Learn;,Video Type:DevByte;,ct: AIG;,gds:N/A;",28,en,en,https://i.ytimg.com/vi/NWNNDvehBIU/default.jpg,https://i.ytimg.com/vi/NWNNDvehBIU/hqdefault.jpg,407,public,35103,782,37,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,NWNNDvehBIU,"Hi, I'm Megan. I'm a product manager on
the Google Labs team and I'm going to
walk you through a quick tutorial on a
product called Opal. It's a noode mini
app builder that helps you automate
workflows and build mini apps for
yourself or your friends. For most
people, this top section will be empty
since you don't have any Opals yet. So,
most people actually start at the
gallery to look for examples and
inspiration of what you can do with
Opal. Let's click create new and put in
an idea I had.
So, this is an app that plans a weekly
meal plan. I want it based on the number
of people and the number of times that I
might want to cook in a week. And this
is something I do every week. So, I like
that it's repeatable and I can always
come back to it. And while we're waiting
for that to load, this is what we call
the visual editor. So, here essentially
is the logic behind the app idea that I
just suggested. And we can actually take
a look at each one of these steps. So,
the first one, the yellow is user input.
That's right here. You can also manually
add these blocks at any time to your
canvas. So, you could always build it
from scratch as well. So, this asks
essentially the user what the number of
people might be. And then this asks a
cooking frequency. Great. I'm looking
here. This is the generate step here. So
again, this is now looking at asking a
generative model and in this case it's
Gemini 2.5 flash. But as you can see in
the dropown, you can select any AI model
you want from the Google ecosystem. And
here you can see they've actually
expanded my prompt so that it's way more
detailed than what I just put in that
original prompt box. And then this looks
like the final output step. So this is
showing me what it would look like when
I run the app. Uh so let's give it a go.
So I'll click here. This is the start
button. And essentially what we're doing
is previewing the app. Let's say two
people and I only want to cook four
times this week. And now it's working.
And if I go here to console, you can
actually see all the interim stuff. So
what's actually happening while this app
is starting to run. So here I can see
that they're calling a specific model. I
can expand that as well. I don't really
need to see all those details, but if I
were curious, here's what I could look
at. [snorts] And while it's doing that,
let me show you also the theme. So this
is the section where you can actually
change that original cover image. And so
here you can either generate something
or you can upload it directly. And if
you hit randomize, you'll get something
that feels relevant to the app. All
right, let's take a look and see how
that's going. All right, you can already
start to see some content coming out. So
I can see the original meal plan that
it's generated, but it looks like it
hasn't generated the final output yet.
So once this is ready, I'll actually
have a nice little landing page for
people to see all the meal prep preps
that I've made.
All right. So, if I go to the preview
tab, I can see that it's ready. And I
can also click to the app toggle to see
the more immersive view. Great. So, I
can see here there's a few meals
planned. All right. They know that I'm
only cooking four times, which is nice.
Cool. So, that's the output. And if I
wanted to save this exact HTML output
with someone, I could click this share
output button. I'd have to share the
Opal first. So, I'd do that by clicking
up here. I click share app. You can
publish this, which means that anybody
that you send the link to could be able
to access your Opal. And then once you
do that, you'd copy this publish link to
send to other people. But once you've
done that, you can easily click this
share output button like I tried to show
you earlier. And if you click this copy
share URL, then other people can just
open it and go directly here. Um, but if
you wanted people to actually go to your
Opal or run it themselves, then you
would share this publish link with them
instead. And this link would take them
here. So if we start over, you could
have anybody run the app that you just
made.
All right, let's make some edits now. So
here, let's try natural language. That's
the easiest thing to do. And here, I
just want the app to ask me what
leftovers I have to make sure we
actually use those ingredients. But if I
wanted to do this manually, I could have
also clicked this user input and it
would have added another field to the
board. So I would have been able to edit
it by saying what I want that step to do
and then I would just add a little
connection here. So here you can see
that because I entered it here, it did
it for me automatically. But I could
have just manually added one here. All
right. And actually the same goes for
everything here. If I wanted a different
step, let's say I want the model to do
something else. So after I get a menu, I
want it to generate an image of
something. I could have done that here
and selected a different model. All
right. But for the sake of these
changes, let's add one more here and say
generate some images to go along with my
meals. All right. So, when you take a
look at what actually changes here, that
was the first change was pretty obvious
because it just added an input to the
board. For this type of change, you'd
actually have to click in and see what
changed about each step. So, here
because I expect some images to go along
with my meals, I'd actually expect
another generate step here created to be
able to ask an image generation model.
So here it's using image in4. Let's
actually use nano banana which is the
fun newest image model. Um and you can
see it's actually just added the step to
my flow. And at the end of the day you
can see here also it's now accounted for
the fact that there's images. So in
output I would expect to see not only
the recipes and the meal plans but also
the images that go along with it. Before
I go on to other topics I also want to
show you a few other hidden features. So
here you can also see version history.
If you click here, you can edit the
title and description of your Opal, but
it's automatically titled for you here
as well. The description actually just
goes right below here. So, even if you
had nothing, you can edit this field as
well. Um, and again, theme. If you had
themed this or uploaded your own cover
image, you would see that image
portrayed here. Uh, lastly here, you
also have some some controls to undo and
redo. So, you can click those or use
these to zoom in or zoom out of your
canvas. And this button just centers
you. So, you can always come back to
home. A few more features I want to show
you. So the first one is adding an
asset. So for this example, I wanted to
make a pet comic strip. I wanted to let
someone upload their pet photo, their
name, and then be able to generate this
specific style of comic strip every
time. So I clicked add asset and then I
upload an image, but you can also pull
from drive, YouTube, etc. And here I
wanted the cartoons to always come out
this specific look. So what happens here
is this prompt actually references that
specific image and tells the model that
it needs to take inspiration from that
photo when it generates the comic strip.
So you can imagine this happening with
anything. So if you uploaded a doc, you
could copy that specific structured
format or you could give a video or an
image to add inspiration or ask the
model to mimic that exactly. Another
neat feature is actually the ability to
change the output. So previously I
showed you what it looks like when you
have an output as essentially a web
page. But here what you can also do if
you click here is save something to
Google Doc or a slide or a sheet. So if
you generated let's say a custom report.
So here it's like viral trends for
social media. I could just change this
to save to Google Docs. And then when I
looked at the app and uh output, you
would actually see the doc, a link to
it, click out, and you could share the
doc with someone as well. And anytime
you ran the same Opal again, it would
just upload more content into the same
doc."
LXqassleEcw,Donâ€™t forget to join us at #TheAndroidShow,"A behind the scenes peek at where Google devs ideate, design, and ship features that help developers distribute their apps and games. Get the full scoop at #TheAndroidShow!

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Sajna Verna",2025-11-05T20:00:31Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Campaign: ;,type:Trailer/Promo;,ct: AIG;;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/LXqassleEcw/default.jpg,https://i.ytimg.com/vi/LXqassleEcw/hqdefault.jpg,24,public,6689,120,8,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,LXqassleEcw,"I've just left the engineering floors,
but this is where the magic happens.
This is where we ideulate, design,
build, and ship features that help
developers distribute their apps and
games to users all over the world, no
matter where they are. Join us at the
Android Show where we can tell you a
little bit more about how we're bringing
data clarity to developers and bringing
and leveraging the best of Gemini models
to accelerate your workflows. We'll see
you there."
wACQU4TWHTw,That feeling when you drop your app's wait time. âš¡ï¸,"Why, yes. I am happier that my app is low-latency. 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: M.E Francis",2025-11-05T05:01:13Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Influence;,Campaign: ;,type:G4D SV: Comedic skits;,ct: AIG;;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/wACQU4TWHTw/default.jpg,https://i.ytimg.com/vi/wACQU4TWHTw/hqdefault.jpg,13,public,8015,97,4,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,wACQU4TWHTw,"[music]
We're living
like a bonfire [music]
on the"
VK5fxLOKPu8,What is SRE?,"Our Google SREs explain what Site Reliability Engineering is and why reliability is an essential component of modern software engineering.

Resources: 
Learn about SRE â†’ https://goo.gle/43IXUv2 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Salim Virji",2025-11-05T00:00:40Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Campaign: BBDS;,type:G4D SV: Educational ;,ct: AIG; ;,gds:Yes;,series: AASRE;",28,en,en,https://i.ytimg.com/vi/VK5fxLOKPu8/default.jpg,https://i.ytimg.com/vi/VK5fxLOKPu8/hqdefault.jpg,75,public,8411,334,8,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,VK5fxLOKPu8,"What is S sur or site reliability
engineering? And where does reliability
belong in software engineering? Since
the epoch, software engineers have
designed and written code and then
turned it over to ops or IT to run
day-to-day. This often meant a gap in
knowledge between the group that
developed the software and the group
that [music] operated the software. Each
of these groups had different special
skills and would not necessarily talk to
each other. So what is SRE? SRE is what
happens when you ask a software engineer
to design an operations team. The result
is the practice of designing and
operating a production system. This
helps us address the problem of having
teams that need to talk to each other
and sometimes don't. And it brings
together experts [music] to build and
operate these production systems. So,
let's take a step back. What is a
production system? It's several pieces
of software working together to deliver
a digital service. something you access
over the web or through an app or with a
dedicated device like your learning
thermostat [music] or a smartwatch.
Coordinating the behavior of these
different pieces of software and
ensuring that the system which emerges
is sufficiently available for users.
This is the responsibility of a site
reliability engineer. Discover more on
our website surre.google Google and
through the podcast Google's podcast on
production software."
gq6mXwicBO4,Whatâ€™s going to happen when we call show()? Go!,"Here's a Python scoping puzzle that catches many developers off guard! This code appears straightforward - print a global variable, reassign it locally, print again. But Python's scoping rules have a surprise in store. Will this print 10 and 5, or will it raise an error? Understanding local versus global scope is essential for writing correct Python code.

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speakers: Juan Vazquez",2025-11-03T14:01:23Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Campaign: G4D: Puzzles;,Video Type:G4D SV: Educational ;,ct: AIG;;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/gq6mXwicBO4/default.jpg,https://i.ytimg.com/vi/gq6mXwicBO4/hqdefault.jpg,20,public,16177,135,14,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,gq6mXwicBO4,"Take a look at this Python code with me
really quickly. This function looks
totally fine. It prints a variable, then
changes it. But when we run it, things
don't go as planned. Take a look.
What is going to happen when we call
show? Does it print 10 and five or blow
up completely? Drop your answer in the
comments."
1QdNxfI2wTc,A developerâ€™s haunted house ðŸ‘»,"Outdated dependencies, endless meetings, and 3-way merge conflicts? Screamworthy! 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: M.E Francis",2025-10-31T04:00:06Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Campaign: ;,type:G4D SV: Holiday or Observance;,ct: AIG;;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/1QdNxfI2wTc/default.jpg,https://i.ytimg.com/vi/1QdNxfI2wTc/hqdefault.jpg,24,public,15649,213,13,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,,
Y1SRd-HKbBU,How should devs use AI thoughtfully? (3 questions to guide you),"3 essential questions to thoughtfully integrate AI into your products by prioritizing user needs.

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Mahima Pushkarna 
Products Mentioned:  Google AI",2025-10-30T23:00:59Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Influence;,Campaign: BBDS;,Video Type:G4D SV: Educational ;,ct: AIG; ;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/Y1SRd-HKbBU/default.jpg,https://i.ytimg.com/vi/Y1SRd-HKbBU/hqdefault.jpg,90,public,11041,203,6,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,Y1SRd-HKbBU,"AI is everywhere, but how do we use it
thoughtfully? It's tempting to jump
right into using generative AI, but
before you do so, you need a real user
need and then see how AI can add unique
value. Here are three basic questions to
guide you. First, how do people frame
and experience the problem? Understand
what factors lie at the heart of the
problem and what people do to manage it?
Because people can experience the same
problem in different ways in different
contexts. The same problem can also
affect some people more deeply than
others. Your AI will need to be better
than any existing solution. Second, can
AI offer a unique solution that truly
adds value? Your users will need to
learn how to control your AI. So,
consider where it truly shines. Is the
go to automate a repetitive or tedious
task or to help make sense of complex
data at a scale that otherwise is
incredibly difficult. If a nonAI
approach works just as well, it's
probably better. Third, what does adding
AI take away from the user's experience?
Ultimately, we like doing things and
want to remain in control. Too much
automation, especially where it isn't
needed, can create frustration. Not
enough oversight, and people may not
trust your [music] AI. Your AI should
help all your user groups over long
periods of time [music] and in many
situations. The goal is to build a
consistently useful [music] tool, not
just a technically impressive one. So by
leading with your users's needs, you can
thoughtfully use AI in your products."
Pm3vxmvrK18,Connecting Gemini API to the Outside World with Tools | Intro to Tools,"Learn what tools are and why they're needed to get real-time information through a practical demo in Google AI Studio. This video shows how to enable the ""Grounding with Google Search"" tool to bypass a model's knowledge cutoff and provide a correct, cited answer to a real-world query. This is Part 1 of our series on using tools with the Gemini API.

Resources:

Start building AI apps â†’ https://goo.gle/4oARdmR 

Watch more AI Studio Tutorialsâ†’https://goo.gle/AIStudioTutorials 
Subscribe to Google for Developers â†’ https://goo.gle/developers 

Products Mentioned: Google AI, Gemini, Gemini API",2025-10-30T22:20:02Z,"Google,developers,Type: Upload Only;,ct: AIG;",28,en,en,https://i.ytimg.com/vi/Pm3vxmvrK18/default.jpg,https://i.ytimg.com/vi/Pm3vxmvrK18/hqdefault.jpg,176,public,10028,179,7,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,Pm3vxmvrK18,"If you're using LLMs without connecting
them to tools, you're missing out on
some of their most powerful features. In
this series, I'll show you how to use
tools with Gemini, from running code to
browsing the internet to integrating
with APIs. But before we do that, let's
talk about what tools are and why they
are so important. LLMs produce responses
by generating probable tokens. They take
in a sequence of text and return tokens
one at a time. Each token is generated
from a forward pass through the model.
This mechanism has been really
successful at producing models that can
generate fluent and nuance text, solve
tough problems, and write executable
code. But LLMs have some limitations
because out of the box, they're not
connected to the outside world. When
responding to user queries, they don't
have access to knowledge outside of
their training data, which means they
can't return real-time information about
things like the weather or traffic or
the news. They also can't take actions
like booking flights, sending messages,
or writing information to a database.
And while they might be able to solve
math problems, relying on an LLM to do
the actual computation isn't necessarily
the most efficient solution. And that's
where tools come in. They help us to
extend the capabilities of large
language models by giving them access to
the outside world. Let's check out an
example in AI studio.
So here I am in Google AI studio and uh
you can see over here on the right hand
side there are several built-in tools
that you can experiment with. Uh we're
going to try out one of those tools
right now. So I'm based in Austin and we
just had the ACL music festival. So I
will ask Gemini who headlined ACL
festival 2025
and we can run this query and you'll see
that the model says that the headliners
have not been announced yet. Um but
actually this event already happened. So
if we scroll up to the top you can see
we're using Gemini flash latest and
right here it says that the knowledge
cut off for this model is January 2025.
So that means that anything that
happened after January 2025 the model
doesn't have access to. But of course,
we often need our models to have access
to more real-time information. So, what
I'm going to do is I'm going to copy
this query and refresh the chat. And
we'll type in the exact same query as
before, who headline ACL Festival 2025,
but this time we will turn on grounding
with Google search. And this essentially
gives the model access to the results
from a Google search query. So, you can
see that this time when we run the
query, the model returns the correct
answer. So, it returns the headliners as
well as information um on citations. So,
where did this content actually come
from? Connecting Gemini to search is a
great way to get access to recent
information and to get responses that
are grounded in citations so you know
where that information came from. And
that's all for part one, but stay tuned
for future tips on how to use tools."
-YGIV5nrI3M,Just in from the news desk ðŸ“°: Gemini 2.5 Computer Use Model,"The Computer Use model is a specialized model built on Gemini 2.5 Proâ€™s capabilities to power agents that can interact with user interfaces. See how this tool enables you to build browser control agents that interact with and automate tasks.

Resources:
Learn moreâ†’ https://goo.gle/Computer-Use-Model 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

#GoogleDeveloperNews

Speaker: Juan Vasquez 
Products Mentioned:  Products Mentioned: Google AI, Gemini, Gemini API",2025-10-30T16:00:00Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Campaign: ;,Video Type:G4D SV: Educational ;,ct: AIG; ;,gds:Yes;,series: Google Developer News;",28,en,en,https://i.ytimg.com/vi/-YGIV5nrI3M/default.jpg,https://i.ytimg.com/vi/-YGIV5nrI3M/hqdefault.jpg,70,public,11028,225,4,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,-YGIV5nrI3M,"Are you ready for the next step in
aentic AI? We all know AI can help us
accomplish great things, but what about
the tedious, repetitive stuff that still
needs our attention? Think filling out
submitting registration forms, expense
reports, or unsubscribing from mailing
lists. Developing agents with the power
to do these things safely will benefit
us all. And that's why we created the
Gemini 2.5 computer use model. It's
built on Gemini 2.5 Pro's best-in-class
visual understanding and reasoning
capabilities. It powers agents capable
of interacting with user interfaces and
it outperforms rivals on multiple
benchmarks. And the best part, it also
offers lower latency. Of course, with
all that power comes a responsibility to
ensure safety and security. We've
trained features directly into the model
and devs can prevent it from
autocompleting risky or harmful actions
such as harming a systems integrity or
bypassing catches. So you can build
tomorrow's agents with confidence. The
Gemini 2.5 computer use model is
available now in public preview. Give it
a go via the Gemini API on Google AI
Studio and Vertex AI. We can't wait to
see what you do with it."
DuP1yo6jWmA,The Halloween Haul - The Developerâ€™s Version,"Some tricks, some treats, but everyone is excited for those AI goodies ðŸ‘»

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: M.E Francis, Meg Bauman, Bri Davis",2025-10-30T04:00:12Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Campaign: ;,type:G4D SV: Holiday or Observance;,ct: AIG;;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/DuP1yo6jWmA/default.jpg,https://i.ytimg.com/vi/DuP1yo6jWmA/hqdefault.jpg,18,public,18347,150,8,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,DuP1yo6jWmA,"What'd you get? I got two candy bars and
a well-written feature spec with clear
requirements.
>> Oh man, I got an apple and three tickets
full of unreproducible bugs.
>> Hey, they're giving away kidsized LLMs
with infinite context windows at the
house down the street. Let's go. [gasps]"
gYZS61BRXW0,Dev Debates: small commits or big commits? Tests before or after?,"Developers sound off on their favorite coding practices. Share yours below. 

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Event: Google IO",2025-10-29T04:01:00Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Influence;,Campaign: ;,Video Type:G4D SV: I/O ;,ct: AIG; ;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/gYZS61BRXW0/default.jpg,https://i.ytimg.com/vi/gYZS61BRXW0/hqdefault.jpg,46,public,11887,169,15,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,,
VwmFyowbSR0,JJ Con â€¢ Keynote,"Jujutsu, aka JJ, is an open-source, Git-compatible VCS that is both simple and powerful. This year, JJ Con 2025 was a dedicated conference hosted by Google for the Jujutsu version control system. In this talk from JJ Con, Martin von Zweigbergk presents on architecture and future plans.

Resources: 
Learn more about Jujutsu, aka JJ, here â†’ https://goo.gle/3L81HMa 
View the slides for this presentation here â†’ https://goo.gle/4qfuFJZ 
Check out other presentations from JJ Con in the playlist â†’ https://www.youtube.com/playlist?list=PLOU2XLYxmsILM5cRwAK6yKdtKnCK6Y4Oh

Watch more videos from JJ Con 2025 â†’ https://goo.gle/3Jxbj2p 
Subscribe to Google for Developers â†’ https://goo.gle/developers

Products mentioned: Google Open Source",2025-10-27T18:52:53Z,"Google,developers,Purpose: Learn;,Video Type:Livestream;,ct: ;,gds:N/A;,pr_pr: Google Open Source;,ct: AIG;",28,en,en,https://i.ytimg.com/vi/VwmFyowbSR0/default.jpg,https://i.ytimg.com/vi/VwmFyowbSR0/hqdefault.jpg,738,public,1488,31,4,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,VwmFyowbSR0,"Thank you all for coming here. Um it's
exciting exciting to see so many of you.
Um
um as reminder this is getting recorded
um streamed so keep that in mind.
Um yeah, I said thanks for um thanks for
coming um and thanks for all you've done
for the project. Um and like
um like PRs, bug reports, documentation
um user support
and like media and blog posts and all
the surrounding tools you have created.
Thank you.
Um, and thanks for being a welcoming
community on on Discord and and
everywhere.
Um, and thanks to Kevin and Emily for
for organizing
and um, yeah, it's project has grown
pretty quickly um, from I mean it's
about six years old now. Um but yeah
it's it's the last few years have
uh grown exponentially as you can see.
Um so I was going to just start with
talking a bit about the early
development uh history um and um it
started almost six years ago now. Um the
first commit from that date uh October
2019
um was a small commit with just like a
100 lines of code or or less. Um but
already some some familiar code like the
especially the the pluggable back end uh
was planned from commit one apparently.
I didn't even remember this.
Um
and um yeah the it started
probably many of you know it started
just as an experiment with the working
cops commit uh feature to see how how
that would work and turned out to work
pretty well. So um continued on deploy
it for for a while and then like the uh
first class conflicts uh feature kind of
grew out of that because when you have
conflict in the working copy you don't
want to be stuck you if the working copy
is is a commit and should be able to
handle this conflict to state too. So,
uh, I was I kind of had to figure out
how to to handle this.
Um, and, um, like I had some early
version working with, uh, some simple
conflicts. Um, and then like in, um,
April, I I got got it to work like
refactored and and
got it to work with model a bit
differently. Uh, and then it started
working like pretty much the same way it
currently does where you can like
reorder a chain of commits and like
several times and and if you go back to
the original states the conflicts are
gone and that was pretty exciting moment
when when when I got that to to see the
actually working practice.
Um
then the
uh few months later um I was trying to
figure out how to make the internal APIs
simpler or simple uh for uh for so each
command did not have to like worry about
concurrent uh changes by other commands
or anything else that could have
happened to the repo. Um and that's
where the operation log came from.
Um and like then it was just a happy
accident that it turned out to be useful
for Andu and um restoring the repo too.
Um
then um yeah and like six months after
that or so uh we moved the repository to
to GitHub
u from my internal material material
repository
um and uh
uh right yeah so so and and like a few
months after um I realized that like
after having had these first class
conflicts for for here or something. I
realized that this actually means that
you can always rebase all the
descendants all the time and also
because uh JD was faster than I expected
uh like after having worked with
material uh it turns out you can
actually it it is a good user experience
to just always rebase all the commits.
Um so I enable that by defaults
um yeah and then
um
The next year in February, we saw JJ on
the AI news front page for the first
time. Um, and like that was the first
real attention the project got.
Um, and like pretty soon after that, we
we got approval to start the
integrations at Google. So, it was a
very exciting uh time for for me. Um and
uh yeah uh like two years after we we
got our first uh internal users um about
20 users roll out to about 20 users then
uh they were mostly pretty happy already
then but like we have obviously improved
a lot of things since then. Uh and now
just any day now we're at what we
consider the open uh open beta release
which means anyone can use it and um
still have some bugs of course and uh
features were missing but uh um yeah we
have about 8 900 users um and growing
quickly
um externally I don't know uh guess we
have at least 100 users or so uh hard to
know. Um, probably similar shape to the
GitHub store history maybe. So, it's
maybe growing pretty quickly, but who
knows? Um,
yeah. And so, what I'm looking forward
to in the future, um, short term, we
have we still don't have copy tracking
support as as you probably have
discovered.
Um, so we we have a design after a lot
of time spent on that. Um, and I'm
hopefully it will work. Uh, we're
working on getting that actually
implemented.
Uh, so that means you should be able to
like the uh re rebase a commit even if
it was if the files were up uh renamed
upstream for example. Um and yeah, I
think we can we can get um
like
get blame for example to work well with
uh this copy history. We will be able to
model like uh merged files which I think
most other pieces don't. Uh, so I'm
excited to see how how that turns out
with for for Blame and maybe we'll even
be able to um propagate changes uh when
you moved parts of the file into another
file. But that's a lot of work. We'll
see how it goes.
Um um and like another feature we're
missing today is the RPC API. We talked
a bit about it. Um it makes it hard for
integrations. Currently they we have the
JJ lib library but uh it's too low level
it's one problem. So we can just move
more into it. So that's not a big
problem. Um and so like GD I think
currently uses it but they have to
duplicate a bunch of stuff. Um but like
the another benefit of the RPC API uh it
would be that we can um that uh
the integrations we'll be able to use
the any like custom backends like at
Google for example we have custom
backends for all our storage um and like
if you if you run the uh if you run GG
internally that just doesn't work
because it doesn't know about our
backend
Um,
and yeah, another big thing we're
looking forward to is uh cloud-based
hosting like like we have at Google. Um,
so East River Source Control is uh
working on uh exciting stuff there.
Um, and I'm hopeful we will be able to
um make life better for game studios
especially um um because they they work
with large files and uh um they can't
use git currently many of them because
git expects
uh yeah git's not very good at dealing
with large files in general.
Um, and I also hope we'll be able to see
a an open source VFS um, someday uh,
like like the one we have at Google, so
you can also more uh, efficiently work
with large files especially.
Um, yeah, that's it. Um, some practical
things for uh, for today. Um on Discord
there's a JJcon channel
uh where you can like if you have any
questions or any you want to share
anything here.
Um the rough schedule you can you can
find it there's a link from that JDCon
channel and there's a it's on the wiki
uh it links to the wiki um the the the
full schedule but like um this the
morning will spend here in in Mars Grove
with we'll have talks here then lunch
will be served in the lobby
um and then uh there will be design
discussions here and uh in Walker
Canyon, which is across from the lobby.
Um, and maybe some other smaller rooms
if necessary.
And yeah. Yeah, please mute your cell
phones.
Um,
the um
unconference in person. Uh, what was
this, Kevin? Sorry.
So, there's a whiteboard at the back
over there. Um, add your topics there.
And
>> yeah, everyone heard that there's a
backboard whiteboard in the back uh
where you can add topics for the
conference part.
>> Um,
yeah, if you haven't checked in in
lobby, you will get a t-shirt
and stickers. Um, and we have access,
you have access to much of this floor.
Um, if there's a door where you need a
badge, you don't have access, obviously.
Um, there are bathrooms, I think, out
this way.
Um, there is some gender neutral and
more private bathrooms further that way.
Um, and also like um the organizers
here, we wear this beige uh t-shirt with
the J logo. So if you have any
questions, you can find one of us.
Yeah, that's it.
Thank you."
LoV307-d19A,What do you think the second print will show? Go!,"Here's a classic Python gotcha that trips up developers at all levels! This simple helper function appears straightforward, but mutable default arguments create unexpected behavior. When you call this function twice, the results might surprise you. Understanding how Python handles default arguments is crucial for avoiding subtle bugs. What do you think the second print statement outputs?

Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speakers: Juan Vasquez",2025-10-27T13:00:51Z,"Google,developers,pr_pr: Google for Developers;,Purpose: Learn;,Campaign: G4D: Puzzles;,Video Type:G4D SV: Educational ;,ct: AIG;;,gds:Yes;",28,en,en,https://i.ytimg.com/vi/LoV307-d19A/default.jpg,https://i.ytimg.com/vi/LoV307-d19A/hqdefault.jpg,17,public,6641,96,16,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,LoV307-d19A,"Hey, hey, stop scrolling for a second.
Help me with this. We're writing a
simple helper to collect items in a
list. Nothing fancy. But when we call it
twice, something weird happens. Take a
look at the code. [music]
What do you think the second print will
show? Drop your answer in the comments
and let us know."
HKW163OhuSU,Introducing Google Maps Grounding in the Gemini API,"Unlock powerful new experiences by connecting the Gemini API with Google Maps. With access to real-time data from over 250 million places, you can now build incredible, location-aware AI applications.

Resources: 
Start building AI apps â†’ https://goo.gle/4oARdmR 

Watch more AI Studio Tutorialsâ†’https://goo.gle/AIStudioTutorials 
Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Logan Kilpatrick 
Products mentioned: Google AI Studio, Gemini APi, Gemini, Google Maps",2025-10-24T19:24:30Z,"Google,developers,pr_pr: Gemini API / AI Studio;,Purpose: Learn;,Video Type:DevByte;,ct: ;,gds:N/A;",28,en,en,https://i.ytimg.com/vi/HKW163OhuSU/default.jpg,https://i.ytimg.com/vi/HKW163OhuSU/hqdefault.jpg,188,public,8417,172,8,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,HKW163OhuSU,"Hey everyone, how's it going? Excited to
announce our new grounding with Google
Maps feature or the Google Maps tool in
the Gemini API. I'm super excited for
this. So, the TLDDR is make it really
easy to access all of the [snorts] 250
million plus uh place data that Google
Maps has for developers to build really
interesting applications on top of this
with Gemini. Um, so we have this new
experience inside of AI Studio where you
can sort of try different demos and
prototypes and also build them yourself
and remix them. Um, so here we have one
that's built with our live API and this
new feature. So let's try it out in
action. Uh, and we'll see what's
possible with this new capability.
>> Hi there. I'm a demo agent powered by
grounding with Google Maps.
>> Okay, cool. Yeah, this is interesting.
Um, we're hovering over Chicago right
now. I've always wanted to visit Chicago
and check out uh check out the city, so
maybe we can look at some Italian
restaurants.
>> I'll use Grounding with Google Maps for
that request. Great choice. Chicago is a
fantastic city. Please wait while I
confer with Grounding with Google Maps
to find some Italian restaurants for
you. Here are the top three Italian
restaurants in Chicago, Illinois.
>> Nice. These look interesting, but I've
always wanted to try this place,
Zarella, that I've been hearing about
and seeing. Supposedly, it's a really
good I think it's it's spelled Zarella,
not Zorella.
>> Apologies. I can't find a restaurant
called Zorella in Chicago. Perhaps the
name is spelled differently. If you'd
like, I can give you some more
suggestions from Google Maps.
>> Yeah, maybe you try three different
spellings and we'll see if we can get
it.
I'll use grounding with Google Maps for
that request.
I found one restaurant matching your
query. Zerella Pizzeria in
>> That's it. That place looks awesome. Um
I don't know if it's possible, but can
you I haven't tested this before, but
can you get me reservations? Is that is
that possible?
>> I am sorry I can't directly make
reservations.
>> I didn't think so. But awesome. This was
really cool. So I think the gist is um
and this this sort of experience is is
very customizable. So, um, actually, if
we go to the settings panel, you can
swap out some of the different models if
you want to. You can change the voices
and you can also see what we did in the
system prompt here. So, in this example,
we're really trying to get the model,
the live API, um, to actually use
grounding with Google search. And it
says things like, if you look down in in
some of the system instructions, it says
things like, when you're doing a tool
call, make sure to say, ""Give me a
second while I use grounding with Google
search."" So, all of that's customizable.
The experience is really flexible. Um,
and then you can actually use this uh
chat on the lefth hand side to either
add features to this experience or
continue to customize it. So, um, I'm
really excited for people to get their
hands on this. Uh, the new Google Google
Maps grounding experience um, and send
us feedback. The blog post that I'll
link below has a bunch of details about
the all the pricing and availability and
code samples and things like that. So,
uh enjoy and thanks for watching."
MwhJ2AAEWVo,Build an AI Restaurant Finder in 60 Seconds with Gemini & Google Maps Grounding in AI Studio,"Go from idea to a live restaurant recommender app in under a minute. This tutorial shows you how to use the power of Gemini and Google Maps Grounding in Google AI Studio to build powerful, location-aware AI applications with incredible speed.

Resources: 
Start building AI apps â†’ https://goo.gle/4oARdmR 

Watch more AI Studio Tutorialsâ†’https://goo.gle/AIStudioTutorials 
Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Ammaar Reshi
Products mentioned: Google AI Studio, Gemini",2025-10-24T19:23:11Z,"Google,developers,pr_pr: Gemini API / AI Studio;,Purpose: Learn;,Video Type:DevByte;,ct: ;,gds:N/A;",28,en,en,https://i.ytimg.com/vi/MwhJ2AAEWVo/default.jpg,https://i.ytimg.com/vi/MwhJ2AAEWVo/hqdefault.jpg,140,public,5274,88,6,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,MwhJ2AAEWVo,"Hey everyone, we just launched an
overhauled vibe coding experience inside
of AI Studio, which makes it super easy
for anybody to make AI powered apps
using Gemini. And so what you'll notice
immediately when you come in here now is
that you have all of these AI chips that
allow you to add essentially AI
superpowers to your app. So if you want
to make a nano banana powered app, you
can do that. You can build
conversational voice apps with the live
API or animate images with VO. And you
don't need to know how to tie any of
these sorts of API requests together.
Um, and if you're also stuck for an
idea, you can hit the classic I'm
feeling lucky button and it'll come up
with something pretty creative and
interesting and show you how to tie
different parts of this experience
together. Um, but I have an app idea in
mind. So, I'm actually going to go and
make something entirely new, which is uh
I want to know what to eat tonight uh
from the local restaurants around me
using Google Maps Grounding, a new tool
we actually launched uh last week and I
want to make an app with it. Uh and so I
can say um I want an app that allows me
to enter my favorite cuisine and then it
finds local restaurants around me for me
to try.
>> Amazing. So, what this should do is now
make an app powered by Gemini and uh
Maps Grounding and then find local
restaurants for me in my area. And
again, I don't need to know how to learn
about maps grounding in terms of how the
API requests work or anything like that.
AI Studio just figures it out all for
me. Uh so yeah, let's see what it comes
up with.
Okay, sweet. So, my app is ready and you
can see that it's asking for my
location. That's a good start. Uh, and
then I can kind of type in what I want.
So, in this case, I want Pakistani food.
And let's see what kind of restaurants
it finds for me.
That's awesome. So, added nice
animations. Uh, you can see that it
actually found local restaurants and the
descriptions around them. Um, and then I
think I'll be able to click this and
it'll take me to Google Maps and show me
the restaurant on the map as well.
That's awesome. I can then see the
reviews and ratings for me. And so I'm
really excited to see what kind of
awesome apps you guys create using the
Gemini SDK. Uh you can make all sorts of
amazing dynamic apps. Uh so yeah, just
get started. Go to ai.studio/build
Studio/build
uh and go from"
ZHpT3ev2XLA,Vibe Code with Your Voice in Google AI Studio,"Introducing a new way to build: vibe code with your voice. See how you can dictate changes, add features, or describe your app using our new speech-to-text feature in Google AI Studio. It intelligently removes filler words and mistakes for a clean, efficient prompt every time.

Resources: 
Start building AI apps â†’ https://goo.gle/4oARdmR 

Watch more AI Studio Tutorialsâ†’https://goo.gle/AIStudioTutorials 
Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Ammaar Reshi
Products mentioned: Google AI Studio",2025-10-24T18:45:14Z,"Google,developers,pr_pr: Gemini API / AI Studio;,Purpose: Learn;,Video Type:DevByte;,ct: ;,gds:N/A;",28,en,en,https://i.ytimg.com/vi/ZHpT3ev2XLA/default.jpg,https://i.ytimg.com/vi/ZHpT3ev2XLA/hqdefault.jpg,70,public,4528,42,3,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,ZHpT3ev2XLA,"Hey everybody, we've now added the
ability for you to vibe code with your
voice. This makes it super easy to
iterate on apps uh just by talking to
it. Uh and if you've got lots of ideas,
but it takes a while to type up, this is
a great solution. So now look for the
speech to text icon in the prompt area.
Uh and then when you go ahead and press
that, I'll be able to guide the app with
my voice. So
I want to make this app futuristic.
Let's add a cyberpunk look and feel to
it. And I would also love for you to add
a decade selector um to the app.
Now, I did that drawn out um at the end
on purpose. And as you can see, it
actually cleaned that out so it kept it
exactly to what I was saying while
removing all of the filler and uh
mistakes that I might have been making
while I was dictating. And then I can
just go ahead and send that
And there you have it. It's made a
futuristic version of the app. It's
added all of the animations and it was
all vibecoded with my voice. A new
quality of life improvement to remix and
vibe code in AI Studio."
zlDziSQ4JTk,New & Improved API Key and Project Management in Google AI Studio,"We've shipped a brand new API key and Projects page inside of Google AI Studio, making it easier to create, import, and manage projects. 

Resources: 
Start building AI apps â†’ https://goo.gle/4oARdmR 

Watch more AI Studio Tutorialsâ†’https://goo.gle/AIStudioTutorials 
Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Logan Kilpatrick 
Products mentioned: Google AI Studio",2025-10-24T18:35:46Z,"Google,developers,pr_pr: Gemini API / AI Studio;,Purpose: Learn;,Video Type:DevByte;,ct: ;,gds:N/A;",28,en,en,https://i.ytimg.com/vi/zlDziSQ4JTk/default.jpg,https://i.ytimg.com/vi/zlDziSQ4JTk/hqdefault.jpg,134,public,3436,39,3,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,zlDziSQ4JTk,"Hey everyone, how's it going? Excited to
show the new revamped API key page and
also a new project page available in AI
studio. So this is for API developers
who are sort of trying to manage all the
stuff around their account that's using
the underlying API. Uh so we have this
new we have this new project page. You
can see all the projects that are
connected to your to your sort of AI
studio account from an API perspective.
You can actually import other products
uh other projects rather. This was one
of the points of feedback that we got,
which is sometimes folks have like tons
of of Google Cloud projects behind the
scenes. They were by default seeing all
those or not seeing those inside of AI
Studio and folks want to be able to
control sort of what projects they see,
what projects they don't. Um, you can
actually also go and create net new
projects. Uh, so I'll do testing one two
three. Uh, and I'll go and make that
project net new project inside of AI
Studio. So now I have another new
project uh, which I can set up billing
for, etc., etc. You can also see the
usage. So, we announced a new usage
dashboard earlier. You can see the
billing for this, etc. Um, on the API
key front, uh, this is also really
helpful because it allows you to do a
bunch of things like we added this new
renaming key feature, which folks have
been asking for for a long time. You can
also create more keys now inside of AI
Studio. Um, and also see sort of a bunch
of usage and billing data separately
broken down by project as well. So, a
bunch of new features in this sort of
redesigned experience in addition to
what we also did from a usage and
billing standpoint uh a couple of weeks
ago. So, this should be fully rolled out
now to everyone. Um, and there's also a
couple of quick callouts. If you, for
example, don't see your project uh
anymore that maybe you saw before, we we
did it based on activity. So, um you
might be now like seeing a project
hidden that you hadn't used in a while.
Um so, you can always reimpport that
project. Uh so if you click import
projects, it'll search through all your
available projects um on the right hand
side and then you can choose the ones
that potentially need to be reimpported
if you want to continue to see those. So
hopefully this makes it easier if if
you're sort of using AI Studio and have
many cloud projects or have many
projects in general and you want to
separate billing, usage, etc. Um so
enjoy the updates. Send us feedback if
things are confusing or the like. Uh,
and we'll see you in the next update."
joa1N3HlDak,Overview of the new vibe coding experience in AI Studio,"Introducing the new AI first vibe coding experience in Google AI Studio. Built to take you from prompt to production with Gemini, and optimized for AI app creation. 

Resources: 
Start building AI apps â†’ https://goo.gle/4oARdmR 

Watch more AI Studio Tutorialsâ†’https://goo.gle/AIStudioTutorials 
Subscribe to Google for Developers â†’ https://goo.gle/developers 

Speaker: Logan Kilpatrick 
Products mentioned: Google AI Studio",2025-10-24T18:34:43Z,"Google,developers,pr_pr: Gemini API / AI Studio;,Purpose: Learn;,Video Type:DevByte;,ct: ;,gds:N/A;",28,en,en,https://i.ytimg.com/vi/joa1N3HlDak/default.jpg,https://i.ytimg.com/vi/joa1N3HlDak/hqdefault.jpg,281,public,17506,352,16,UC_x5XG1OV2P6uZZ5FSM9Ttw,Google for Developers,"Subscribe to join a community of creative developers and learn the latest in Google technology â€” from AI and cloud, to mobile and web.

Explore more at developers.google.com

",US,https://yt3.ggpht.com/WZ_63J_-745xyW_DGxGi3VUyTZAe0Jvhw2ZCg7fdz-tv9esTbNPZTFR9X79QzA0ArIrMjYJCDA=s88-c-k-c0x00ffffff-no-rj,2570000,6866,joa1N3HlDak,"Hey everyone, how's it going? Excited to
announce the new vibe coding experience
in AI Studio. Um, so we've got a whole
week of of updates and launches that are
going to be taking place around this new
experience. Uh, so we'll start off today
with this new build start experience.
Uh, when you go into vibe coding. Um, so
if you click on the leftnav, if you
click on build or from the homepage, you
click on vibe code, uh, you get dropped
into this new experience. Um, the idea
here is let's enable you to build AI
powered apps. So I think as uh we think
about like what are the unique things
that our team can do uh we have all this
amazing Gemini API infrastructure behind
the scenes and we want to help people
build AI apps with it. Um so you have
all these great suggestions of ways to
sort of you know as the as the quote
says supercharge your apps with AI. Um
there's lots of stuff again this is
basically everything that's available in
our API. I don't want to necessarily
make the decision. So this is my
favorite feature is you click on feeling
lucky um and we get some really really
cool different experiences. Uh, and you
can sort of cycle through different
options. Um, so let's do this. Create a
trivia game featuring an AI host with
dynamic users user chosen personality.
Um, it's going to generate images with
imagine. It's going to use flashlight
for faster responses. Um, and then we'll
also have 2.5 Pro. So, let's build it
and see what happens. Um, and while this
takes place, I'll sort of speed along,
but um, you see on the lefth hand side,
you have just a traditional like code
assist experience where you can chat
with uh, the model and make suggestions.
Um, you have deployment options up in
the right, you can save to GitHub, you
can deploy your app, uh, you can just
share it with people, you can switch
over to an API key, you can download the
code, you can make copies, lots of
things that are available. Um, you can
also go in and directly edit the code.
So during right now, we'll see the
generation happening. Um, but if you
wanted to go in and tweak a bunch of
stuff, you can do that. Uh, which is
awesome. Um, this experience is also
free. Uh, so you don't need to there's
you don't need to sort of put in a
credit card or anything to get started.
If you want to deploy, um, through Cloud
Run or you want to use some of the
premium models that aren't available for
free like V3.1 for example, uh, you do
need to switch over to a paid API key,
which we make easy to do in the top
right hand corner. Uh but for the most
part the experience is is free to get
started with and and build some cool
things. Um so you can also again we're
using uh 2.5 pros thinking capability
right now. So it's putting together a
plan and then we can actually see all
the different files that are being
created. And something that's sometimes
helpful to me at least uh is you can
hover over and see like what is actually
happening in these files. So create the
entry point for the react app rendering
the app component. So, this is helpful
for me also as someone who is not a
native TypeScript developer. I know
Python way better. Um, to be able to see
all these different things. So, all
right, we've got our trivia game app
featuring an AI host with a dynamic user
chosen personality. Um, so let's see.
Maybe we'll do a
sarcastic robot. Um, and we'll see what
happens.
One thing of the principles governing
the buoyancy would be intuitive. What is
the name of the phen where the objects
apparent weight decreases in fluid the
upward force? No idea. Um
pro le principle. Nope, not right. Yeah,
so lots of fun uh lots of fun
possibilities and it looks like the I
think 2.5 pro if I remember correctly.
Yeah, the AI powered chatbot is actually
powering these questions. Um then we
have an image studio which is really
interesting. So maybe we'll do um I
don't know how these things are
connected together. So that's maybe like
my follow on of like what why why do I
have all these different features uh
trivia chat and image generation all in
one place seems odd. Uh but that's the
fun part is you can go in and you
continue to edit and sort of refine the
experience that you want iteratively. Um
we also have these AI suggested
features. Um, so maybe if we do
uh yeah, we'll do this one. And it comes
together with and these are actually
suggested based on 2.5 flashlight
looking at the code in the context of
what you're building. And this is
implement a feature to display history
previously generated images in the image
studio tab allowing users to revisit
creation. So very cool. You can actually
continue to iterate um if you want. But
this is sort of the the guts of the new
experience. Um again you can ignore this
pirate persona one which is a joke. uh
that is only available internally. Um
lots of cool things you can click
through, you can add all these uh and
put together some really really
interesting ideas uh based on what's
possible. So I hope you enjoy this
experience. We'll have more updates over
the next couple of days and uh send us
feedback. Enjoy enjoy building. See you."
